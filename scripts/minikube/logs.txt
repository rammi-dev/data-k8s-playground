
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMMAND   â”‚                                                                                                                                                                                                                                                                     ARGS                                                                                                                                                                                                                                                                     â”‚ PROFILE  â”‚  USER   â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ addons     â”‚ enable ingress-dns                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:23 UTC â”‚ 06 Feb 26 15:24 UTC â”‚
â”‚ addons     â”‚ enable dashboard                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:24 UTC â”‚ 06 Feb 26 15:24 UTC â”‚
â”‚ addons     â”‚ enable metrics-server                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:24 UTC â”‚ 06 Feb 26 15:24 UTC â”‚
â”‚ addons     â”‚ enable registry                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:24 UTC â”‚ 06 Feb 26 15:28 UTC â”‚
â”‚ addons     â”‚ enable metallb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:28 UTC â”‚ 06 Feb 26 15:28 UTC â”‚
â”‚ ip         â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:29 UTC â”‚ 06 Feb 26 15:29 UTC â”‚
â”‚ ssh        â”‚ -n minikube sudo mkdir -p /var/lib/rook && sudo chmod 755 /var/lib/rook                                                                                                                                                                                                                                                                                                                                                                                                                                                                      â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:29 UTC â”‚ 06 Feb 26 15:29 UTC â”‚
â”‚ ssh        â”‚ -n minikube sudo sysctl -w fs.inotify.max_user_watches=1048576                                                                                                                                                                                                                                                                                                                                                                                                                                                                               â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:29 UTC â”‚ 06 Feb 26 15:29 UTC â”‚
â”‚ ssh        â”‚ -n minikube sudo sysctl -w fs.inotify.max_user_instances=256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:29 UTC â”‚ 06 Feb 26 15:29 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m02 sudo mkdir -p /var/lib/rook && sudo chmod 755 /var/lib/rook                                                                                                                                                                                                                                                                                                                                                                                                                                                                  â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:29 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m02 sudo sysctl -w fs.inotify.max_user_watches=1048576                                                                                                                                                                                                                                                                                                                                                                                                                                                                           â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m02 sudo sysctl -w fs.inotify.max_user_instances=256                                                                                                                                                                                                                                                                                                                                                                                                                                                                             â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m03 sudo mkdir -p /var/lib/rook && sudo chmod 755 /var/lib/rook                                                                                                                                                                                                                                                                                                                                                                                                                                                                  â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m03 sudo sysctl -w fs.inotify.max_user_watches=1048576                                                                                                                                                                                                                                                                                                                                                                                                                                                                           â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ ssh        â”‚ -n minikube-m03 sudo sysctl -w fs.inotify.max_user_instances=256                                                                                                                                                                                                                                                                                                                                                                                                                                                                             â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ addons     â”‚ list                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:30 UTC â”‚ 06 Feb 26 15:30 UTC â”‚
â”‚ node       â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:32 UTC â”‚                     â”‚
â”‚ node       â”‚ list                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:33 UTC â”‚                     â”‚
â”‚ ssh        â”‚ minikube                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:33 UTC â”‚                     â”‚
â”‚ ssh        â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:33 UTC â”‚                     â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:37 UTC â”‚ 06 Feb 26 15:37 UTC â”‚
â”‚ ssh        â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:37 UTC â”‚ 06 Feb 26 15:38 UTC â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:40 UTC â”‚ 06 Feb 26 15:41 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt exec-opt=native.cgroupdriver=systemd --docker-opt log-driver=json-file --docker-opt log-opt=max-size=100m --docker-opt storage-driver=overlay2 --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --docker-opt max-concurrent-downloads=1                            â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:49 UTC â”‚                     â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --container-runtime=docker --docker-opt exec-opt=native.cgroupdriver=systemd --docker-opt log-driver=json-file --docker-opt log-opt=max-size=100m --docker-opt storage-driver=overlay2 --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --docker-opt max-concurrent-downloads=1 â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:51 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:51 UTC â”‚ 06 Feb 26 15:51 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --container-runtime=docker --docker-opt exec-opt=native.cgroupdriver=systemd --docker-opt log-driver=json-file --docker-opt log-opt=max-size=100m --docker-opt storage-driver=overlay2 --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --docker-opt max-concurrent-downloads=1 â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:51 UTC â”‚                     â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --docker-opt max-concurrent-downloads=1                                                                                                                                                                                        â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:54 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:54 UTC â”‚ 06 Feb 26 15:54 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --docker-opt max-concurrent-downloads=1                                                                                                                                                                                        â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:54 UTC â”‚ 06 Feb 26 15:58 UTC â”‚
â”‚ ssh        â”‚ -n minikube -- sudo tee /etc/docker/daemon.json                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 15:58 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:03 UTC â”‚ 06 Feb 26 16:04 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:04 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:05 UTC â”‚ 06 Feb 26 16:05 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:05 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:10 UTC â”‚ 06 Feb 26 16:10 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:16 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:34 UTC â”‚ 06 Feb 26 16:36 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:41 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:46 UTC â”‚ 06 Feb 26 16:47 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=true --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                 â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:47 UTC â”‚                     â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:53 UTC â”‚ 06 Feb 26 16:53 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=true --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500 --nodes=1                                                                                                                                                                                                                       â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:54 UTC â”‚ 06 Feb 26 16:54 UTC â”‚
â”‚ node       â”‚ add                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 16:54 UTC â”‚ 06 Feb 26 16:55 UTC â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:00 UTC â”‚ 06 Feb 26 17:00 UTC â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:01 UTC â”‚ 06 Feb 26 17:01 UTC â”‚
â”‚ delete     â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:06 UTC â”‚ 06 Feb 26 17:06 UTC â”‚
â”‚ start      â”‚ --driver=docker --nodes=3 --cpus=7 --memory=12288 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=true --docker-opt dns=8.8.8.8 --docker-opt dns=8.8.4.4 --docker-opt mtu=1500                                                                                                                                                                                                                                 â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:06 UTC â”‚ 06 Feb 26 17:10 UTC â”‚
â”‚ addons     â”‚ enable ingress                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:11 UTC â”‚ 06 Feb 26 17:13 UTC â”‚
â”‚ ssh        â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:13 UTC â”‚                     â”‚
â”‚ addons     â”‚ enable ingress-dns                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:13 UTC â”‚ 06 Feb 26 17:13 UTC â”‚
â”‚ addons     â”‚ enable dashboard                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:13 UTC â”‚ 06 Feb 26 17:13 UTC â”‚
â”‚ addons     â”‚ enable metrics-server                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:13 UTC â”‚ 06 Feb 26 17:13 UTC â”‚
â”‚ addons     â”‚ enable registry                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:13 UTC â”‚                     â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:14 UTC â”‚ 06 Feb 26 17:14 UTC â”‚
â”‚ ssh        â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:15 UTC â”‚                     â”‚
â”‚ ssh        â”‚                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:17 UTC â”‚                     â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:21 UTC â”‚ 06 Feb 26 17:21 UTC â”‚
â”‚ completion â”‚ bash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:21 UTC â”‚ 06 Feb 26 17:21 UTC â”‚
â”‚ addons     â”‚ enable metrics-server                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        â”‚ minikube â”‚ vagrant â”‚ v1.38.0 â”‚ 06 Feb 26 17:22 UTC â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2026/02/06 17:06:50
Running on machine: data-playground
Binary: Built with gc go1.25.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0206 17:06:50.534434  190148 out.go:360] Setting OutFile to fd 1 ...
I0206 17:06:50.535115  190148 out.go:413] isatty.IsTerminal(1) = true
I0206 17:06:50.535122  190148 out.go:374] Setting ErrFile to fd 2...
I0206 17:06:50.535124  190148 out.go:413] isatty.IsTerminal(2) = true
I0206 17:06:50.535345  190148 root.go:338] Updating PATH: /home/vagrant/.minikube/bin
I0206 17:06:50.535777  190148 out.go:368] Setting JSON to false
I0206 17:06:50.537178  190148 start.go:134] hostinfo: {"hostname":"data-playground","uptime":11862,"bootTime":1770385748,"procs":283,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-134-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"192972b9-1c7f-4e25-afd8-23acdf6b03e9"}
I0206 17:06:50.537247  190148 start.go:144] virtualization: vbox guest
I0206 17:06:50.538757  190148 out.go:179] ðŸ˜„  minikube v1.38.0 on Ubuntu 22.04 (vbox/amd64)
I0206 17:06:50.539667  190148 out.go:179]     â–ª MINIKUBE_K8S_VERSION=v1.34.0
I0206 17:06:50.539732  190148 notify.go:220] Checking for updates...
I0206 17:06:50.545467  190148 out.go:179]     â–ª MINIKUBE_CPUS=7
I0206 17:06:50.546260  190148 out.go:179]     â–ª MINIKUBE_NODES=3
I0206 17:06:50.547223  190148 out.go:179]     â–ª MINIKUBE_MEMORY=12288
I0206 17:06:50.548078  190148 out.go:179]     â–ª MINIKUBE_DRIVER=docker
I0206 17:06:50.549067  190148 out.go:179]     â–ª MINIKUBE_DISK_SIZE=40g
I0206 17:06:50.549959  190148 out.go:179]     â–ª MINIKUBE_EXTRA_CONFIG=kubelet.housekeeping-interval=10s kubelet.max-pods=50 kubelet.serialize-image-pulls=true
I0206 17:06:50.551517  190148 driver.go:422] Setting default libvirt URI to qemu:///system
I0206 17:06:50.575477  190148 docker.go:125] docker version: linux-29.2.1:Docker Engine - Community
I0206 17:06:50.575528  190148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0206 17:06:50.659933  190148 info.go:266] docker info: {ID:4e026bac-3bb6-43df-9b72-b685e8b47580 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:46 SystemTime:2026-02-06 17:06:50.649350103 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-134-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:42054021120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:data-playground Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
I0206 17:06:50.659988  190148 docker.go:320] overlay module found
I0206 17:06:50.661272  190148 out.go:179] âœ¨  Using the docker driver based on user configuration
I0206 17:06:50.662217  190148 start.go:310] selected driver: docker
I0206 17:06:50.662223  190148 start.go:932] validating driver "docker" against <nil>
I0206 17:06:50.662229  190148 start.go:943] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0206 17:06:50.663176  190148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0206 17:06:50.775087  190148 info.go:266] docker info: {ID:4e026bac-3bb6-43df-9b72-b685e8b47580 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:46 SystemTime:2026-02-06 17:06:50.764164081 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-134-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:42054021120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:data-playground Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
W0206 17:06:50.775238  190148 out.go:285] â—  Starting v1.39.0, minikube will default to "containerd" container runtime. See #21973 for more info.
I0206 17:06:50.775304  190148 start_flags.go:332] no existing cluster config was found, will generate one from the flags 
I0206 17:06:50.775420  190148 start_flags.go:1000] Wait components to verify : map[apiserver:true system_pods:true]
I0206 17:06:50.776032  190148 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I0206 17:06:50.777080  190148 cni.go:83] Creating CNI manager for ""
I0206 17:06:50.777110  190148 cni.go:135] multinode detected (0 nodes found), recommending kindnet
I0206 17:06:50.777115  190148 start_flags.go:341] Found "CNI" CNI - setting NetworkPlugin=cni
I0206 17:06:50.777156  190148 start.go:357] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 17:06:50.777921  190148 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0206 17:06:50.778454  190148 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 17:06:50.779169  190148 out.go:179] ðŸšœ  Pulling base image v0.0.49 ...
I0206 17:06:50.779852  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:06:50.779868  190148 preload.go:202] Found local preload: /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0206 17:06:50.779877  190148 cache.go:66] Caching tarball of preloaded images
I0206 17:06:50.779923  190148 preload.go:250] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0206 17:06:50.779928  190148 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 17:06:50.780171  190148 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 17:06:50.780561  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:06:50.780574  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/config.json: {Name:mkbf9f977730e32076054a9dc7ff2814bc45f0c2 Timeout:1m0s Delay:500ms}
I0206 17:06:50.843773  190148 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 17:06:50.843898  190148 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 17:06:50.843905  190148 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0206 17:06:50.843907  190148 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0206 17:06:50.843912  190148 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 17:06:50.843914  190148 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 17:06:59.438110  190148 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 17:06:59.438149  190148 cache.go:244] Successfully downloaded all kic artifacts
I0206 17:06:59.438187  190148 start.go:359] acquireMachinesLock for minikube: {Name:mk50794f3b668552bcb175548a808224fc99ceb9 Timeout:10m0s Delay:500ms}
I0206 17:06:59.438218  190148 start.go:363] duration metric: took 24.28Âµs to acquireMachinesLock for "minikube"
I0206 17:06:59.438225  190148 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0206 17:06:59.438254  190148 start.go:124] createHost starting for "" (driver="docker")
I0206 17:06:59.439858  190148 out.go:252] ðŸ”¥  Creating docker container (CPUs=7, Memory=12288MB) ...
I0206 17:06:59.440259  190148 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 17:06:59.440293  190148 client.go:173] LocalClient.Create starting
I0206 17:06:59.440324  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/ca.pem
I0206 17:06:59.440336  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:06:59.440343  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:06:59.440367  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/cert.pem
I0206 17:06:59.440373  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:06:59.440377  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:06:59.440689  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:06:59.457784  190148 network_create.go:78] Found existing network {name:minikube subnet:0xc001afc9f0 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0206 17:06:59.457803  190148 kic.go:120] calculated static IP "192.168.49.2" for the "minikube" container
I0206 17:06:59.457838  190148 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 17:06:59.473578  190148 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0206 17:06:59.488466  190148 oci.go:102] Successfully created a docker volume minikube
I0206 17:06:59.488506  190148 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 17:07:00.079203  190148 oci.go:106] Successfully prepared a docker volume minikube
I0206 17:07:00.079270  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:07:00.079275  190148 kic.go:193] Starting extracting preloaded images to volume ...
I0206 17:07:00.079306  190148 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 17:07:03.503447  190148 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (3.424104441s)
I0206 17:07:03.503605  190148 kic.go:202] duration metric: took 3.424188154s to extract preloaded images to volume ...
W0206 17:07:03.503710  190148 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 17:07:03.503729  190148 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 17:07:03.503761  190148 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 17:07:03.748578  190148 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=12288mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 17:07:04.273254  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0206 17:07:04.516663  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:04.540563  190148 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0206 17:07:04.670684  190148 oci.go:143] the created container "minikube" has a running status.
I0206 17:07:04.670698  190148 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube/id_rsa...
I0206 17:07:04.729055  190148 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 17:07:04.847682  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:04.868538  190148 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 17:07:04.868545  190148 kic_runner.go:113] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 17:07:04.950415  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:04.972810  190148 machine.go:96] provisionDockerMachine start ...
I0206 17:07:04.972863  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:04.997510  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:04.997673  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:04.997677  190148 main.go:144] libmachine: About to run SSH command:
hostname
I0206 17:07:04.998096  190148 main.go:144] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35646->127.0.0.1:32903: read: connection reset by peer
I0206 17:07:08.150830  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0206 17:07:08.150843  190148 ubuntu.go:182] provisioning hostname "minikube"
I0206 17:07:08.150883  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:08.171148  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:08.171290  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:08.171295  190148 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0206 17:07:08.313296  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0206 17:07:08.313337  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:08.330169  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:08.330284  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:08.330290  190148 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 17:07:08.468714  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 17:07:08.468726  190148 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 17:07:08.468805  190148 ubuntu.go:190] setting up certificates
I0206 17:07:08.468810  190148 provision.go:83] configureAuth start
I0206 17:07:08.468845  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 17:07:08.489364  190148 provision.go:142] copyHostCerts
I0206 17:07:08.489399  190148 exec_runner.go:143] found /home/vagrant/.minikube/cert.pem, removing ...
I0206 17:07:08.489403  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/cert.pem
I0206 17:07:08.489446  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 17:07:08.489670  190148 exec_runner.go:143] found /home/vagrant/.minikube/key.pem, removing ...
I0206 17:07:08.489674  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/key.pem
I0206 17:07:08.489690  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 17:07:08.489725  190148 exec_runner.go:143] found /home/vagrant/.minikube/ca.pem, removing ...
I0206 17:07:08.489726  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/ca.pem
I0206 17:07:08.489736  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 17:07:08.489761  190148 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0206 17:07:08.547189  190148 provision.go:176] copyRemoteCerts
I0206 17:07:08.547224  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 17:07:08.547242  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:08.576017  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:08.681383  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 17:07:08.696897  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0206 17:07:08.712350  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0206 17:07:08.727777  190148 provision.go:86] duration metric: took 258.95946ms to configureAuth
I0206 17:07:08.727789  190148 ubuntu.go:206] setting minikube options for container-runtime
I0206 17:07:08.727883  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:07:08.727907  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:08.745903  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:08.746032  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:08.746035  190148 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 17:07:08.878994  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 17:07:08.879003  190148 ubuntu.go:71] root file system type: overlay
I0206 17:07:08.879062  190148 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 17:07:08.879094  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:08.897902  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:08.898014  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:08.898044  190148 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 17:07:09.051105  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 17:07:09.051145  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:09.067960  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:07:09.068075  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32903 <nil> <nil>}
I0206 17:07:09.068082  190148 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 17:07:10.805747  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 17:07:09.047520695 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 17:07:10.805760  190148 machine.go:99] duration metric: took 5.832941129s to provisionDockerMachine
I0206 17:07:10.805765  190148 client.go:176] duration metric: took 11.365469669s to LocalClient.Create
I0206 17:07:10.805800  190148 start.go:166] duration metric: took 11.365541781s to libmachine.API.Create "minikube"
I0206 17:07:10.805804  190148 start.go:292] postStartSetup for "minikube" (driver="docker")
I0206 17:07:10.805809  190148 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 17:07:10.805838  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 17:07:10.805858  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:10.827032  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:10.926706  190148 ssh_runner.go:194] Run: cat /etc/os-release
I0206 17:07:10.930627  190148 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 17:07:10.930636  190148 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 17:07:10.930641  190148 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 17:07:10.930759  190148 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 17:07:10.930769  190148 start.go:295] duration metric: took 124.962708ms for postStartSetup
I0206 17:07:10.930955  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 17:07:10.954246  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:07:10.954423  190148 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 17:07:10.954458  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:10.974297  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:11.074075  190148 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 17:07:11.080058  190148 start.go:127] duration metric: took 11.641793933s to createHost
I0206 17:07:11.080067  190148 start.go:82] releasing machines lock for "minikube", held for 11.641846178s
I0206 17:07:11.080105  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 17:07:11.100818  190148 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 17:07:11.100793  190148 ssh_runner.go:194] Run: cat /version.json
I0206 17:07:11.100820  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:11.100861  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:11.139183  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:11.139258  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:11.235996  190148 ssh_runner.go:194] Run: systemctl --version
I0206 17:07:11.319212  190148 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0206 17:07:11.326139  190148 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 17:07:11.326172  190148 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 17:07:11.351207  190148 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 17:07:11.351218  190148 start.go:497] detecting cgroup driver to use...
I0206 17:07:11.351239  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:07:11.351300  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:07:11.366879  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 17:07:11.376179  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 17:07:11.386127  190148 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 17:07:11.386151  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 17:07:11.395329  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:07:11.405869  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 17:07:11.417448  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:07:11.427354  190148 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 17:07:11.437240  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 17:07:11.446589  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 17:07:11.457229  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 17:07:11.469077  190148 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 17:07:11.476774  190148 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 17:07:11.483501  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:11.551542  190148 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 17:07:11.637382  190148 start.go:497] detecting cgroup driver to use...
I0206 17:07:11.637409  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:07:11.637434  190148 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 17:07:11.649695  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:07:11.660400  190148 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 17:07:11.702713  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:07:11.713219  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 17:07:11.723289  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:07:11.736989  190148 ssh_runner.go:194] Run: which cri-dockerd
I0206 17:07:11.741199  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 17:07:11.748998  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 17:07:11.761251  190148 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 17:07:11.840608  190148 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 17:07:11.909130  190148 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 17:07:11.909191  190148 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 17:07:11.926447  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 17:07:11.938989  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:12.012307  190148 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 17:07:13.308433  190148 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.296103155s)
I0206 17:07:13.308481  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 17:07:13.322939  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 17:07:13.344969  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:07:13.366807  190148 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 17:07:13.517651  190148 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 17:07:13.611531  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:13.677430  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 17:07:13.691505  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 17:07:13.704148  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:13.864111  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 17:07:13.945019  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:07:13.955487  190148 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 17:07:13.955520  190148 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 17:07:13.959895  190148 start.go:575] Will wait 60s for crictl version
I0206 17:07:13.959973  190148 ssh_runner.go:194] Run: which crictl
I0206 17:07:13.966287  190148 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 17:07:13.997501  190148 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 17:07:13.997541  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:07:14.041624  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:07:14.081318  190148 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 17:07:14.082738  190148 out.go:179]     â–ª opt dns=8.8.8.8
I0206 17:07:14.083579  190148 out.go:179]     â–ª opt dns=8.8.4.4
I0206 17:07:14.084233  190148 out.go:179]     â–ª opt mtu=1500
I0206 17:07:14.085045  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:07:14.108864  190148 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 17:07:14.119661  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:07:14.140156  190148 out.go:179]     â–ª kubelet.housekeeping-interval=10s
I0206 17:07:14.141138  190148 out.go:179]     â–ª kubelet.max-pods=50
I0206 17:07:14.142331  190148 out.go:179]     â–ª kubelet.serialize-image-pulls=true
I0206 17:07:14.143384  190148 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} ...
I0206 17:07:14.143462  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:07:14.143488  190148 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0206 17:07:14.167618  190148 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0206 17:07:14.167625  190148 docker.go:623] Images already preloaded, skipping extraction
I0206 17:07:14.167658  190148 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0206 17:07:14.189138  190148 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0206 17:07:14.189148  190148 cache_images.go:85] Images are preloaded, skipping loading
I0206 17:07:14.189153  190148 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0206 17:07:14.189217  190148 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.2 --serialize-image-pulls=true

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 17:07:14.189247  190148 ssh_runner.go:194] Run: docker info --format {{.CgroupDriver}}
I0206 17:07:14.245522  190148 cni.go:83] Creating CNI manager for ""
I0206 17:07:14.245530  190148 cni.go:135] multinode detected (1 nodes found), recommending kindnet
I0206 17:07:14.245547  190148 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0206 17:07:14.245565  190148 kubeadm.go:196] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0206 17:07:14.245638  190148 kubeadm.go:202] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0206 17:07:14.245675  190148 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 17:07:14.255719  190148 binaries.go:50] Found k8s binaries, skipping transfer
I0206 17:07:14.255745  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0206 17:07:14.262995  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (378 bytes)
I0206 17:07:14.276414  190148 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 17:07:14.294000  190148 ssh_runner.go:361] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0206 17:07:14.309126  190148 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 17:07:14.314602  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:07:14.325918  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:14.419408  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:07:14.437909  190148 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.2
I0206 17:07:14.437917  190148 certs.go:193] generating shared ca certs ...
I0206 17:07:14.437925  190148 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 17:07:14.438006  190148 certs.go:234] skipping valid "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 17:07:14.438026  190148 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 17:07:14.438030  190148 certs.go:255] generating profile certs ...
I0206 17:07:14.438060  190148 certs.go:362] generating signed profile cert for "minikube-user": /home/vagrant/.minikube/profiles/minikube/client.key
I0206 17:07:14.438066  190148 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/client.crt with IP's: []
I0206 17:07:14.502291  190148 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/client.crt ...
I0206 17:07:14.502306  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/client.crt: {Name:mkf790f0c34d5ddb75771609e12fde6153db43d9 Timeout:1m0s Delay:500ms}
I0206 17:07:14.502404  190148 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/client.key ...
I0206 17:07:14.502407  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/client.key: {Name:mk0f0945b480ffce7f464c4edb019923256f817f Timeout:1m0s Delay:500ms}
I0206 17:07:14.502435  190148 certs.go:362] generating signed profile cert for "minikube": /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0206 17:07:14.502440  190148 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0206 17:07:14.534192  190148 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0206 17:07:14.534203  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk6f33b21b8d250aaf51d0fcac514b1fe047019b Timeout:1m0s Delay:500ms}
I0206 17:07:14.534282  190148 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0206 17:07:14.534284  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk5f5ba42dbe6233edabcc1790c1948752e4571e Timeout:1m0s Delay:500ms}
I0206 17:07:14.534306  190148 certs.go:380] copying /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/vagrant/.minikube/profiles/minikube/apiserver.crt
I0206 17:07:14.534340  190148 certs.go:384] copying /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/vagrant/.minikube/profiles/minikube/apiserver.key
I0206 17:07:14.534364  190148 certs.go:362] generating signed profile cert for "aggregator": /home/vagrant/.minikube/profiles/minikube/proxy-client.key
I0206 17:07:14.534369  190148 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0206 17:07:14.571626  190148 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/proxy-client.crt ...
I0206 17:07:14.571635  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/proxy-client.crt: {Name:mk775a814f966808cd93d4d7921f7632223b601d Timeout:1m0s Delay:500ms}
I0206 17:07:14.571701  190148 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/proxy-client.key ...
I0206 17:07:14.571703  190148 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/proxy-client.key: {Name:mk966fe2af01fcb352fc6da315d62a37c8f905e4 Timeout:1m0s Delay:500ms}
I0206 17:07:14.571776  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 17:07:14.571792  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 17:07:14.571805  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 17:07:14.571815  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 17:07:14.572143  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 17:07:14.592650  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 17:07:14.614010  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 17:07:14.631122  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 17:07:14.649137  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0206 17:07:14.670439  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0206 17:07:14.687100  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0206 17:07:14.705136  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0206 17:07:14.725919  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 17:07:14.743518  190148 ssh_runner.go:361] scp memory --> /var/lib/minikube/kubeconfig (722 bytes)
I0206 17:07:14.758635  190148 ssh_runner.go:194] Run: openssl version
I0206 17:07:14.765401  190148 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 17:07:14.772528  190148 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 17:07:14.780357  190148 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 17:07:14.785086  190148 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 17:07:14.785107  190148 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 17:07:14.806374  190148 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 17:07:14.814228  190148 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 17:07:14.822414  190148 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 17:07:14.827141  190148 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 17:07:14.827164  190148 kubeadm.go:400] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 17:07:14.827227  190148 ssh_runner.go:194] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0206 17:07:14.851007  190148 ssh_runner.go:194] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0206 17:07:14.859620  190148 ssh_runner.go:194] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0206 17:07:14.869646  190148 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0206 17:07:14.869671  190148 ssh_runner.go:194] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0206 17:07:14.877557  190148 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0206 17:07:14.877562  190148 kubeadm.go:157] found existing configuration files:

I0206 17:07:14.877580  190148 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0206 17:07:14.886067  190148 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0206 17:07:14.886087  190148 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/admin.conf
I0206 17:07:14.892922  190148 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0206 17:07:14.900337  190148 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0206 17:07:14.900356  190148 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0206 17:07:14.908964  190148 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0206 17:07:14.915986  190148 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0206 17:07:14.916008  190148 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0206 17:07:14.924524  190148 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0206 17:07:14.932890  190148 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0206 17:07:14.932919  190148 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0206 17:07:14.941110  190148 ssh_runner.go:285] Start: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0206 17:07:14.981605  190148 kubeadm.go:318] [init] Using Kubernetes version: v1.34.0
I0206 17:07:14.981634  190148 kubeadm.go:318] [preflight] Running pre-flight checks
I0206 17:07:15.005707  190148 kubeadm.go:318] [preflight] The system verification failed. Printing the output from the verification:
I0206 17:07:15.005744  190148 kubeadm.go:318] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-134-generic[0m
I0206 17:07:15.005761  190148 kubeadm.go:318] [0;37mOS[0m: [0;32mLinux[0m
I0206 17:07:15.005786  190148 kubeadm.go:318] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0206 17:07:15.005838  190148 kubeadm.go:318] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0206 17:07:15.005862  190148 kubeadm.go:318] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0206 17:07:15.005885  190148 kubeadm.go:318] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0206 17:07:15.005908  190148 kubeadm.go:318] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0206 17:07:15.005932  190148 kubeadm.go:318] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0206 17:07:15.005955  190148 kubeadm.go:318] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0206 17:07:15.005982  190148 kubeadm.go:318] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0206 17:07:15.078441  190148 kubeadm.go:318] [preflight] Pulling images required for setting up a Kubernetes cluster
I0206 17:07:15.078522  190148 kubeadm.go:318] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0206 17:07:15.078564  190148 kubeadm.go:318] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0206 17:07:15.091341  190148 kubeadm.go:318] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0206 17:07:15.092530  190148 out.go:252]     â–ª Generating certificates and keys ...
I0206 17:07:15.092765  190148 kubeadm.go:318] [certs] Using existing ca certificate authority
I0206 17:07:15.092802  190148 kubeadm.go:318] [certs] Using existing apiserver certificate and key on disk
I0206 17:07:15.258250  190148 kubeadm.go:318] [certs] Generating "apiserver-kubelet-client" certificate and key
I0206 17:07:15.480686  190148 kubeadm.go:318] [certs] Generating "front-proxy-ca" certificate and key
I0206 17:07:15.700853  190148 kubeadm.go:318] [certs] Generating "front-proxy-client" certificate and key
I0206 17:07:15.777527  190148 kubeadm.go:318] [certs] Generating "etcd/ca" certificate and key
I0206 17:07:15.877656  190148 kubeadm.go:318] [certs] Generating "etcd/server" certificate and key
I0206 17:07:15.877750  190148 kubeadm.go:318] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0206 17:07:16.085455  190148 kubeadm.go:318] [certs] Generating "etcd/peer" certificate and key
I0206 17:07:16.085824  190148 kubeadm.go:318] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0206 17:07:16.264057  190148 kubeadm.go:318] [certs] Generating "etcd/healthcheck-client" certificate and key
I0206 17:07:16.390154  190148 kubeadm.go:318] [certs] Generating "apiserver-etcd-client" certificate and key
I0206 17:07:16.497959  190148 kubeadm.go:318] [certs] Generating "sa" key and public key
I0206 17:07:16.498008  190148 kubeadm.go:318] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0206 17:07:16.776088  190148 kubeadm.go:318] [kubeconfig] Writing "admin.conf" kubeconfig file
I0206 17:07:16.852986  190148 kubeadm.go:318] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0206 17:07:16.947174  190148 kubeadm.go:318] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0206 17:07:17.071258  190148 kubeadm.go:318] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0206 17:07:17.154181  190148 kubeadm.go:318] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0206 17:07:17.154227  190148 kubeadm.go:318] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0206 17:07:17.156040  190148 kubeadm.go:318] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0206 17:07:17.158170  190148 out.go:252]     â–ª Booting up control plane ...
I0206 17:07:17.158289  190148 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0206 17:07:17.158328  190148 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0206 17:07:17.159698  190148 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0206 17:07:17.170733  190148 kubeadm.go:318] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0206 17:07:17.170785  190148 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0206 17:07:17.175866  190148 kubeadm.go:318] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0206 17:07:17.175963  190148 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0206 17:07:17.175981  190148 kubeadm.go:318] [kubelet-start] Starting the kubelet
I0206 17:07:17.255102  190148 kubeadm.go:318] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0206 17:07:17.255160  190148 kubeadm.go:318] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0206 17:07:17.759016  190148 kubeadm.go:318] [kubelet-check] The kubelet is healthy after 503.64686ms
I0206 17:07:17.760821  190148 kubeadm.go:318] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0206 17:07:17.760874  190148 kubeadm.go:318] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0206 17:07:17.760925  190148 kubeadm.go:318] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0206 17:07:17.760970  190148 kubeadm.go:318] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0206 17:07:25.575519  190148 kubeadm.go:318] [control-plane-check] kube-controller-manager is healthy after 7.801839159s
I0206 17:07:28.652191  190148 kubeadm.go:318] [control-plane-check] kube-scheduler is healthy after 10.873971739s
I0206 17:07:30.276912  190148 kubeadm.go:318] [control-plane-check] kube-apiserver is healthy after 12.50417915s
I0206 17:07:30.315097  190148 kubeadm.go:318] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0206 17:07:30.378977  190148 kubeadm.go:318] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0206 17:07:30.396441  190148 kubeadm.go:318] [upload-certs] Skipping phase. Please see --upload-certs
I0206 17:07:30.396545  190148 kubeadm.go:318] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0206 17:07:30.404321  190148 kubeadm.go:318] [bootstrap-token] Using token: xh77xv.9uwjfldt1kxh91kp
I0206 17:07:30.421937  190148 out.go:252]     â–ª Configuring RBAC rules ...
I0206 17:07:30.442279  190148 kubeadm.go:318] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0206 17:07:30.472595  190148 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0206 17:07:30.511301  190148 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0206 17:07:30.515416  190148 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0206 17:07:30.519498  190148 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0206 17:07:30.523734  190148 kubeadm.go:318] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0206 17:07:30.691879  190148 kubeadm.go:318] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0206 17:07:31.465803  190148 kubeadm.go:318] [addons] Applied essential addon: CoreDNS
I0206 17:07:31.884075  190148 kubeadm.go:318] [addons] Applied essential addon: kube-proxy
I0206 17:07:31.884084  190148 kubeadm.go:318] 
I0206 17:07:31.884115  190148 kubeadm.go:318] Your Kubernetes control-plane has initialized successfully!
I0206 17:07:31.884116  190148 kubeadm.go:318] 
I0206 17:07:31.884155  190148 kubeadm.go:318] To start using your cluster, you need to run the following as a regular user:
I0206 17:07:31.884156  190148 kubeadm.go:318] 
I0206 17:07:31.884169  190148 kubeadm.go:318]   mkdir -p $HOME/.kube
I0206 17:07:31.884198  190148 kubeadm.go:318]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0206 17:07:31.884226  190148 kubeadm.go:318]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0206 17:07:31.884228  190148 kubeadm.go:318] 
I0206 17:07:31.884254  190148 kubeadm.go:318] Alternatively, if you are the root user, you can run:
I0206 17:07:31.884256  190148 kubeadm.go:318] 
I0206 17:07:31.884279  190148 kubeadm.go:318]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0206 17:07:31.884280  190148 kubeadm.go:318] 
I0206 17:07:31.884306  190148 kubeadm.go:318] You should now deploy a pod network to the cluster.
I0206 17:07:31.884343  190148 kubeadm.go:318] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0206 17:07:31.884377  190148 kubeadm.go:318]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0206 17:07:31.884393  190148 kubeadm.go:318] 
I0206 17:07:31.884475  190148 kubeadm.go:318] You can now join any number of control-plane nodes by copying certificate authorities
I0206 17:07:31.884518  190148 kubeadm.go:318] and service account keys on each node and then running the following as root:
I0206 17:07:31.884520  190148 kubeadm.go:318] 
I0206 17:07:31.884563  190148 kubeadm.go:318]   kubeadm join control-plane.minikube.internal:8443 --token xh77xv.9uwjfldt1kxh91kp \
I0206 17:07:31.884616  190148 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 \
I0206 17:07:31.884626  190148 kubeadm.go:318] 	--control-plane 
I0206 17:07:31.884627  190148 kubeadm.go:318] 
I0206 17:07:31.884670  190148 kubeadm.go:318] Then you can join any number of worker nodes by running the following on each as root:
I0206 17:07:31.884671  190148 kubeadm.go:318] 
I0206 17:07:31.890481  190148 kubeadm.go:318] kubeadm join control-plane.minikube.internal:8443 --token xh77xv.9uwjfldt1kxh91kp \
I0206 17:07:31.890535  190148 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 
I0206 17:07:31.939088  190148 kubeadm.go:318] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0206 17:07:31.939207  190148 kubeadm.go:318] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-134-generic\n", err: exit status 1
I0206 17:07:31.939258  190148 kubeadm.go:318] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0206 17:07:31.939272  190148 cni.go:83] Creating CNI manager for ""
I0206 17:07:31.939276  190148 cni.go:135] multinode detected (1 nodes found), recommending kindnet
I0206 17:07:31.977019  190148 out.go:179] ðŸ”—  Configuring CNI (Container Networking Interface) ...
I0206 17:07:32.031049  190148 ssh_runner.go:194] Run: stat /opt/cni/bin/portmap
I0206 17:07:32.110571  190148 cni.go:181] applying CNI manifest using /var/lib/minikube/binaries/v1.34.0/kubectl ...
I0206 17:07:32.110579  190148 ssh_runner.go:361] scp memory --> /var/tmp/minikube/cni.yaml (2620 bytes)
I0206 17:07:32.259640  190148 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0206 17:07:33.814224  190148 ssh_runner.go:234] Completed: sudo /var/lib/minikube/binaries/v1.34.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (1.554563881s)
I0206 17:07:33.814261  190148 ssh_runner.go:194] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0206 17:07:33.814674  190148 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0206 17:07:33.814712  190148 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_06T17_07_33_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0206 17:07:33.835233  190148 ops.go:34] apiserver oom_adj: -16
I0206 17:07:34.603107  190148 kubeadm.go:1113] duration metric: took 788.468158ms to wait for elevateKubeSystemPrivileges
I0206 17:07:35.020175  190148 ssh_runner.go:234] Completed: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_06T17_07_33_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.205447088s)
I0206 17:07:35.020195  190148 kubeadm.go:402] duration metric: took 20.193032748s to StartCluster
I0206 17:07:35.020209  190148 settings.go:141] acquiring lock: {Name:mkb2c3059065ecb0ccdda4c7ff3af85f8f0082c2 Timeout:1m0s Delay:500ms}
I0206 17:07:35.020248  190148 settings.go:149] Updating kubeconfig:  /home/vagrant/.kube/config
I0206 17:07:35.020941  190148 lock.go:60] WriteFile acquiring /home/vagrant/.kube/config: {Name:mk584b224ce915a9a9ad34e6e788268489afc021 Timeout:1m0s Delay:500ms}
I0206 17:07:35.021171  190148 start.go:237] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0206 17:07:35.021515  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0206 17:07:35.021608  190148 addons.go:528] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0206 17:07:35.021648  190148 addons.go:71] Setting storage-provisioner=true in profile "minikube"
I0206 17:07:35.021657  190148 addons.go:240] Setting addon storage-provisioner=true in "minikube"
I0206 17:07:35.021674  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:07:35.021693  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:07:35.021711  190148 addons.go:71] Setting default-storageclass=true in profile "minikube"
I0206 17:07:35.021717  190148 addons_storage_classes.go:34] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0206 17:07:35.022198  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:35.022239  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:35.073960  190148 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I0206 17:07:35.154745  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:07:35.507455  190148 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0206 17:07:35.541236  190148 addons.go:437] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0206 17:07:35.541245  190148 ssh_runner.go:361] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0206 17:07:35.541284  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:35.826399  190148 addons.go:240] Setting addon default-storageclass=true in "minikube"
I0206 17:07:35.826425  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:07:35.826676  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:07:35.914753  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:36.516432  190148 addons.go:437] installing /etc/kubernetes/addons/storageclass.yaml
I0206 17:07:36.516442  190148 ssh_runner.go:361] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0206 17:07:36.516475  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:07:36.700227  190148 ssh_runner.go:234] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.678691783s)
I0206 17:07:36.700311  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0206 17:07:36.815464  190148 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0206 17:07:37.248204  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:07:37.461123  190148 ssh_runner.go:234] Completed: sudo systemctl daemon-reload: (2.306360319s)
I0206 17:07:37.461157  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:07:38.084799  190148 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0206 17:07:39.121143  190148 ssh_runner.go:234] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (2.420816983s)
I0206 17:07:39.121159  190148 start.go:989] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0206 17:07:39.704133  190148 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0206 17:07:40.663660  190148 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.848178164s)
I0206 17:07:40.663701  190148 ssh_runner.go:234] Completed: sudo systemctl start kubelet: (3.202539574s)
I0206 17:07:40.664050  190148 api_server.go:51] waiting for apiserver process to appear ...
I0206 17:07:40.664070  190148 ssh_runner.go:194] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0206 17:07:40.664121  190148 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.579313245s)
I0206 17:07:40.790737  190148 api_server.go:71] duration metric: took 5.769512674s to wait for apiserver process to appear ...
I0206 17:07:40.790747  190148 api_server.go:87] waiting for apiserver healthz status ...
I0206 17:07:40.790758  190148 api_server.go:298] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0206 17:07:40.827941  190148 out.go:179] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0206 17:07:40.834155  190148 api_server.go:324] https://192.168.49.2:8443/healthz returned 200:
ok
I0206 17:07:40.851226  190148 api_server.go:140] control plane version: v1.34.0
I0206 17:07:40.853915  190148 api_server.go:130] duration metric: took 63.159601ms to wait for apiserver health ...
I0206 17:07:40.853922  190148 system_pods.go:42] waiting for kube-system pods to appear ...
I0206 17:07:40.856202  190148 addons.go:531] duration metric: took 5.83459609s for enable addons: enabled=[storage-provisioner default-storageclass]
I0206 17:07:40.900296  190148 system_pods.go:58] 8 kube-system pods found
I0206 17:07:40.900317  190148 system_pods.go:60] "coredns-66bc5c9577-8sc9r" [e3d62037-c827-4d83-af20-8dd27878eff1] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0206 17:07:40.900322  190148 system_pods.go:60] "etcd-minikube" [d033ade2-c4db-42c7-b98d-15a42838551e] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0206 17:07:40.900325  190148 system_pods.go:60] "kindnet-kbzfm" [ef168f5d-82c9-4802-8351-9b81b285a9ae] Pending / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0206 17:07:40.900327  190148 system_pods.go:60] "kube-apiserver-minikube" [6a102963-93ea-4ce8-938f-539b035a1b9f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0206 17:07:40.900330  190148 system_pods.go:60] "kube-controller-manager-minikube" [f0986505-1eab-45b7-827f-29c0c71f6c7e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0206 17:07:40.900332  190148 system_pods.go:60] "kube-proxy-bt4jh" [0545b78d-5877-4590-a485-bd9d670b62f5] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0206 17:07:40.900333  190148 system_pods.go:60] "kube-scheduler-minikube" [ad5fc239-4728-4823-8131-b2337c3f6dc7] Running
I0206 17:07:40.900335  190148 system_pods.go:60] "storage-provisioner" [fc4d809a-2bda-479d-bef7-0ef5516ef716] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0206 17:07:40.900339  190148 system_pods.go:73] duration metric: took 46.414441ms to wait for pod list to return data ...
I0206 17:07:40.900345  190148 kubeadm.go:586] duration metric: took 5.879124871s to wait for: map[apiserver:true system_pods:true]
I0206 17:07:40.900352  190148 node_conditions.go:101] verifying NodePressure condition ...
I0206 17:07:40.914901  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:07:40.914912  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:07:40.914921  190148 node_conditions.go:104] duration metric: took 14.566636ms to run NodePressure ...
I0206 17:07:40.914929  190148 start.go:243] waiting for startup goroutines ...
I0206 17:07:40.914933  190148 start.go:248] waiting for cluster config update ...
I0206 17:07:40.914938  190148 start.go:257] writing updated cluster config ...
I0206 17:07:40.920979  190148 out.go:203] 
I0206 17:07:40.924955  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:07:40.925007  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:07:40.926258  190148 out.go:179] ðŸ‘  Starting "minikube-m02" worker node in "minikube" cluster
I0206 17:07:40.927223  190148 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 17:07:40.928273  190148 out.go:179] ðŸšœ  Pulling base image v0.0.49 ...
I0206 17:07:40.929576  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:07:40.929583  190148 cache.go:66] Caching tarball of preloaded images
I0206 17:07:40.929664  190148 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 17:07:40.929665  190148 preload.go:250] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0206 17:07:40.929671  190148 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 17:07:40.929715  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:07:42.104041  190148 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 17:07:42.119745  190148 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 17:07:42.119760  190148 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0206 17:07:42.119762  190148 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0206 17:07:42.119772  190148 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 17:07:42.119775  190148 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 17:08:47.405528  190148 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 17:08:47.405578  190148 cache.go:244] Successfully downloaded all kic artifacts
I0206 17:08:47.405600  190148 start.go:359] acquireMachinesLock for minikube-m02: {Name:mk7681bfb3a3fdfa52a372a04630919dda81469f Timeout:10m0s Delay:500ms}
I0206 17:08:47.405633  190148 start.go:363] duration metric: took 24.952Âµs to acquireMachinesLock for "minikube-m02"
I0206 17:08:47.405641  190148 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name:m02 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:08:47.405683  190148 start.go:124] createHost starting for "m02" (driver="docker")
I0206 17:08:47.406892  190148 out.go:252] ðŸ”¥  Creating docker container (CPUs=7, Memory=12288MB) ...
I0206 17:08:47.406990  190148 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 17:08:47.407003  190148 client.go:173] LocalClient.Create starting
I0206 17:08:47.407047  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/ca.pem
I0206 17:08:47.407060  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:08:47.407193  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:08:47.407224  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/cert.pem
I0206 17:08:47.407232  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:08:47.407237  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:08:47.407341  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:08:47.427500  190148 network_create.go:78] Found existing network {name:minikube subnet:0xc001b814d0 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0206 17:08:47.427520  190148 kic.go:120] calculated static IP "192.168.49.3" for the "minikube-m02" container
I0206 17:08:47.427552  190148 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 17:08:47.462911  190148 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I0206 17:08:47.486960  190148 oci.go:102] Successfully created a docker volume minikube-m02
I0206 17:08:47.487002  190148 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 17:08:48.734451  190148 cli_runner.go:217] Completed: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (1.247415519s)
I0206 17:08:48.734464  190148 oci.go:106] Successfully prepared a docker volume minikube-m02
I0206 17:08:48.734490  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:08:48.734496  190148 kic.go:193] Starting extracting preloaded images to volume ...
I0206 17:08:48.734524  190148 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 17:08:54.232524  190148 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (5.497977303s)
I0206 17:08:54.232539  190148 kic.go:202] duration metric: took 5.498041004s to extract preloaded images to volume ...
W0206 17:08:54.232593  190148 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 17:08:54.232606  190148 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 17:08:54.232627  190148 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 17:08:54.349454  190148 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=12288mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 17:08:55.360502  190148 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=12288mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945: (1.011012666s)
I0206 17:08:55.360590  190148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I0206 17:08:55.644094  190148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 17:08:55.869474  190148 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I0206 17:08:56.239945  190148 oci.go:143] the created container "minikube-m02" has a running status.
I0206 17:08:56.239962  190148 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube-m02/id_rsa...
I0206 17:08:56.352230  190148 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 17:08:56.818969  190148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 17:08:57.138822  190148 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 17:08:57.138829  190148 kic_runner.go:113] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 17:08:58.078666  190148 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 17:08:58.716201  190148 machine.go:96] provisionDockerMachine start ...
I0206 17:08:58.716252  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:08:59.603737  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:08:59.603937  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:08:59.603943  190148 main.go:144] libmachine: About to run SSH command:
hostname
I0206 17:09:00.578377  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0206 17:09:00.578390  190148 ubuntu.go:182] provisioning hostname "minikube-m02"
I0206 17:09:00.578429  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:00.601398  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:09:00.601526  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:09:00.601531  190148 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0206 17:09:01.324224  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0206 17:09:01.324264  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:02.036411  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:09:02.040682  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:09:02.040692  190148 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 17:09:02.780308  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 17:09:02.780320  190148 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 17:09:02.780327  190148 ubuntu.go:190] setting up certificates
I0206 17:09:02.780333  190148 provision.go:83] configureAuth start
I0206 17:09:02.780365  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 17:09:03.202967  190148 provision.go:142] copyHostCerts
I0206 17:09:03.203002  190148 exec_runner.go:143] found /home/vagrant/.minikube/ca.pem, removing ...
I0206 17:09:03.203011  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/ca.pem
I0206 17:09:03.203044  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 17:09:03.203130  190148 exec_runner.go:143] found /home/vagrant/.minikube/cert.pem, removing ...
I0206 17:09:03.203137  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/cert.pem
I0206 17:09:03.203161  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 17:09:03.203204  190148 exec_runner.go:143] found /home/vagrant/.minikube/key.pem, removing ...
I0206 17:09:03.203206  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/key.pem
I0206 17:09:03.203216  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 17:09:03.203243  190148 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube-m02 san=[127.0.0.1 192.168.49.3 localhost minikube minikube-m02]
I0206 17:09:03.240628  190148 provision.go:176] copyRemoteCerts
I0206 17:09:03.240666  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 17:09:03.240696  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:03.847004  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32908 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 17:09:04.413116  190148 ssh_runner.go:234] Completed: sudo mkdir -p /etc/docker /etc/docker /etc/docker: (1.172435462s)
I0206 17:09:04.414599  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 17:09:04.585854  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0206 17:09:04.720749  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0206 17:09:04.900470  190148 provision.go:86] duration metric: took 2.120127964s to configureAuth
I0206 17:09:04.900529  190148 ubuntu.go:206] setting minikube options for container-runtime
I0206 17:09:04.904110  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:09:04.904155  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:05.410375  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:09:05.410506  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:09:05.410511  190148 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 17:09:06.324376  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 17:09:06.324384  190148 ubuntu.go:71] root file system type: overlay
I0206 17:09:06.325332  190148 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 17:09:06.325377  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:06.657888  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:09:06.658020  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:09:06.658056  190148 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 17:09:07.122001  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 17:09:07.122045  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:07.143697  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:09:07.143827  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32908 <nil> <nil>}
I0206 17:09:07.143834  190148 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 17:09:09.343840  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 17:09:07.113257706 +0000
@@ -9,23 +9,35 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+Environment=NO_PROXY=192.168.49.2
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 17:09:09.343853  190148 machine.go:99] duration metric: took 10.627643553s to provisionDockerMachine
I0206 17:09:09.343858  190148 client.go:176] duration metric: took 21.936852792s to LocalClient.Create
I0206 17:09:09.343866  190148 start.go:166] duration metric: took 21.936876555s to libmachine.API.Create "minikube"
I0206 17:09:09.343869  190148 start.go:292] postStartSetup for "minikube-m02" (driver="docker")
I0206 17:09:09.343875  190148 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 17:09:09.343907  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 17:09:09.343926  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:09.366940  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32908 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 17:09:09.468643  190148 ssh_runner.go:194] Run: cat /etc/os-release
I0206 17:09:09.473078  190148 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 17:09:09.473086  190148 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 17:09:09.473092  190148 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 17:09:09.473123  190148 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 17:09:09.473131  190148 start.go:295] duration metric: took 129.258779ms for postStartSetup
I0206 17:09:09.473332  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 17:09:09.491590  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:09:09.491760  190148 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 17:09:09.491778  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:09.510125  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32908 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 17:09:09.632872  190148 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 17:09:09.638278  190148 start.go:127] duration metric: took 22.232586662s to createHost
I0206 17:09:09.638287  190148 start.go:82] releasing machines lock for "minikube-m02", held for 22.232651035s
I0206 17:09:09.638329  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 17:09:09.678803  190148 out.go:179] ðŸŒ  Found network options:
I0206 17:09:09.695291  190148 out.go:179]     â–ª NO_PROXY=192.168.49.2
W0206 17:09:09.697082  190148 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 17:09:09.697166  190148 proxy.go:121] fail to check proxy env: Error ip not in block
I0206 17:09:09.697240  190148 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0206 17:09:09.697305  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:09.697361  190148 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 17:09:09.697394  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 17:09:09.823892  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32908 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 17:09:09.831833  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32908 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
W0206 17:09:09.932064  190148 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 17:09:09.932146  190148 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 17:09:10.010257  190148 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 17:09:10.010268  190148 start.go:497] detecting cgroup driver to use...
I0206 17:09:10.010291  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:09:10.010346  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:09:10.026456  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 17:09:10.035651  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 17:09:10.045657  190148 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 17:09:10.045690  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 17:09:10.054425  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:09:10.063365  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 17:09:10.071630  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:09:10.086686  190148 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 17:09:10.098114  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 17:09:10.109192  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 17:09:10.118122  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 17:09:10.127321  190148 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 17:09:10.135061  190148 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 17:09:10.142907  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:10.236336  190148 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 17:09:10.374876  190148 start.go:497] detecting cgroup driver to use...
I0206 17:09:10.374904  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:09:10.374930  190148 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 17:09:10.388294  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:09:10.399196  190148 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 17:09:10.434277  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:09:10.446388  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 17:09:10.458732  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:09:10.475409  190148 ssh_runner.go:194] Run: which cri-dockerd
I0206 17:09:10.479967  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 17:09:10.487621  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 17:09:10.501701  190148 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 17:09:10.599991  190148 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 17:09:10.778810  190148 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 17:09:10.778838  190148 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 17:09:10.791112  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 17:09:10.826534  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:10.912321  190148 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 17:09:12.448095  190148 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.535758477s)
I0206 17:09:12.448127  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 17:09:12.459844  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 17:09:12.474316  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:09:12.485528  190148 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 17:09:12.558889  190148 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 17:09:12.811339  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:12.936945  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 17:09:12.948452  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 17:09:12.958740  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:13.078433  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 17:09:13.199697  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:09:13.220243  190148 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 17:09:13.220275  190148 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 17:09:13.226971  190148 start.go:575] Will wait 60s for crictl version
I0206 17:09:13.226999  190148 ssh_runner.go:194] Run: which crictl
I0206 17:09:13.231327  190148 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 17:09:13.263749  190148 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 17:09:13.263795  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:09:13.293568  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:09:13.322955  190148 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 17:09:13.324147  190148 out.go:179]     â–ª opt dns=8.8.8.8
I0206 17:09:13.325529  190148 out.go:179]     â–ª opt dns=8.8.4.4
I0206 17:09:13.326251  190148 out.go:179]     â–ª opt mtu=1500
I0206 17:09:13.327042  190148 out.go:179]     â–ª env NO_PROXY=192.168.49.2
I0206 17:09:13.327907  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:09:13.375338  190148 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 17:09:13.379911  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:09:13.398289  190148 mustload.go:66] Loading cluster: minikube
I0206 17:09:13.398397  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:09:13.398502  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:09:13.423755  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:09:13.424001  190148 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.3
I0206 17:09:13.424005  190148 certs.go:193] generating shared ca certs ...
I0206 17:09:13.424014  190148 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 17:09:13.424101  190148 certs.go:234] skipping valid "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 17:09:13.424142  190148 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 17:09:13.424209  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 17:09:13.424232  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 17:09:13.424252  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 17:09:13.424270  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 17:09:13.424304  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 17:09:13.449065  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 17:09:13.474246  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 17:09:13.495849  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 17:09:13.517562  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 17:09:13.534908  190148 ssh_runner.go:194] Run: openssl version
I0206 17:09:13.592393  190148 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 17:09:13.605356  190148 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 17:09:13.612695  190148 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 17:09:13.619167  190148 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 17:09:13.619186  190148 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 17:09:13.639481  190148 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 17:09:13.646954  190148 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 17:09:13.663577  190148 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 17:09:13.667597  190148 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 17:09:13.667615  190148 kubeadm.go:934] updating node {m02 192.168.49.3 8443 v1.34.0 docker false true} ...
I0206 17:09:13.667725  190148 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m02 --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.3 --serialize-image-pulls=true

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 17:09:13.667755  190148 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 17:09:13.675486  190148 binaries.go:50] Found k8s binaries, skipping transfer
I0206 17:09:13.675508  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0206 17:09:13.684252  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (382 bytes)
I0206 17:09:13.697091  190148 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 17:09:13.709288  190148 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 17:09:13.714264  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:09:13.725671  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:13.843870  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:09:13.857427  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:09:13.857546  190148 start.go:319] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 17:09:13.857596  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0206 17:09:13.857615  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:09:13.947153  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:09:14.151178  190148 start.go:345] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:09:14.151219  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 2downs.jegrsbdspch824au --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m02"
I0206 17:09:15.751128  190148 ssh_runner.go:234] Completed: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 2downs.jegrsbdspch824au --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m02": (1.599883888s)
I0206 17:09:15.751171  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "systemctl daemon-reload && systemctl enable kubelet && systemctl start kubelet"
I0206 17:09:16.339241  190148 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m02 minikube.k8s.io/updated_at=2026_02_06T17_09_16_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0206 17:09:16.935206  190148 start.go:321] duration metric: took 3.077655675s to joinCluster
I0206 17:09:16.935258  190148 start.go:237] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:09:16.935376  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:09:16.963002  190148 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I0206 17:09:16.988310  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:09:17.091082  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:09:17.106462  190148 kubeadm.go:586] duration metric: took 171.187847ms to wait for: map[apiserver:true system_pods:true]
I0206 17:09:17.106474  190148 node_conditions.go:101] verifying NodePressure condition ...
I0206 17:09:17.113557  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:09:17.113566  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:09:17.113573  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:09:17.113574  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:09:17.113576  190148 node_conditions.go:104] duration metric: took 7.099248ms to run NodePressure ...
I0206 17:09:17.113582  190148 start.go:243] waiting for startup goroutines ...
I0206 17:09:17.113596  190148 start.go:257] writing updated cluster config ...
I0206 17:09:17.118127  190148 out.go:203] 
I0206 17:09:17.119708  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:09:17.119758  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:09:17.121663  190148 out.go:179] ðŸ‘  Starting "minikube-m03" worker node in "minikube" cluster
I0206 17:09:17.123179  190148 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 17:09:17.124687  190148 out.go:179] ðŸšœ  Pulling base image v0.0.49 ...
I0206 17:09:17.125844  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:09:17.125852  190148 cache.go:66] Caching tarball of preloaded images
I0206 17:09:17.125962  190148 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 17:09:17.125947  190148 preload.go:250] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0206 17:09:17.125953  190148 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 17:09:17.126010  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:09:18.185849  190148 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 17:09:18.185989  190148 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 17:09:18.185997  190148 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0206 17:09:18.186000  190148 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0206 17:09:18.186005  190148 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 17:09:18.186007  190148 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 17:10:29.817646  190148 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 17:10:29.817698  190148 cache.go:244] Successfully downloaded all kic artifacts
I0206 17:10:29.817721  190148 start.go:359] acquireMachinesLock for minikube-m03: {Name:mk288658bee5399c456495e25cfaba36322fce43 Timeout:10m0s Delay:500ms}
I0206 17:10:29.817753  190148 start.go:363] duration metric: took 23.789Âµs to acquireMachinesLock for "minikube-m03"
I0206 17:10:29.817760  190148 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name:m03 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:10:29.817818  190148 start.go:124] createHost starting for "m03" (driver="docker")
I0206 17:10:29.818774  190148 out.go:252] ðŸ”¥  Creating docker container (CPUs=7, Memory=12288MB) ...
I0206 17:10:29.818897  190148 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 17:10:29.818909  190148 client.go:173] LocalClient.Create starting
I0206 17:10:29.818939  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/ca.pem
I0206 17:10:29.818976  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:10:29.818985  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:10:29.819013  190148 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/cert.pem
I0206 17:10:29.819019  190148 main.go:144] libmachine: Decoding PEM data...
I0206 17:10:29.819024  190148 main.go:144] libmachine: Parsing certificate...
I0206 17:10:29.819192  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:10:29.839677  190148 network_create.go:78] Found existing network {name:minikube subnet:0xc001f63770 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0206 17:10:29.839697  190148 kic.go:120] calculated static IP "192.168.49.4" for the "minikube-m03" container
I0206 17:10:29.839729  190148 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 17:10:29.918124  190148 cli_runner.go:164] Run: docker volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I0206 17:10:29.936827  190148 oci.go:102] Successfully created a docker volume minikube-m03
I0206 17:10:29.936871  190148 cli_runner.go:164] Run: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 17:10:31.261415  190148 cli_runner.go:217] Completed: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (1.324518615s)
I0206 17:10:31.261433  190148 oci.go:106] Successfully prepared a docker volume minikube-m03
I0206 17:10:31.261459  190148 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 17:10:31.261464  190148 kic.go:193] Starting extracting preloaded images to volume ...
I0206 17:10:31.261495  190148 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 17:10:37.032246  190148 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (5.77072975s)
I0206 17:10:37.032261  190148 kic.go:202] duration metric: took 5.770793852s to extract preloaded images to volume ...
W0206 17:10:37.032337  190148 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 17:10:37.032353  190148 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 17:10:37.032372  190148 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 17:10:37.245901  190148 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=12288mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 17:10:38.561935  190148 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=12288mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945: (1.316002423s)
I0206 17:10:38.561975  190148 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Running}}
I0206 17:10:39.153245  190148 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 17:10:39.447370  190148 cli_runner.go:164] Run: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I0206 17:10:40.509580  190148 cli_runner.go:217] Completed: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables: (1.062185596s)
I0206 17:10:40.509593  190148 oci.go:143] the created container "minikube-m03" has a running status.
I0206 17:10:40.509600  190148 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube-m03/id_rsa...
I0206 17:10:40.583735  190148 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 17:10:41.783472  190148 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 17:10:42.351771  190148 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 17:10:42.351779  190148 kic_runner.go:113] Args: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 17:10:43.944080  190148 kic_runner.go:122] Done: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]: (1.592284709s)
I0206 17:10:43.944135  190148 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 17:10:44.282243  190148 machine.go:96] provisionDockerMachine start ...
I0206 17:10:44.282290  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:44.344942  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:44.345086  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:44.345099  190148 main.go:144] libmachine: About to run SSH command:
hostname
I0206 17:10:44.643620  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0206 17:10:44.643633  190148 ubuntu.go:182] provisioning hostname "minikube-m03"
I0206 17:10:44.643673  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:45.011602  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:45.011736  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:45.011741  190148 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0206 17:10:45.392847  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0206 17:10:45.392893  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:45.655406  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:45.655534  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:45.655541  190148 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 17:10:46.257844  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 17:10:46.257856  190148 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 17:10:46.257864  190148 ubuntu.go:190] setting up certificates
I0206 17:10:46.257868  190148 provision.go:83] configureAuth start
I0206 17:10:46.257903  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 17:10:46.306383  190148 provision.go:142] copyHostCerts
I0206 17:10:46.306418  190148 exec_runner.go:143] found /home/vagrant/.minikube/ca.pem, removing ...
I0206 17:10:46.306423  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/ca.pem
I0206 17:10:46.306472  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 17:10:46.306521  190148 exec_runner.go:143] found /home/vagrant/.minikube/cert.pem, removing ...
I0206 17:10:46.306523  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/cert.pem
I0206 17:10:46.306534  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 17:10:46.306555  190148 exec_runner.go:143] found /home/vagrant/.minikube/key.pem, removing ...
I0206 17:10:46.306556  190148 exec_runner.go:202] rm: /home/vagrant/.minikube/key.pem
I0206 17:10:46.306564  190148 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 17:10:46.306585  190148 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube-m03 san=[127.0.0.1 192.168.49.4 localhost minikube minikube-m03]
I0206 17:10:46.315984  190148 provision.go:176] copyRemoteCerts
I0206 17:10:46.316013  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 17:10:46.316031  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:46.337614  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32913 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 17:10:46.434777  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 17:10:46.454188  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0206 17:10:46.472320  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0206 17:10:46.489290  190148 provision.go:86] duration metric: took 231.413776ms to configureAuth
I0206 17:10:46.489301  190148 ubuntu.go:206] setting minikube options for container-runtime
I0206 17:10:46.489423  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:10:46.489447  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:46.544234  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:46.544361  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:46.544365  190148 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 17:10:46.737241  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 17:10:46.737250  190148 ubuntu.go:71] root file system type: overlay
I0206 17:10:46.737339  190148 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 17:10:46.737378  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:46.915087  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:46.915209  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:46.915248  190148 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 17:10:47.063371  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 17:10:47.063414  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:47.125818  190148 main.go:144] libmachine: Using SSH client type: native
I0206 17:10:47.126004  190148 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32913 <nil> <nil>}
I0206 17:10:47.126012  190148 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 17:10:49.594872  190148 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 17:10:47.058709379 +0000
@@ -9,23 +9,36 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 --dns=8.8.8.8 --dns=8.8.4.4 --mtu=1500 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 17:10:49.594890  190148 machine.go:99] duration metric: took 5.312640038s to provisionDockerMachine
I0206 17:10:49.594897  190148 client.go:176] duration metric: took 19.775986486s to LocalClient.Create
I0206 17:10:49.594939  190148 start.go:166] duration metric: took 19.776042079s to libmachine.API.Create "minikube"
I0206 17:10:49.594944  190148 start.go:292] postStartSetup for "minikube-m03" (driver="docker")
I0206 17:10:49.594949  190148 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 17:10:49.594984  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 17:10:49.595003  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:49.625989  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32913 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 17:10:49.734567  190148 ssh_runner.go:194] Run: cat /etc/os-release
I0206 17:10:49.739800  190148 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 17:10:49.739808  190148 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 17:10:49.739814  190148 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 17:10:49.739846  190148 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 17:10:49.739854  190148 start.go:295] duration metric: took 144.90784ms for postStartSetup
I0206 17:10:49.740055  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 17:10:49.823653  190148 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 17:10:49.823871  190148 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 17:10:49.823890  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:49.844157  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32913 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 17:10:49.947882  190148 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 17:10:49.954593  190148 start.go:127] duration metric: took 20.136768078s to createHost
I0206 17:10:49.954603  190148 start.go:82] releasing machines lock for "minikube-m03", held for 20.13684671s
I0206 17:10:49.954642  190148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 17:10:49.975729  190148 out.go:179] ðŸŒ  Found network options:
I0206 17:10:49.976714  190148 out.go:179]     â–ª NO_PROXY=192.168.49.2,192.168.49.3
W0206 17:10:49.977878  190148 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 17:10:49.977884  190148 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 17:10:49.977919  190148 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 17:10:49.977921  190148 proxy.go:121] fail to check proxy env: Error ip not in block
I0206 17:10:49.977969  190148 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0206 17:10:49.977987  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:49.978045  190148 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 17:10:49.978071  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 17:10:50.280417  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32913 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 17:10:50.380116  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32913 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
W0206 17:10:50.471887  190148 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 17:10:50.471921  190148 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 17:10:50.580988  190148 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 17:10:50.581000  190148 start.go:497] detecting cgroup driver to use...
I0206 17:10:50.581023  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:10:50.581080  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:10:50.598366  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 17:10:50.607725  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 17:10:50.617604  190148 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 17:10:50.617635  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 17:10:50.629643  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:10:50.638818  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 17:10:50.649269  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 17:10:50.667576  190148 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 17:10:50.677155  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 17:10:50.686720  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 17:10:50.695856  190148 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 17:10:50.706375  190148 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 17:10:50.714214  190148 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 17:10:50.722432  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:50.883840  190148 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 17:10:50.969725  190148 start.go:497] detecting cgroup driver to use...
I0206 17:10:50.969752  190148 detect.go:178] detected "systemd" cgroup driver on host os
I0206 17:10:50.969777  190148 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 17:10:50.986967  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:10:50.999006  190148 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 17:10:51.012727  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 17:10:51.024004  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 17:10:51.035640  190148 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 17:10:51.049812  190148 ssh_runner.go:194] Run: which cri-dockerd
I0206 17:10:51.054387  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 17:10:51.073409  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 17:10:51.091281  190148 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 17:10:51.163103  190148 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 17:10:51.243891  190148 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 17:10:51.243915  190148 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 17:10:51.257787  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 17:10:51.268295  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:51.367690  190148 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 17:10:52.837478  190148 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.469770071s)
I0206 17:10:52.837513  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 17:10:52.850924  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 17:10:52.861827  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:10:52.872256  190148 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 17:10:52.960377  190148 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 17:10:53.041037  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:53.110168  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 17:10:53.122239  190148 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 17:10:53.131707  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:53.204873  190148 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 17:10:53.290012  190148 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 17:10:53.323759  190148 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 17:10:53.323792  190148 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 17:10:53.328428  190148 start.go:575] Will wait 60s for crictl version
I0206 17:10:53.328452  190148 ssh_runner.go:194] Run: which crictl
I0206 17:10:53.333531  190148 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 17:10:53.434892  190148 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 17:10:53.434929  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:10:53.463805  190148 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 17:10:53.505944  190148 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 17:10:53.532638  190148 out.go:179]     â–ª opt dns=8.8.8.8
I0206 17:10:53.533876  190148 out.go:179]     â–ª opt dns=8.8.4.4
I0206 17:10:53.535075  190148 out.go:179]     â–ª opt mtu=1500
I0206 17:10:53.535935  190148 out.go:179]     â–ª env NO_PROXY=192.168.49.2
I0206 17:10:53.536981  190148 out.go:179]     â–ª env NO_PROXY=192.168.49.2,192.168.49.3
I0206 17:10:53.538255  190148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 17:10:53.831771  190148 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 17:10:53.909662  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:10:53.922725  190148 mustload.go:66] Loading cluster: minikube
I0206 17:10:53.922853  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:10:53.922960  190148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 17:10:54.070790  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:10:54.070931  190148 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.4
I0206 17:10:54.070934  190148 certs.go:193] generating shared ca certs ...
I0206 17:10:54.070941  190148 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 17:10:54.070992  190148 certs.go:234] skipping valid "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 17:10:54.071010  190148 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 17:10:54.071048  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 17:10:54.071061  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 17:10:54.071075  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 17:10:54.071085  190148 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 17:10:54.071110  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 17:10:54.110082  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 17:10:54.126347  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 17:10:54.142948  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 17:10:54.159444  190148 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 17:10:54.176359  190148 ssh_runner.go:194] Run: openssl version
I0206 17:10:54.181741  190148 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 17:10:54.192898  190148 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 17:10:54.202424  190148 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 17:10:54.207235  190148 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 17:10:54.207255  190148 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 17:10:54.229897  190148 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 17:10:54.238569  190148 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 17:10:54.247153  190148 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 17:10:54.251764  190148 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 17:10:54.251783  190148 kubeadm.go:934] updating node {m03 192.168.49.4 8443 v1.34.0 docker false true} ...
I0206 17:10:54.251835  190148 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m03 --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.4 --serialize-image-pulls=true

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 17:10:54.251866  190148 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 17:10:54.263314  190148 binaries.go:50] Found k8s binaries, skipping transfer
I0206 17:10:54.263342  190148 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0206 17:10:54.271368  190148 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (382 bytes)
I0206 17:10:54.284037  190148 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 17:10:54.296125  190148 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 17:10:54.300924  190148 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 17:10:54.312953  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:54.386121  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:10:54.398417  190148 host.go:67] Checking if "minikube" exists ...
I0206 17:10:54.398902  190148 start.go:319] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12288 CPUs:7 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[dns=8.8.8.8 dns=8.8.4.4 mtu=1500] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:true}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 17:10:54.398975  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0206 17:10:54.398992  190148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 17:10:54.579595  190148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32903 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 17:10:54.749643  190148 start.go:345] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:10:54.749679  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token lrb885.sqp6dswj6qsmmdd3 --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m03"
I0206 17:10:55.747303  190148 ssh_runner.go:194] Run: sudo /bin/bash -c "systemctl daemon-reload && systemctl enable kubelet && systemctl start kubelet"
I0206 17:10:56.431189  190148 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m03 minikube.k8s.io/updated_at=2026_02_06T17_10_56_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0206 17:10:56.898994  190148 start.go:321] duration metric: took 2.500086513s to joinCluster
I0206 17:10:56.899033  190148 start.go:237] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 17:10:56.910736  190148 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 17:10:56.956585  190148 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I0206 17:10:56.991615  190148 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 17:10:57.697189  190148 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 17:10:57.838336  190148 kubeadm.go:586] duration metric: took 939.287576ms to wait for: map[apiserver:true system_pods:true]
I0206 17:10:57.838349  190148 node_conditions.go:101] verifying NodePressure condition ...
I0206 17:10:57.845702  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:10:57.845710  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:10:57.845717  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:10:57.845718  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:10:57.845719  190148 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 17:10:57.845720  190148 node_conditions.go:122] node cpu capacity is 18
I0206 17:10:57.845723  190148 node_conditions.go:104] duration metric: took 7.371847ms to run NodePressure ...
I0206 17:10:57.845729  190148 start.go:243] waiting for startup goroutines ...
I0206 17:10:57.845742  190148 start.go:257] writing updated cluster config ...
I0206 17:10:57.845926  190148 ssh_runner.go:194] Run: rm -f paused
I0206 17:10:59.015701  190148 start.go:629] kubectl: 1.28.15, cluster: 1.34.0 (minor skew: 6)
I0206 17:10:59.064538  190148 out.go:203] 
W0206 17:10:59.110787  190148 out.go:285] â—  /usr/bin/kubectl is version 1.28.15, which may have incompatibilities with Kubernetes 1.34.0.
I0206 17:10:59.131968  190148 out.go:179]     â–ª Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I0206 17:10:59.154771  190148 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 06 17:07:13 minikube cri-dockerd[1338]: time="2026-02-06T17:07:13Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 06 17:07:13 minikube cri-dockerd[1338]: time="2026-02-06T17:07:13Z" level=info msg="Setting cgroupDriver systemd"
Feb 06 17:07:13 minikube cri-dockerd[1338]: time="2026-02-06T17:07:13Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 06 17:07:13 minikube cri-dockerd[1338]: time="2026-02-06T17:07:13Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 06 17:07:13 minikube cri-dockerd[1338]: time="2026-02-06T17:07:13Z" level=info msg="Start cri-dockerd grpc backend"
Feb 06 17:07:13 minikube systemd[1]: Started cri-docker.service - CRI Interface for Docker Application Container Engine.
Feb 06 17:07:19 minikube dockerd[1049]: time="2026-02-06T17:07:19.255403406Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=f76c2e48cc9d ep=k8s_POD_kube-apiserver-minikube_kube-system_8312b4cdc4b705c0e12f63794469cfad_0 net=host nid=da3e48509f4a
Feb 06 17:07:19 minikube cri-dockerd[1338]: time="2026-02-06T17:07:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/13ad54fc3440f4b7c7d8c74778009b6025b771f9d44f9a4617275a61e7700ee5/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:07:20 minikube dockerd[1049]: time="2026-02-06T17:07:20.181625885Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=9b9da348143d ep=k8s_POD_kube-scheduler-minikube_kube-system_dc6cf0a7bcb54d1f95cecc4d7b6b7d67_0 net=host nid=da3e48509f4a
Feb 06 17:07:20 minikube dockerd[1049]: time="2026-02-06T17:07:20.193359092Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=54284cebc87a ep=k8s_POD_kube-controller-manager-minikube_kube-system_3b51c8241e224d47681cce32ea99b407_0 net=host nid=da3e48509f4a
Feb 06 17:07:20 minikube dockerd[1049]: time="2026-02-06T17:07:20.504224576Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=e353fc34be76 ep=k8s_POD_etcd-minikube_kube-system_e3a36fac0ae701bc11fad0a6716eec2c_0 net=host nid=da3e48509f4a
Feb 06 17:07:20 minikube cri-dockerd[1338]: time="2026-02-06T17:07:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9bed051a3439b5f701291ee518f6484a25fd9399f8a4154fb216477e9bcd9c71/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:07:20 minikube cri-dockerd[1338]: time="2026-02-06T17:07:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/075f6e10fa5f4c5a16c0ff7d2997c85bf421f0bbc9484d30a0395cd89647a92c/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:07:20 minikube cri-dockerd[1338]: time="2026-02-06T17:07:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8a64192ec24240e0bc612db4402dcca593afc0feb4b0c0cb8fcb8eaa4abfafb2/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:07:35 minikube cri-dockerd[1338]: time="2026-02-06T17:07:35Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 06 17:07:40 minikube dockerd[1049]: time="2026-02-06T17:07:40.764662998Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=5c443f0dba61 ep=k8s_POD_kube-proxy-bt4jh_kube-system_0545b78d-5877-4590-a485-bd9d670b62f5_0 net=host nid=da3e48509f4a
Feb 06 17:07:40 minikube cri-dockerd[1338]: time="2026-02-06T17:07:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e03f71b1a68ec0783b83a920114a669187c1529930c17cc1ba9a59cef3e1f4a6/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:07:43 minikube dockerd[1049]: time="2026-02-06T17:07:43.352674349Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=95b5bdbe48d3 ep=k8s_POD_kindnet-kbzfm_kube-system_ef168f5d-82c9-4802-8351-9b81b285a9ae_0 net=host nid=da3e48509f4a
Feb 06 17:07:43 minikube cri-dockerd[1338]: time="2026-02-06T17:07:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/244b71b3d70bcfefa8cd8703234d0f2bb023279a96e81c0ad3d8b8ada20d24bc/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Feb 06 17:07:55 minikube cri-dockerd[1338]: time="2026-02-06T17:07:55Z" level=info msg="Pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: 26d8d89fd7a0: Downloading [=========>                                         ]  5.632MB/31.09MB"
Feb 06 17:08:05 minikube cri-dockerd[1338]: time="2026-02-06T17:08:05Z" level=info msg="Pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: 26d8d89fd7a0: Downloading [===================>                               ]  11.96MB/31.09MB"
Feb 06 17:08:15 minikube cri-dockerd[1338]: time="2026-02-06T17:08:15Z" level=info msg="Pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: 26d8d89fd7a0: Downloading [======================================>            ]  23.94MB/31.09MB"
Feb 06 17:08:25 minikube cri-dockerd[1338]: time="2026-02-06T17:08:25Z" level=info msg="Pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: 26d8d89fd7a0: Extracting [===>                                               ]  2.294MB/31.09MB"
Feb 06 17:08:35 minikube cri-dockerd[1338]: time="2026-02-06T17:08:35Z" level=info msg="Pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: 26d8d89fd7a0: Extracting [=======================>                           ]  14.42MB/31.09MB"
Feb 06 17:08:43 minikube cri-dockerd[1338]: time="2026-02-06T17:08:43Z" level=info msg="Stop pulling image docker.io/kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88: Status: Downloaded newer image for kindest/kindnetd:v20251212-v0.29.0-alpha-105-g20ccfc88"
Feb 06 17:09:02 minikube dockerd[1049]: time="2026-02-06T17:09:02.687528710Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=f0853063ba81 ep=k8s_POD_storage-provisioner_kube-system_fc4d809a-2bda-479d-bef7-0ef5516ef716_0 net=host nid=da3e48509f4a
Feb 06 17:09:03 minikube dockerd[1049]: time="2026-02-06T17:09:03.175363774Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=f972788685df ep=k8s_POD_coredns-66bc5c9577-8sc9r_kube-system_e3d62037-c827-4d83-af20-8dd27878eff1_0 net=none nid=a9f339259776
Feb 06 17:09:03 minikube cri-dockerd[1338]: time="2026-02-06T17:09:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/955b255b01ebb4bfb64aae214cfbb58235f533c4eee1b680cf1c4cbbdb1fa840/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:09:03 minikube cri-dockerd[1338]: time="2026-02-06T17:09:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c6134f171045285568bb2c0aadd3a07a1d1037388bc3722028e0a13b2f7043fe/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Feb 06 17:11:49 minikube dockerd[1049]: time="2026-02-06T17:11:49.276413449Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=20e30843c32d ep=k8s_POD_ingress-nginx-admission-create-4ztft_ingress-nginx_267e8c5f-7c2a-4a4c-9cd9-e83773a20a68_0 net=none nid=a9f339259776
Feb 06 17:11:49 minikube dockerd[1049]: time="2026-02-06T17:11:49.340284298Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=caea859cf115 ep=k8s_POD_ingress-nginx-admission-patch-wwtsc_ingress-nginx_85f973e5-98f8-4e73-8a54-0b43cc2aba63_0 net=none nid=a9f339259776
Feb 06 17:11:49 minikube cri-dockerd[1338]: time="2026-02-06T17:11:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3cca0aa42fe10ea604c5865487c2a1b81d9d73a2a941c4b5e0a816793eec4a9c/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 06 17:11:49 minikube cri-dockerd[1338]: time="2026-02-06T17:11:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/91773a9c8f9800a8596264a847a8e388d72380ecfdf471688866170b79174a46/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 06 17:11:50 minikube dockerd[1049]: time="2026-02-06T17:11:50.591674773Z" level=warning msg="reference for unknown type: " digest="sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 06 17:12:01 minikube cri-dockerd[1338]: time="2026-02-06T17:12:01Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: 62de241dac5f: Extracting [==================================================>]  31.24kB/31.24kB"
Feb 06 17:12:07 minikube cri-dockerd[1338]: time="2026-02-06T17:12:07Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 06 17:12:08 minikube cri-dockerd[1338]: time="2026-02-06T17:12:08Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.5@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285"
Feb 06 17:12:11 minikube dockerd[1049]: time="2026-02-06T17:12:11.411229138Z" level=info msg="ignoring event" container=63556bd3d1bca67687fe33b965e1230017cb89fc37aab06f35b74acc355c8ed8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 06 17:12:12 minikube dockerd[1049]: time="2026-02-06T17:12:12.166349396Z" level=info msg="ignoring event" container=6e27bb0f78341e1cf0ad67adf80d4f757c2c3f3627e6f11de32abe3c0ed841ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 06 17:12:14 minikube dockerd[1049]: time="2026-02-06T17:12:14.246852017Z" level=info msg="ignoring event" container=3cca0aa42fe10ea604c5865487c2a1b81d9d73a2a941c4b5e0a816793eec4a9c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 06 17:12:14 minikube dockerd[1049]: time="2026-02-06T17:12:14.556260794Z" level=info msg="ignoring event" container=91773a9c8f9800a8596264a847a8e388d72380ecfdf471688866170b79174a46 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 06 17:12:17 minikube dockerd[1049]: time="2026-02-06T17:12:17.531921996Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=8e39e785d16b ep=k8s_POD_ingress-nginx-controller-85d4c799dd-w2g4m_ingress-nginx_d1785e7b-05ff-43e5-9c7c-579901489a3d_0 net=none nid=a9f339259776
Feb 06 17:12:17 minikube cri-dockerd[1338]: time="2026-02-06T17:12:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/92df01686160cf14bdb9ba5725d687783782fbb38ee3eac0429421efbb8dc5a4/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 06 17:12:18 minikube dockerd[1049]: time="2026-02-06T17:12:18.678058825Z" level=warning msg="reference for unknown type: " digest="sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47" remote="registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47"
Feb 06 17:12:28 minikube cri-dockerd[1338]: time="2026-02-06T17:12:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Downloading [=====================>                             ]  9.577MB/22.75MB"
Feb 06 17:12:38 minikube cri-dockerd[1338]: time="2026-02-06T17:12:38Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Extracting [==>                                                ]  1.802MB/33.68MB"
Feb 06 17:12:48 minikube cri-dockerd[1338]: time="2026-02-06T17:12:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Downloading [============>                                      ]  5.696MB/22.29MB"
Feb 06 17:12:58 minikube cri-dockerd[1338]: time="2026-02-06T17:12:58Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 65d72828f76d: Extracting [===============================>                   ]  20.91MB/33.68MB"
Feb 06 17:13:08 minikube cri-dockerd[1338]: time="2026-02-06T17:13:08Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Downloading [==========================================>        ]  18.77MB/22.29MB"
Feb 06 17:13:18 minikube cri-dockerd[1338]: time="2026-02-06T17:13:18Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: 7d95f7f15f87: Extracting [============================================>      ]  20.19MB/22.75MB"
Feb 06 17:13:28 minikube cri-dockerd[1338]: time="2026-02-06T17:13:28Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: c765003409c8: Extracting [==================================>                ]  15.37MB/22.29MB"
Feb 06 17:13:32 minikube cri-dockerd[1338]: time="2026-02-06T17:13:32Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.14.1@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47"
Feb 06 17:14:23 minikube dockerd[1049]: time="2026-02-06T17:14:23.604826450Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=a50ffd418e8d ep=k8s_POD_registry-proxy-8t8s4_kube-system_322646eb-fac6-45a2-853c-4c283f5987e3_0 net=none nid=a9f339259776
Feb 06 17:14:24 minikube cri-dockerd[1338]: time="2026-02-06T17:14:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54e7d8058ccfd1b1b0bf51a44005e6f203fe021e7dc43fbfb33401ccb81477ad/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 06 17:14:29 minikube dockerd[1049]: time="2026-02-06T17:14:29.207227632Z" level=warning msg="reference for unknown type: " digest="sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93" remote="registry.k8s.io/minikube/kube-registry-proxy@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93"
Feb 06 17:14:44 minikube cri-dockerd[1338]: time="2026-02-06T17:14:44Z" level=info msg="Pulling image registry.k8s.io/minikube/kube-registry-proxy:v0.0.11@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93: 1074353eec0d: Downloading [===========>                                       ]  879.6kB/3.86MB"
Feb 06 17:14:54 minikube cri-dockerd[1338]: time="2026-02-06T17:14:54Z" level=info msg="Pulling image registry.k8s.io/minikube/kube-registry-proxy:v0.0.11@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93: 1074353eec0d: Extracting [========>                                          ]  655.4kB/3.86MB"
Feb 06 17:15:04 minikube cri-dockerd[1338]: time="2026-02-06T17:15:04Z" level=info msg="Pulling image registry.k8s.io/minikube/kube-registry-proxy:v0.0.11@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93: 25f453064fd3: Extracting [===============================================>   ]  1.769MB/1.856MB"
Feb 06 17:15:14 minikube cri-dockerd[1338]: time="2026-02-06T17:15:14Z" level=info msg="Pulling image registry.k8s.io/minikube/kube-registry-proxy:v0.0.11@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93: e7a44e8fb074: Extracting [=======>                                           ]  589.8kB/3.741MB"
Feb 06 17:15:19 minikube cri-dockerd[1338]: time="2026-02-06T17:15:19Z" level=info msg="Stop pulling image registry.k8s.io/minikube/kube-registry-proxy:v0.0.11@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93: Status: Downloaded newer image for registry.k8s.io/minikube/kube-registry-proxy@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD                                         NAMESPACE
b1f517f3df6d7       registry.k8s.io/minikube/kube-registry-proxy@sha256:e321acf067df0a78fba3ff97748c10029ca2c413c5b7207e4ca000c62fcdac93         7 minutes ago       Running             registry-proxy            0                   54e7d8058ccfd       registry-proxy-8t8s4                        kube-system
9f06bbd053edf       registry.k8s.io/ingress-nginx/controller@sha256:f95a79b85fb93ac3de752c71a5c27d5ceae10a18b61904dec224c1c6a4581e47             8 minutes ago       Running             controller                0                   92df01686160c       ingress-nginx-controller-85d4c799dd-w2g4m   ingress-nginx
6e27bb0f78341       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285   10 minutes ago      Exited              patch                     0                   91773a9c8f980       ingress-nginx-admission-patch-wwtsc         ingress-nginx
63556bd3d1bca       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:03a00eb0e255e8a25fa49926c24cde0f7e12e8d072c445cdf5136ec78b546285   10 minutes ago      Exited              create                    0                   3cca0aa42fe10       ingress-nginx-admission-create-4ztft        ingress-nginx
38cdbae944645       52546a367cc9e                                                                                                                13 minutes ago      Running             coredns                   0                   c6134f1710452       coredns-66bc5c9577-8sc9r                    kube-system
8e8d70b47e691       6e38f40d628db                                                                                                                13 minutes ago      Running             storage-provisioner       0                   955b255b01ebb       storage-provisioner                         kube-system
669f9b061e513       kindest/kindnetd@sha256:377e2e7a513148f7c942b51cd57bdce1589940df856105384ac7f753a1ab43ae                                     13 minutes ago      Running             kindnet-cni               0                   244b71b3d70bc       kindnet-kbzfm                               kube-system
89bdd65b057dc       df0860106674d                                                                                                                14 minutes ago      Running             kube-proxy                0                   e03f71b1a68ec       kube-proxy-bt4jh                            kube-system
d6a0d0772f3a6       5f1f5298c888d                                                                                                                15 minutes ago      Running             etcd                      0                   8a64192ec2424       etcd-minikube                               kube-system
1faa432e47e8c       a0af72f2ec6d6                                                                                                                15 minutes ago      Running             kube-controller-manager   0                   075f6e10fa5f4       kube-controller-manager-minikube            kube-system
4779bd73c271e       46169d968e920                                                                                                                15 minutes ago      Running             kube-scheduler            0                   9bed051a3439b       kube-scheduler-minikube                     kube-system
8fc203227dd5b       90550c43ad2bc                                                                                                                15 minutes ago      Running             kube-apiserver            0                   13ad54fc3440f       kube-apiserver-minikube                     kube-system


==> controller_ingress [9f06bbd053ed] <==
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.14.1
  Build:         153f3d41d96fc959a450bd60996f06585083c926
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.27.1

-------------------------------------------------------------------------------

W0206 17:13:33.213555       7 client_config.go:667] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0206 17:13:33.213647       7 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0206 17:13:33.218083       7 main.go:248] "Running in Kubernetes cluster" major="1" minor="34" git="v1.34.0" state="clean" commit="f28b4c9efbca5c5c0af716d9f2d5702667ee8a45" platform="linux/amd64"
I0206 17:13:33.238213       7 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0206 17:13:33.245361       7 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0206 17:13:33.252239       7 nginx.go:273] "Starting NGINX Ingress controller"
I0206 17:13:33.258094       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"5a822e22-7ba4-4140-85e2-bbb41c769396", APIVersion:"v1", ResourceVersion:"735", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0206 17:13:33.273905       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"a13239f6-b948-44ab-b114-dc038802389a", APIVersion:"v1", ResourceVersion:"736", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0206 17:13:33.279645       7 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"8dcb9f5c-2cf4-4d8f-b54f-6324c4545971", APIVersion:"v1", ResourceVersion:"737", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0206 17:13:34.468015       7 nginx.go:319] "Starting NGINX process"
I0206 17:13:34.470911       7 nginx.go:339] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0206 17:13:34.468846       7 leaderelection.go:257] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0206 17:13:34.471857       7 controller.go:214] "Configuration changes detected, backend reload required"
I0206 17:13:34.542496       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-85d4c799dd-w2g4m"
I0206 17:13:34.552724       7 leaderelection.go:271] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0206 17:13:34.571691       7 controller.go:228] "Backend successfully reloaded"
I0206 17:13:34.578444       7 controller.go:240] "Initial sync, sleeping for 1 second"
I0206 17:13:34.587922       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-85d4c799dd-w2g4m", UID:"d1785e7b-05ff-43e5-9c7c-579901489a3d", APIVersion:"v1", ResourceVersion:"771", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0206 17:13:34.634084       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-85d4c799dd-w2g4m" node="minikube"
I0206 17:13:34.721835       7 status.go:224] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-85d4c799dd-w2g4m" node="minikube"


==> coredns [38cdbae94464] <==
[INFO] 10.244.0.6:41164 - 64435 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000093746s
[INFO] 10.244.0.6:41164 - 64610 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.00008556s
[INFO] 10.244.0.6:42552 - 15495 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000095886s
[INFO] 10.244.0.6:42552 - 15693 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.011052155s
[INFO] 10.244.0.6:60743 - 936 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.005908725s
[INFO] 10.244.0.6:60743 - 694 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.002825553s
[INFO] 10.244.0.6:40002 - 54033 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000137591s
[INFO] 10.244.0.6:40002 - 54438 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.004530219s
[INFO] 10.244.0.6:44985 - 27919 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000098966s
[INFO] 10.244.0.6:44985 - 28371 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000096394s
[INFO] 10.244.0.6:57915 - 52471 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.004380548s
[INFO] 10.244.0.6:57915 - 52007 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000112845s
[INFO] 10.244.0.6:50962 - 24111 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.011229141s
[INFO] 10.244.0.6:50962 - 24360 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.023612741s
[INFO] 10.244.0.6:53771 - 27875 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.01042248s
[INFO] 10.244.0.6:53771 - 27474 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.011422507s
[INFO] 10.244.0.6:51670 - 6911 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.004290421s
[INFO] 10.244.0.6:51670 - 7092 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.007655374s
[INFO] 10.244.0.6:54383 - 22743 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.005255026s
[INFO] 10.244.0.6:54383 - 22288 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.013185879s
[INFO] 10.244.0.6:51201 - 25884 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000110452s
[INFO] 10.244.0.6:51201 - 25729 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.005803552s
[INFO] 10.244.0.6:44181 - 27005 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.004539674s
[INFO] 10.244.0.6:44181 - 26830 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.00357091s
[INFO] 10.244.0.6:59139 - 40170 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000120582s
[INFO] 10.244.0.6:59139 - 40323 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.004057593s
[INFO] 10.244.0.6:45685 - 40418 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000147511s
[INFO] 10.244.0.6:45685 - 40631 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.000102771s
[INFO] 10.244.1.4:51134 - 3153 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000184393s
[INFO] 10.244.1.4:51134 - 3385 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000280605s
[INFO] 10.244.1.4:40090 - 51004 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000385415s
[INFO] 10.244.1.4:40090 - 50811 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000222668s
[INFO] 10.244.1.4:49395 - 60059 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000061572s
[INFO] 10.244.1.4:49395 - 59893 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000037185s
[INFO] 10.244.1.4:48879 - 39493 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.000262558s
[INFO] 10.244.1.4:48879 - 39331 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000153439s
[INFO] 10.244.1.4:58698 - 12942 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000122871s
[INFO] 10.244.1.4:58698 - 12546 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000241758s
[INFO] 10.244.1.4:37593 - 7831 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000134494s
[INFO] 10.244.1.4:37593 - 7460 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000152486s
[INFO] 10.244.1.4:52901 - 47929 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000100854s
[INFO] 10.244.1.4:52901 - 47778 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.00032444s
[INFO] 10.244.1.4:54296 - 30501 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.000084779s
[INFO] 10.244.1.4:54296 - 30344 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000059373s
[INFO] 10.244.2.4:48540 - 6134 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000152064s
[INFO] 10.244.2.4:48540 - 5782 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000310517s
[INFO] 10.244.2.4:60700 - 61749 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000192203s
[INFO] 10.244.2.4:60700 - 61595 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000103069s
[INFO] 10.244.2.4:57491 - 6481 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000092259s
[INFO] 10.244.2.4:57491 - 6313 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000068916s
[INFO] 10.244.2.4:52959 - 48768 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.000129373s
[INFO] 10.244.2.4:52959 - 48634 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000192051s
[INFO] 10.244.2.4:40897 - 37348 "AAAA IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000105809s
[INFO] 10.244.2.4:40897 - 36915 "A IN registry.kube-system.svc.cluster.local.kube-system.svc.cluster.local. udp 86 false 512" NXDOMAIN qr,aa,rd 179 0.000072478s
[INFO] 10.244.2.4:36894 - 46124 "AAAA IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000216602s
[INFO] 10.244.2.4:36894 - 45945 "A IN registry.kube-system.svc.cluster.local.svc.cluster.local. udp 74 false 512" NXDOMAIN qr,aa,rd 167 0.000284256s
[INFO] 10.244.2.4:46916 - 15764 "AAAA IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000099821s
[INFO] 10.244.2.4:46916 - 15583 "A IN registry.kube-system.svc.cluster.local.cluster.local. udp 70 false 512" NXDOMAIN qr,aa,rd 163 0.000219877s
[INFO] 10.244.2.4:45838 - 13176 "AAAA IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 149 0.000087996s
[INFO] 10.244.2.4:45838 - 12982 "A IN registry.kube-system.svc.cluster.local. udp 56 false 512" NOERROR qr,aa,rd 110 0.000038734s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_02_06T17_07_33_0700
                    minikube.k8s.io/version=v1.38.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 06 Feb 2026 17:07:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 06 Feb 2026 17:22:25 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 06 Feb 2026 17:20:31 +0000   Fri, 06 Feb 2026 17:07:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 06 Feb 2026 17:20:31 +0000   Fri, 06 Feb 2026 17:07:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 06 Feb 2026 17:20:31 +0000   Fri, 06 Feb 2026 17:07:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 06 Feb 2026 17:20:31 +0000   Fri, 06 Feb 2026 17:08:59 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
Allocatable:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
System Info:
  Machine ID:                 4571ab41f5a9a03217021e966978f901
  System UUID:                245db15f-ca5e-4b0f-a4ab-44595c61b5ed
  Boot ID:                    70e0b837-ed90-4568-b52a-f391e235a958
  Kernel Version:             5.15.0-134-generic
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://29.2.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-85d4c799dd-w2g4m    100m (0%)     0 (0%)      90Mi (0%)        0 (0%)         10m
  kube-system                 coredns-66bc5c9577-8sc9r                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     14m
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         14m
  kube-system                 kindnet-kbzfm                                100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      14m
  kube-system                 kube-apiserver-minikube                      250m (1%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-proxy-bt4jh                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         14m
  kube-system                 registry-proxy-8t8s4                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m13s
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         14m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                950m (5%)   100m (0%)
  memory             310Mi (0%)  220Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age   From             Message
  ----     ------                             ----  ----             -------
  Normal   Starting                           14m   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  14m   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           14m   kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced            14m   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            14m   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              14m   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               14m   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     14m   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeReady                          13m   kubelet          Node minikube status is now: NodeReady


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2026_02_06T17_09_16_0700
                    minikube.k8s.io/version=v1.38.0
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 06 Feb 2026 17:09:16 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Fri, 06 Feb 2026 17:22:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 06 Feb 2026 17:19:50 +0000   Fri, 06 Feb 2026 17:09:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 06 Feb 2026 17:19:50 +0000   Fri, 06 Feb 2026 17:09:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 06 Feb 2026 17:19:50 +0000   Fri, 06 Feb 2026 17:09:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 06 Feb 2026 17:19:50 +0000   Fri, 06 Feb 2026 17:10:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
Allocatable:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
System Info:
  Machine ID:                 4571ab41f5a9a03217021e966978f901
  System UUID:                9571af51-0b08-4148-97a3-52e249d02578
  Boot ID:                    70e0b837-ed90-4568-b52a-f391e235a958
  Kernel Version:             5.15.0-134-generic
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://29.2.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (5 in total)
  Namespace                   Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                     ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-vknbw                            100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      13m
  kube-system                 kube-proxy-j5xvm                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 registry-6b586f9694-2ttpv                0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m14s
  kube-system                 registry-proxy-fhhf6                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m13s
  kubernetes-dashboard        kubernetes-dashboard-855c9754f9-dptrj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m46s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           13m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  13m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           13m                kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory            13m (x8 over 13m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              13m (x8 over 13m)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               13m (x7 over 13m)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            13m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     13m                node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2026_02_06T17_10_56_0700
                    minikube.k8s.io/version=v1.38.0
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 06 Feb 2026 17:10:56 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Fri, 06 Feb 2026 17:22:23 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 06 Feb 2026 17:21:01 +0000   Fri, 06 Feb 2026 17:10:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 06 Feb 2026 17:21:01 +0000   Fri, 06 Feb 2026 17:10:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 06 Feb 2026 17:21:01 +0000   Fri, 06 Feb 2026 17:10:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 06 Feb 2026 17:21:01 +0000   Fri, 06 Feb 2026 17:11:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
Allocatable:
  cpu:                18
  ephemeral-storage:  40581564Ki
  hugepages-2Mi:      0
  memory:             41068380Ki
  pods:               50
System Info:
  Machine ID:                 4571ab41f5a9a03217021e966978f901
  System UUID:                9e866cc5-3cba-44d1-8836-36126a347ab1
  Boot ID:                    70e0b837-ed90-4568-b52a-f391e235a958
  Kernel Version:             5.15.0-134-generic
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://29.2.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (6 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-nzw4c                                 100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      11m
  kube-system                 kube-ingress-dns-minikube                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m50s
  kube-system                 kube-proxy-kjn8c                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kube-system                 metrics-server-85b7d694d7-d7m66               100m (0%)     0 (0%)      200Mi (0%)       0 (0%)         8m32s
  kube-system                 registry-proxy-7frrg                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m13s
  kubernetes-dashboard        dashboard-metrics-scraper-77bf4d6c4c-lx98z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m46s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                200m (1%)   100m (0%)
  memory             250Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           11m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  11m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           11m                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced            11m                kubelet          Updated Node Allocatable limit across pods
  Normal   CIDRAssignmentFailed               11m                cidrAllocator    Node minikube-m03 status is now: CIDRAssignmentFailed
  Normal   NodeHasSufficientMemory            11m (x8 over 11m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              11m (x8 over 11m)  kubelet          Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               11m (x7 over 11m)  kubelet          Node minikube-m03 status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     11m                node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller


==> dmesg <==
[  +0.000001] RBP: ffffbab202d1fef0 R08: 0000000000000000 R09: ffff96d483408e40
[  +0.000001] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000008
[  +0.000000] R13: ffffbab202d1ff58 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001] FS:  000000c000055898(0000) GS:ffff96dd77c40000(0000) knlGS:0000000000000000
[  +0.000001] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 000000c0002e3010 CR3: 0000000248048000 CR4: 00000000000106e0
[  +0.000002] Call Trace:
[  +0.000001]  <IRQ>
[  +0.000004]  ? show_trace_log_lvl+0x1d6/0x2ea
[  +0.000004]  ? show_trace_log_lvl+0x1d6/0x2ea
[  +0.000002]  ? exit_to_user_mode_prepare+0xa0/0xb0
[  +0.000002]  ? show_regs.part.0+0x23/0x29
[  +0.000001]  ? show_regs.cold+0x8/0xd
[  +0.000001]  ? watchdog_timer_fn+0x1be/0x220
[  +0.000003]  ? lockup_detector_update_enable+0x60/0x60
[  +0.000001]  ? __hrtimer_run_queues+0x104/0x230
[  +0.000003]  ? kvm_clock_get_cycles+0x11/0x20
[  +0.000003]  ? hrtimer_interrupt+0x101/0x220
[  +0.000001]  ? __sysvec_apic_timer_interrupt+0x5e/0xe0
[  +0.000003]  ? sysvec_apic_timer_interrupt+0x7b/0x90
[  +0.000003]  </IRQ>
[  +0.000001]  <TASK>
[  +0.000000]  ? asm_sysvec_apic_timer_interrupt+0x1b/0x20
[  +0.000004]  ? exit_to_user_mode_loop+0x73/0x160
[  +0.000001]  exit_to_user_mode_prepare+0xa0/0xb0
[  +0.000002]  irqentry_exit_to_user_mode+0x9/0x20
[  +0.000001]  irqentry_exit+0x1d/0x30
[  +0.000002]  sysvec_call_function_single+0x4e/0x90
[  +0.000001]  asm_sysvec_call_function_single+0x1b/0x20
[  +0.000001] RIP: 0033:0x4131f0
[  +0.000002] Code: 2d 03 00 00 55 48 89 e5 48 83 ec 50 49 8b 4e 30 8b 91 08 01 00 00 0f 1f 40 00 85 d2 0f 8c fd 02 00 00 ff c2 89 91 08 01 00 00 <b9> 01 00 00 00 86 08 f6 c1 01 74 5d 48 89 44 24 60 4c 89 74 24 28
[  +0.000001] RSP: 002b:000000c00023def0 EFLAGS: 00010202
[  +0.000001] RAX: 0000000000bf17b8 RBX: 000000c000055808 RCX: 000000c000055808
[  +0.000001] RDX: 0000000000000001 RSI: 0000000000001f7f RDI: 00007fff747f9000
[  +0.000001] RBP: 000000c00023df40 R08: 000008df032595a6 R09: 0000000000000001
[  +0.000000] R10: 00007fff747f8080 R11: 00007fff747f8090 R12: 000000c00023dec8
[  +0.000001] R13: 00007f36a15da9f0 R14: 000000c0000036c0 R15: 0000000000000005
[  +0.000001]  </TASK>
[Feb 6 16:04] tmpfs: Unknown parameter 'noswap'
[ +14.871876] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:06] tmpfs: Unknown parameter 'noswap'
[ +13.917127] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:08] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:09] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:24] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:41] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:42] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:43] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:45] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:47] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:48] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:49] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:51] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:54] tmpfs: Unknown parameter 'noswap'
[ +14.516781] tmpfs: Unknown parameter 'noswap'
[Feb 6 16:55] tmpfs: Unknown parameter 'noswap'
[Feb 6 17:07] tmpfs: Unknown parameter 'noswap'
[ +13.474001] tmpfs: Unknown parameter 'noswap'
[Feb 6 17:09] tmpfs: Unknown parameter 'noswap'
[Feb 6 17:10] tmpfs: Unknown parameter 'noswap'


==> etcd [d6a0d0772f3a] <==
{"level":"info","ts":"2026-02-06T17:16:37.904103Z","caller":"traceutil/trace.go:172","msg":"trace[1575807574] range","detail":"{range_begin:/registry/jobs; range_end:; response_count:0; response_revision:1285; }","duration":"373.370619ms","start":"2026-02-06T17:16:37.530730Z","end":"2026-02-06T17:16:37.904100Z","steps":["trace[1575807574] 'range keys from in-memory index tree'  (duration: 368.921577ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:16:37.904112Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-06T17:16:37.530717Z","time spent":"373.392453ms","remote":"127.0.0.1:45634","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/jobs\" limit:1 "}
{"level":"info","ts":"2026-02-06T17:16:52.569752Z","caller":"traceutil/trace.go:172","msg":"trace[4021369] transaction","detail":"{read_only:false; response_revision:1302; number_of_response:1; }","duration":"100.473327ms","start":"2026-02-06T17:16:52.469268Z","end":"2026-02-06T17:16:52.569741Z","steps":["trace[4021369] 'process raft request'  (duration: 100.395916ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:16:57.976770Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"161.307674ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:16:57.982931Z","caller":"traceutil/trace.go:172","msg":"trace[1119330486] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1307; }","duration":"167.468405ms","start":"2026-02-06T17:16:57.815449Z","end":"2026-02-06T17:16:57.982918Z","steps":["trace[1119330486] 'range keys from in-memory index tree'  (duration: 161.282905ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:16:57.980910Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"115.191804ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:16:57.982995Z","caller":"traceutil/trace.go:172","msg":"trace[1821980551] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1307; }","duration":"117.288768ms","start":"2026-02-06T17:16:57.865704Z","end":"2026-02-06T17:16:57.982993Z","steps":["trace[1821980551] 'range keys from in-memory index tree'  (duration: 115.150481ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:25.795584Z","caller":"traceutil/trace.go:172","msg":"trace[2035952724] transaction","detail":"{read_only:false; response_revision:1338; number_of_response:1; }","duration":"190.051942ms","start":"2026-02-06T17:17:25.605521Z","end":"2026-02-06T17:17:25.795573Z","steps":["trace[2035952724] 'process raft request'  (duration: 189.992768ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:26.143387Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":849}
{"level":"info","ts":"2026-02-06T17:17:26.234953Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":849,"took":"87.043497ms","hash":2583845910,"current-db-size-bytes":3387392,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":3387392,"current-db-size-in-use":"3.4 MB"}
{"level":"info","ts":"2026-02-06T17:17:26.239266Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2583845910,"revision":849,"compact-revision":-1}
{"level":"info","ts":"2026-02-06T17:17:33.332907Z","caller":"traceutil/trace.go:172","msg":"trace[1606308606] transaction","detail":"{read_only:false; response_revision:1347; number_of_response:1; }","duration":"106.489844ms","start":"2026-02-06T17:17:33.226406Z","end":"2026-02-06T17:17:33.332896Z","steps":["trace[1606308606] 'process raft request'  (duration: 106.425887ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:17:46.466046Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"115.916208ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:17:46.466085Z","caller":"traceutil/trace.go:172","msg":"trace[599834927] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1362; }","duration":"115.966406ms","start":"2026-02-06T17:17:46.350111Z","end":"2026-02-06T17:17:46.466078Z","steps":["trace[599834927] 'range keys from in-memory index tree'  (duration: 115.873875ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:52.087698Z","caller":"traceutil/trace.go:172","msg":"trace[758498856] linearizableReadLoop","detail":"{readStateIndex:1511; appliedIndex:1511; }","duration":"126.037168ms","start":"2026-02-06T17:17:51.961648Z","end":"2026-02-06T17:17:52.087685Z","steps":["trace[758498856] 'read index received'  (duration: 126.033574ms)","trace[758498856] 'applied index is now lower than readState.Index'  (duration: 3.134Âµs)"],"step_count":2}
{"level":"warn","ts":"2026-02-06T17:17:52.091365Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"129.702127ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:17:52.091393Z","caller":"traceutil/trace.go:172","msg":"trace[2069435141] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1366; }","duration":"129.740949ms","start":"2026-02-06T17:17:51.961645Z","end":"2026-02-06T17:17:52.091386Z","steps":["trace[2069435141] 'agreement among raft nodes before linearized reading'  (duration: 129.675392ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:52.091430Z","caller":"traceutil/trace.go:172","msg":"trace[1010889240] transaction","detail":"{read_only:false; response_revision:1367; number_of_response:1; }","duration":"211.130533ms","start":"2026-02-06T17:17:51.880292Z","end":"2026-02-06T17:17:52.091423Z","steps":["trace[1010889240] 'process raft request'  (duration: 211.062582ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:17:52.915005Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"166.587514ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043174317269168 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" mod_revision:1360 > success:<request_put:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" value_size:427 >> failure:<request_range:<key:\"/registry/leases/ingress-nginx/ingress-nginx-leader\" > >>","response":"size:16"}
{"level":"info","ts":"2026-02-06T17:17:52.915142Z","caller":"traceutil/trace.go:172","msg":"trace[699176740] transaction","detail":"{read_only:false; response_revision:1369; number_of_response:1; }","duration":"266.430577ms","start":"2026-02-06T17:17:52.648704Z","end":"2026-02-06T17:17:52.915134Z","steps":["trace[699176740] 'process raft request'  (duration: 99.650805ms)","trace[699176740] 'compare'  (duration: 166.588324ms)"],"step_count":2}
{"level":"info","ts":"2026-02-06T17:17:54.499211Z","caller":"traceutil/trace.go:172","msg":"trace[2002157431] linearizableReadLoop","detail":"{readStateIndex:1514; appliedIndex:1514; }","duration":"121.754165ms","start":"2026-02-06T17:17:54.377443Z","end":"2026-02-06T17:17:54.499197Z","steps":["trace[2002157431] 'read index received'  (duration: 121.75038ms)","trace[2002157431] 'applied index is now lower than readState.Index'  (duration: 3.32Âµs)"],"step_count":2}
{"level":"warn","ts":"2026-02-06T17:17:54.503492Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"126.03527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:17:54.503520Z","caller":"traceutil/trace.go:172","msg":"trace[781454913] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1369; }","duration":"126.075409ms","start":"2026-02-06T17:17:54.377439Z","end":"2026-02-06T17:17:54.503514Z","steps":["trace[781454913] 'agreement among raft nodes before linearized reading'  (duration: 126.002363ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:54.504445Z","caller":"traceutil/trace.go:172","msg":"trace[1675658207] transaction","detail":"{read_only:false; response_revision:1370; number_of_response:1; }","duration":"216.045004ms","start":"2026-02-06T17:17:54.288393Z","end":"2026-02-06T17:17:54.504438Z","steps":["trace[1675658207] 'process raft request'  (duration: 215.983626ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:55.072408Z","caller":"traceutil/trace.go:172","msg":"trace[1935253638] transaction","detail":"{read_only:false; response_revision:1371; number_of_response:1; }","duration":"248.219966ms","start":"2026-02-06T17:17:54.824181Z","end":"2026-02-06T17:17:55.072401Z","steps":["trace[1935253638] 'process raft request'  (duration: 248.072661ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:17:55.072594Z","caller":"traceutil/trace.go:172","msg":"trace[281096022] linearizableReadLoop","detail":"{readStateIndex:1515; appliedIndex:1516; }","duration":"245.280259ms","start":"2026-02-06T17:17:54.827307Z","end":"2026-02-06T17:17:55.072587Z","steps":["trace[281096022] 'read index received'  (duration: 245.278438ms)","trace[281096022] 'applied index is now lower than readState.Index'  (duration: 1.565Âµs)"],"step_count":2}
{"level":"warn","ts":"2026-02-06T17:17:55.072648Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"245.333991ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:17:55.072658Z","caller":"traceutil/trace.go:172","msg":"trace[142795373] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1371; }","duration":"245.351254ms","start":"2026-02-06T17:17:54.827303Z","end":"2026-02-06T17:17:55.072654Z","steps":["trace[142795373] 'agreement among raft nodes before linearized reading'  (duration: 245.325245ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:17:55.076479Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"145.36498ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:17:55.076497Z","caller":"traceutil/trace.go:172","msg":"trace[18061836] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1371; }","duration":"145.388764ms","start":"2026-02-06T17:17:54.931104Z","end":"2026-02-06T17:17:55.076493Z","steps":["trace[18061836] 'agreement among raft nodes before linearized reading'  (duration: 145.347037ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:18:26.078447Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"155.449214ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128043174317269415 > lease_revoke:<id:70cc9c33ec382134>","response":"size:29"}
{"level":"info","ts":"2026-02-06T17:18:38.387575Z","caller":"traceutil/trace.go:172","msg":"trace[642807060] transaction","detail":"{read_only:false; response_revision:1419; number_of_response:1; }","duration":"111.482731ms","start":"2026-02-06T17:18:38.276082Z","end":"2026-02-06T17:18:38.387564Z","steps":["trace[642807060] 'process raft request'  (duration: 111.412284ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:18:49.224995Z","caller":"traceutil/trace.go:172","msg":"trace[379105097] transaction","detail":"{read_only:false; response_revision:1439; number_of_response:1; }","duration":"124.349818ms","start":"2026-02-06T17:18:49.100631Z","end":"2026-02-06T17:18:49.224981Z","steps":["trace[379105097] 'process raft request'  (duration: 124.26301ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:18:56.170107Z","caller":"traceutil/trace.go:172","msg":"trace[622225057] transaction","detail":"{read_only:false; response_revision:1448; number_of_response:1; }","duration":"145.233584ms","start":"2026-02-06T17:18:56.024857Z","end":"2026-02-06T17:18:56.170090Z","steps":["trace[622225057] 'process raft request'  (duration: 131.354125ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:19:01.216541Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"192.101584ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:19:01.216584Z","caller":"traceutil/trace.go:172","msg":"trace[781957001] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1453; }","duration":"192.15045ms","start":"2026-02-06T17:19:01.024426Z","end":"2026-02-06T17:19:01.216576Z","steps":["trace[781957001] 'range keys from in-memory index tree'  (duration: 192.06569ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:19:01.310318Z","caller":"traceutil/trace.go:172","msg":"trace[1465075781] transaction","detail":"{read_only:false; response_revision:1454; number_of_response:1; }","duration":"123.881397ms","start":"2026-02-06T17:19:01.186420Z","end":"2026-02-06T17:19:01.310301Z","steps":["trace[1465075781] 'process raft request'  (duration: 122.086278ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:19:01.527248Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"109.46503ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:19:01.532628Z","caller":"traceutil/trace.go:172","msg":"trace[746444799] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:1454; }","duration":"114.848731ms","start":"2026-02-06T17:19:01.417765Z","end":"2026-02-06T17:19:01.532614Z","steps":["trace[746444799] 'range keys from in-memory index tree'  (duration: 109.414945ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:19:07.111506Z","caller":"traceutil/trace.go:172","msg":"trace[1564967967] linearizableReadLoop","detail":"{readStateIndex:1619; appliedIndex:1619; }","duration":"197.311135ms","start":"2026-02-06T17:19:06.914183Z","end":"2026-02-06T17:19:07.111494Z","steps":["trace[1564967967] 'read index received'  (duration: 197.307442ms)","trace[1564967967] 'applied index is now lower than readState.Index'  (duration: 3.115Âµs)"],"step_count":2}
{"level":"info","ts":"2026-02-06T17:19:07.114970Z","caller":"traceutil/trace.go:172","msg":"trace[2105416090] transaction","detail":"{read_only:false; response_revision:1459; number_of_response:1; }","duration":"210.304916ms","start":"2026-02-06T17:19:06.904659Z","end":"2026-02-06T17:19:07.114963Z","steps":["trace[2105416090] 'process raft request'  (duration: 210.227971ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:19:07.115397Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"201.205331ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:19:07.115420Z","caller":"traceutil/trace.go:172","msg":"trace[686112010] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1459; }","duration":"201.23467ms","start":"2026-02-06T17:19:06.914181Z","end":"2026-02-06T17:19:07.115416Z","steps":["trace[686112010] 'agreement among raft nodes before linearized reading'  (duration: 201.040472ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:19:07.384497Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"167.092917ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:19:07.395459Z","caller":"traceutil/trace.go:172","msg":"trace[1952985625] range","detail":"{range_begin:/registry/secrets; range_end:; response_count:0; response_revision:1459; }","duration":"178.055148ms","start":"2026-02-06T17:19:07.217392Z","end":"2026-02-06T17:19:07.395447Z","steps":["trace[1952985625] 'range keys from in-memory index tree'  (duration: 167.058778ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:19:07.387179Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"135.114863ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:19:07.486898Z","caller":"traceutil/trace.go:172","msg":"trace[579367700] range","detail":"{range_begin:/registry/daemonsets; range_end:; response_count:0; response_revision:1459; }","duration":"234.82802ms","start":"2026-02-06T17:19:07.252054Z","end":"2026-02-06T17:19:07.486882Z","steps":["trace[579367700] 'range keys from in-memory index tree'  (duration: 135.078516ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:19:20.883320Z","caller":"traceutil/trace.go:172","msg":"trace[687942625] transaction","detail":"{read_only:false; response_revision:1476; number_of_response:1; }","duration":"142.011287ms","start":"2026-02-06T17:19:20.741297Z","end":"2026-02-06T17:19:20.883309Z","steps":["trace[687942625] 'process raft request'  (duration: 106.788424ms)","trace[687942625] 'compare'  (duration: 35.128088ms)"],"step_count":2}
{"level":"info","ts":"2026-02-06T17:19:26.300425Z","caller":"traceutil/trace.go:172","msg":"trace[830437078] transaction","detail":"{read_only:false; response_revision:1486; number_of_response:1; }","duration":"105.306962ms","start":"2026-02-06T17:19:26.195106Z","end":"2026-02-06T17:19:26.300413Z","steps":["trace[830437078] 'process raft request'  (duration: 99.524769ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:19:26.435613Z","caller":"traceutil/trace.go:172","msg":"trace[441955169] transaction","detail":"{read_only:false; response_revision:1488; number_of_response:1; }","duration":"147.421017ms","start":"2026-02-06T17:19:26.288186Z","end":"2026-02-06T17:19:26.435607Z","steps":["trace[441955169] 'process raft request'  (duration: 147.371123ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:19:26.500293Z","caller":"traceutil/trace.go:172","msg":"trace[1991437642] transaction","detail":"{read_only:false; response_revision:1489; number_of_response:1; }","duration":"160.860877ms","start":"2026-02-06T17:19:26.339422Z","end":"2026-02-06T17:19:26.500283Z","steps":["trace[1991437642] 'process raft request'  (duration: 160.798731ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:20:18.195910Z","caller":"traceutil/trace.go:172","msg":"trace[280836747] linearizableReadLoop","detail":"{readStateIndex:1741; appliedIndex:1741; }","duration":"381.644484ms","start":"2026-02-06T17:20:17.814251Z","end":"2026-02-06T17:20:18.195896Z","steps":["trace[280836747] 'read index received'  (duration: 381.635797ms)","trace[280836747] 'applied index is now lower than readState.Index'  (duration: 7.942Âµs)"],"step_count":2}
{"level":"warn","ts":"2026-02-06T17:20:18.206217Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"211.845748ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:20:18.206240Z","caller":"traceutil/trace.go:172","msg":"trace[235035957] range","detail":"{range_begin:/registry/limitranges; range_end:; response_count:0; response_revision:1566; }","duration":"211.875012ms","start":"2026-02-06T17:20:17.994359Z","end":"2026-02-06T17:20:18.206234Z","steps":["trace[235035957] 'agreement among raft nodes before linearized reading'  (duration: 211.823199ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:20:18.196839Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"382.571555ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-06T17:20:18.208153Z","caller":"traceutil/trace.go:172","msg":"trace[1323688802] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1565; }","duration":"393.895456ms","start":"2026-02-06T17:20:17.814249Z","end":"2026-02-06T17:20:18.208144Z","steps":["trace[1323688802] 'agreement among raft nodes before linearized reading'  (duration: 382.547778ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:20:18.198613Z","caller":"traceutil/trace.go:172","msg":"trace[1002252026] transaction","detail":"{read_only:false; response_revision:1566; number_of_response:1; }","duration":"392.660624ms","start":"2026-02-06T17:20:17.805946Z","end":"2026-02-06T17:20:18.198606Z","steps":["trace[1002252026] 'process raft request'  (duration: 392.595636ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-06T17:20:18.208306Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-06T17:20:17.805931Z","time spent":"402.341386ms","remote":"127.0.0.1:45702","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1545 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2026-02-06T17:20:20.907974Z","caller":"traceutil/trace.go:172","msg":"trace[259187677] transaction","detail":"{read_only:false; response_revision:1573; number_of_response:1; }","duration":"100.188835ms","start":"2026-02-06T17:20:20.807774Z","end":"2026-02-06T17:20:20.907962Z","steps":["trace[259187677] 'process raft request'  (duration: 100.115256ms)"],"step_count":1}
{"level":"info","ts":"2026-02-06T17:21:17.333398Z","caller":"traceutil/trace.go:172","msg":"trace[356317345] transaction","detail":"{read_only:false; response_revision:1654; number_of_response:1; }","duration":"118.26492ms","start":"2026-02-06T17:21:17.215115Z","end":"2026-02-06T17:21:17.333380Z","steps":["trace[356317345] 'process raft request'  (duration: 118.179879ms)"],"step_count":1}


==> kernel <==
 17:22:25 up  3:33,  0 user,  load average: 3.94, 8.93, 8.61
Linux minikube 5.15.0-134-generic #145-Ubuntu SMP Wed Feb 12 20:08:39 UTC 2025 x86_64 GNU/Linux
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"


==> kindnet [669f9b061e51] <==
I0206 17:20:47.899143       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:20:47.899196       1 main.go:301] handling current node
I0206 17:20:47.899208       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:20:47.899212       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:20:47.899320       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:20:47.899323       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:20:57.904823       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:20:57.904877       1 main.go:301] handling current node
I0206 17:20:57.904891       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:20:57.904897       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:20:57.905150       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:20:57.905155       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:07.901575       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:07.901628       1 main.go:301] handling current node
I0206 17:21:07.901641       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:07.901645       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:07.901774       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:07.901778       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:17.915963       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:17.915993       1 main.go:301] handling current node
I0206 17:21:17.916005       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:17.916008       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:17.916176       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:17.916187       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:27.910527       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:27.998495       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:28.093439       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:28.093490       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:28.093571       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:28.093575       1 main.go:301] handling current node
I0206 17:21:37.899663       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:37.899872       1 main.go:301] handling current node
I0206 17:21:37.899889       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:37.899895       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:37.900105       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:37.900113       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:47.900485       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:47.900613       1 main.go:301] handling current node
I0206 17:21:47.900625       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:47.900628       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:47.900833       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:47.900870       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:57.902596       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:21:57.902703       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:21:57.902815       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:21:57.902818       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:21:57.902886       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:21:57.902918       1 main.go:301] handling current node
I0206 17:22:07.907964       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:22:07.908072       1 main.go:301] handling current node
I0206 17:22:07.908087       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:22:07.908092       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:22:07.908203       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:22:07.908206       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0206 17:22:17.903199       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0206 17:22:17.993083       1 main.go:301] handling current node
I0206 17:22:17.995287       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0206 17:22:17.995301       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0206 17:22:17.997742       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0206 17:22:17.997752       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 


==> kube-apiserver [8fc203227dd5] <==
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0206 17:14:56.959673       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0206 17:15:58.896369       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:16:06.203577       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0206 17:16:56.965495       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:16:56.969333       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0206 17:16:56.969344       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0206 17:16:56.967917       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:16:56.969403       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0206 17:16:56.996355       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0206 17:17:09.205747       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:17:10.244871       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:17:28.086206       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
W0206 17:17:28.196677       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:17:28.199175       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0206 17:17:29.203537       1 handler_proxy.go:99] no RequestInfo found in the context
W0206 17:17:29.203601       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:17:29.212764       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0206 17:17:29.212771       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0206 17:17:29.212791       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0206 17:17:29.214522       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0206 17:18:22.304700       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:18:22.726795       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0206 17:18:29.214309       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:18:29.214367       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0206 17:18:29.214374       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W0206 17:18:29.230915       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:18:29.241904       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0206 17:18:29.241928       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0206 17:19:30.990799       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:19:46.463212       1 stats.go:136] "Error getting keys" err="empty key: \"\""
W0206 17:20:23.147274       1 handler_proxy.go:99] no RequestInfo found in the context
E0206 17:20:23.147313       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0206 17:20:23.147326       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
E0206 17:20:23.180314       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
E0206 17:20:23.207074       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
E0206 17:20:23.252396       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
E0206 17:20:23.264552       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
E0206 17:20:23.311499       1 remote_available_controller.go:462] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.102.201.113:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.102.201.113:443: connect: connection refused" logger="UnhandledError"
I0206 17:20:23.857539       1 handler.go:285] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0206 17:20:36.451096       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:21:03.046764       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:22:00.826328       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I0206 17:22:06.611876       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [1faa432e47e8] <==
I0206 17:07:35.596481       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0206 17:07:35.643533       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0206 17:07:35.648180       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I0206 17:07:35.648234       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0206 17:07:35.660656       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I0206 17:07:35.661617       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0206 17:07:35.661650       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0206 17:07:35.661709       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I0206 17:07:35.661735       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I0206 17:07:35.663254       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I0206 17:07:35.663263       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0206 17:07:35.663268       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0206 17:09:00.704646       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0206 17:09:16.529857       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m02\" does not exist"
I0206 17:09:16.535412       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube-m02" podCIDRs=["10.244.1.0/24"]
I0206 17:09:20.713198       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m02"
I0206 17:10:43.983347       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I0206 17:10:56.593227       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m03\" does not exist"
I0206 17:10:56.593714       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I0206 17:10:56.676960       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube-m03" podCIDRs=["10.244.2.0/24"]
E0206 17:10:57.343197       1 range_allocator.go:433] "Failed to update node PodCIDR after multiple attempts" err="failed to patch node CIDR: Node \"minikube-m03\" is invalid: [spec.podCIDRs: Invalid value: [\"10.244.3.0/24\",\"10.244.2.0/24\"]: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="minikube-m03" podCIDRs=["10.244.3.0/24"]
E0206 17:10:57.346776       1 range_allocator.go:439] "CIDR assignment for node failed. Releasing allocated CIDR" err="failed to patch node CIDR: Node \"minikube-m03\" is invalid: [spec.podCIDRs: Invalid value: [\"10.244.3.0/24\",\"10.244.2.0/24\"]: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="minikube-m03"
E0206 17:10:57.346818       1 range_allocator.go:252] "Unhandled Error" err="error syncing 'minikube-m03': failed to patch node CIDR: Node \"minikube-m03\" is invalid: [spec.podCIDRs: Invalid value: [\"10.244.3.0/24\",\"10.244.2.0/24\"]: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid], requeuing" logger="UnhandledError"
I0206 17:11:00.984156       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m03"
I0206 17:11:43.404102       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
E0206 17:13:39.182782       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-77bf4d6c4c\" failed with pods \"dashboard-metrics-scraper-77bf4d6c4c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.203915       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-77bf4d6c4c\" failed with pods \"dashboard-metrics-scraper-77bf4d6c4c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.344215       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-77bf4d6c4c\" failed with pods \"dashboard-metrics-scraper-77bf4d6c4c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.350158       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-855c9754f9\" failed with pods \"kubernetes-dashboard-855c9754f9-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.415821       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-855c9754f9\" failed with pods \"kubernetes-dashboard-855c9754f9-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.491017       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-855c9754f9\" failed with pods \"kubernetes-dashboard-855c9754f9-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.491798       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-77bf4d6c4c\" failed with pods \"dashboard-metrics-scraper-77bf4d6c4c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:39.529309       1 replica_set.go:587] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-855c9754f9\" failed with pods \"kubernetes-dashboard-855c9754f9-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
E0206 17:13:53.864474       1 replica_set.go:587] "Unhandled Error" err="sync \"kube-system/metrics-server-85b7d694d7\" failed with pods \"metrics-server-85b7d694d7-\" is forbidden: error looking up service account kube-system/metrics-server: serviceaccount \"metrics-server\" not found" logger="UnhandledError"
I0206 17:14:06.284185       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:14:06.439113       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:14:36.383143       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:14:36.591864       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:15:06.488217       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:15:06.650410       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:15:36.607232       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:15:36.760476       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:16:06.683280       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:16:06.842587       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:16:36.851724       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:16:36.896344       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:17:06.943093       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:17:06.947490       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
E0206 17:17:37.004330       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:17:37.051045       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:18:07.098755       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:18:07.191529       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:18:37.155550       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:18:37.343113       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:19:07.216859       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:19:07.487775       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:19:37.260506       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:19:37.623563       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
E0206 17:20:07.300686       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0206 17:20:07.677574       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"


==> kube-proxy [89bdd65b057d] <==
I0206 17:07:42.943349       1 server_linux.go:53] "Using iptables proxy"
I0206 17:07:43.142292       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I0206 17:07:43.252649       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I0206 17:07:43.252686       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0206 17:07:43.261108       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0206 17:07:43.538708       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0206 17:07:43.538750       1 server_linux.go:132] "Using iptables Proxier"
I0206 17:07:43.589902       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0206 17:07:43.614969       1 server.go:527] "Version info" version="v1.34.0"
I0206 17:07:43.620455       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0206 17:07:43.703534       1 config.go:106] "Starting endpoint slice config controller"
I0206 17:07:43.752703       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0206 17:07:43.755444       1 config.go:403] "Starting serviceCIDR config controller"
I0206 17:07:43.755456       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0206 17:07:43.756052       1 config.go:309] "Starting node config controller"
I0206 17:07:43.756065       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0206 17:07:43.756070       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0206 17:07:43.688955       1 config.go:200] "Starting service config controller"
I0206 17:07:43.768889       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0206 17:07:43.958647       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I0206 17:07:43.970645       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0206 17:07:43.970684       1 shared_informer.go:356] "Caches are synced" controller="service config"


==> kube-scheduler [4779bd73c271] <==
I0206 17:07:25.097097       1 serving.go:386] Generated self-signed cert in-memory
W0206 17:07:28.073737       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0206 17:07:28.073827       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0206 17:07:28.073839       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0206 17:07:28.073846       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0206 17:07:28.207866       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I0206 17:07:28.212588       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0206 17:07:28.223825       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0206 17:07:28.249883       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0206 17:07:28.426895       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0206 17:07:28.427038       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0206 17:07:28.445192       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0206 17:07:28.470355       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E0206 17:07:28.477275       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0206 17:07:28.502783       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0206 17:07:28.518229       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0206 17:07:28.541000       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0206 17:07:28.565960       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0206 17:07:28.542660       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0206 17:07:28.579623       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0206 17:07:28.584619       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0206 17:07:28.582544       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E0206 17:07:28.596303       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E0206 17:07:28.596356       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0206 17:07:28.631730       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0206 17:07:28.646630       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
I0206 17:07:28.244619       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0206 17:07:28.650707       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0206 17:07:28.648465       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0206 17:07:28.700643       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0206 17:07:29.459884       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0206 17:07:29.480731       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0206 17:07:29.484810       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0206 17:07:29.506118       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
I0206 17:07:30.053707       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Feb 06 17:07:33 minikube kubelet[2198]: I0206 17:07:33.150875    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.150861055 podStartE2EDuration="1.150861055s" podCreationTimestamp="2026-02-06 17:07:32 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-02-06 17:07:33.074088837 +0000 UTC m=+2.162694457" watchObservedRunningTime="2026-02-06 17:07:33.150861055 +0000 UTC m=+2.239466676"
Feb 06 17:07:33 minikube kubelet[2198]: I0206 17:07:33.595410    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.595396195 podStartE2EDuration="1.595396195s" podCreationTimestamp="2026-02-06 17:07:32 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-02-06 17:07:33.234756939 +0000 UTC m=+2.323362561" watchObservedRunningTime="2026-02-06 17:07:33.595396195 +0000 UTC m=+2.684001817"
Feb 06 17:07:35 minikube kubelet[2198]: I0206 17:07:35.771778    2198 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 06 17:07:35 minikube kubelet[2198]: I0206 17:07:35.799819    2198 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.590624    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-llwzv\" (UniqueName: \"kubernetes.io/projected/ef168f5d-82c9-4802-8351-9b81b285a9ae-kube-api-access-llwzv\") pod \"kindnet-kbzfm\" (UID: \"ef168f5d-82c9-4802-8351-9b81b285a9ae\") " pod="kube-system/kindnet-kbzfm"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.590649    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/ef168f5d-82c9-4802-8351-9b81b285a9ae-cni-cfg\") pod \"kindnet-kbzfm\" (UID: \"ef168f5d-82c9-4802-8351-9b81b285a9ae\") " pod="kube-system/kindnet-kbzfm"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.590659    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ef168f5d-82c9-4802-8351-9b81b285a9ae-xtables-lock\") pod \"kindnet-kbzfm\" (UID: \"ef168f5d-82c9-4802-8351-9b81b285a9ae\") " pod="kube-system/kindnet-kbzfm"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.590666    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ef168f5d-82c9-4802-8351-9b81b285a9ae-lib-modules\") pod \"kindnet-kbzfm\" (UID: \"ef168f5d-82c9-4802-8351-9b81b285a9ae\") " pod="kube-system/kindnet-kbzfm"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.816736    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/0545b78d-5877-4590-a485-bd9d670b62f5-lib-modules\") pod \"kube-proxy-bt4jh\" (UID: \"0545b78d-5877-4590-a485-bd9d670b62f5\") " pod="kube-system/kube-proxy-bt4jh"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.816762    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f5r64\" (UniqueName: \"kubernetes.io/projected/0545b78d-5877-4590-a485-bd9d670b62f5-kube-api-access-f5r64\") pod \"kube-proxy-bt4jh\" (UID: \"0545b78d-5877-4590-a485-bd9d670b62f5\") " pod="kube-system/kube-proxy-bt4jh"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.816776    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/0545b78d-5877-4590-a485-bd9d670b62f5-xtables-lock\") pod \"kube-proxy-bt4jh\" (UID: \"0545b78d-5877-4590-a485-bd9d670b62f5\") " pod="kube-system/kube-proxy-bt4jh"
Feb 06 17:07:36 minikube kubelet[2198]: I0206 17:07:36.823364    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/0545b78d-5877-4590-a485-bd9d670b62f5-kube-proxy\") pod \"kube-proxy-bt4jh\" (UID: \"0545b78d-5877-4590-a485-bd9d670b62f5\") " pod="kube-system/kube-proxy-bt4jh"
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.057563    2198 projected.go:291] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.057585    2198 projected.go:196] Error preparing data for projected volume kube-api-access-llwzv for pod kube-system/kindnet-kbzfm: configmap "kube-root-ca.crt" not found
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.057641    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/ef168f5d-82c9-4802-8351-9b81b285a9ae-kube-api-access-llwzv podName:ef168f5d-82c9-4802-8351-9b81b285a9ae nodeName:}" failed. No retries permitted until 2026-02-06 17:07:37.557626745 +0000 UTC m=+6.646232363 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-llwzv" (UniqueName: "kubernetes.io/projected/ef168f5d-82c9-4802-8351-9b81b285a9ae-kube-api-access-llwzv") pod "kindnet-kbzfm" (UID: "ef168f5d-82c9-4802-8351-9b81b285a9ae") : configmap "kube-root-ca.crt" not found
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.175763    2198 projected.go:291] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.175784    2198 projected.go:196] Error preparing data for projected volume kube-api-access-f5r64 for pod kube-system/kube-proxy-bt4jh: configmap "kube-root-ca.crt" not found
Feb 06 17:07:37 minikube kubelet[2198]: E0206 17:07:37.175945    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/0545b78d-5877-4590-a485-bd9d670b62f5-kube-api-access-f5r64 podName:0545b78d-5877-4590-a485-bd9d670b62f5 nodeName:}" failed. No retries permitted until 2026-02-06 17:07:37.675811456 +0000 UTC m=+6.764417076 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-f5r64" (UniqueName: "kubernetes.io/projected/0545b78d-5877-4590-a485-bd9d670b62f5-kube-api-access-f5r64") pod "kube-proxy-bt4jh" (UID: "0545b78d-5877-4590-a485-bd9d670b62f5") : configmap "kube-root-ca.crt" not found
Feb 06 17:07:43 minikube kubelet[2198]: I0206 17:07:43.655958    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="244b71b3d70bcfefa8cd8703234d0f2bb023279a96e81c0ad3d8b8ada20d24bc"
Feb 06 17:07:43 minikube kubelet[2198]: I0206 17:07:43.924649    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e03f71b1a68ec0783b83a920114a669187c1529930c17cc1ba9a59cef3e1f4a6"
Feb 06 17:07:44 minikube kubelet[2198]: I0206 17:07:44.974688    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-bt4jh" podStartSLOduration=9.974671285 podStartE2EDuration="9.974671285s" podCreationTimestamp="2026-02-06 17:07:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-02-06 17:07:44.045501954 +0000 UTC m=+13.134107576" watchObservedRunningTime="2026-02-06 17:07:44.974671285 +0000 UTC m=+14.063276906"
Feb 06 17:08:59 minikube kubelet[2198]: I0206 17:08:59.571277    2198 kubelet_node_status.go:439] "Fast updating node status as it just became ready"
Feb 06 17:09:00 minikube kubelet[2198]: I0206 17:09:00.044734    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kindnet-kbzfm" podStartSLOduration=25.303465474 podStartE2EDuration="1m25.044721625s" podCreationTimestamp="2026-02-06 17:07:35 +0000 UTC" firstStartedPulling="2026-02-06 17:07:43.632123115 +0000 UTC m=+12.720728727" lastFinishedPulling="2026-02-06 17:08:43.373379265 +0000 UTC m=+72.461984878" observedRunningTime="2026-02-06 17:08:47.827222121 +0000 UTC m=+76.915827744" watchObservedRunningTime="2026-02-06 17:09:00.044721625 +0000 UTC m=+89.133327243"
Feb 06 17:09:00 minikube kubelet[2198]: I0206 17:09:00.267699    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/fc4d809a-2bda-479d-bef7-0ef5516ef716-tmp\") pod \"storage-provisioner\" (UID: \"fc4d809a-2bda-479d-bef7-0ef5516ef716\") " pod="kube-system/storage-provisioner"
Feb 06 17:09:00 minikube kubelet[2198]: I0206 17:09:00.267727    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/e3d62037-c827-4d83-af20-8dd27878eff1-config-volume\") pod \"coredns-66bc5c9577-8sc9r\" (UID: \"e3d62037-c827-4d83-af20-8dd27878eff1\") " pod="kube-system/coredns-66bc5c9577-8sc9r"
Feb 06 17:09:00 minikube kubelet[2198]: I0206 17:09:00.267736    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-txbsz\" (UniqueName: \"kubernetes.io/projected/fc4d809a-2bda-479d-bef7-0ef5516ef716-kube-api-access-txbsz\") pod \"storage-provisioner\" (UID: \"fc4d809a-2bda-479d-bef7-0ef5516ef716\") " pod="kube-system/storage-provisioner"
Feb 06 17:09:00 minikube kubelet[2198]: I0206 17:09:00.267745    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-65k5f\" (UniqueName: \"kubernetes.io/projected/e3d62037-c827-4d83-af20-8dd27878eff1-kube-api-access-65k5f\") pod \"coredns-66bc5c9577-8sc9r\" (UID: \"e3d62037-c827-4d83-af20-8dd27878eff1\") " pod="kube-system/coredns-66bc5c9577-8sc9r"
Feb 06 17:09:03 minikube kubelet[2198]: I0206 17:09:03.642189    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c6134f171045285568bb2c0aadd3a07a1d1037388bc3722028e0a13b2f7043fe"
Feb 06 17:09:03 minikube kubelet[2198]: I0206 17:09:03.891417    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="955b255b01ebb4bfb64aae214cfbb58235f533c4eee1b680cf1c4cbbdb1fa840"
Feb 06 17:09:08 minikube kubelet[2198]: I0206 17:09:08.046903    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-66bc5c9577-8sc9r" podStartSLOduration=91.046892706 podStartE2EDuration="1m31.046892706s" podCreationTimestamp="2026-02-06 17:07:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-02-06 17:09:08.046874461 +0000 UTC m=+97.135480085" watchObservedRunningTime="2026-02-06 17:09:08.046892706 +0000 UTC m=+97.135498328"
Feb 06 17:09:08 minikube kubelet[2198]: I0206 17:09:08.176343    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=88.176330328 podStartE2EDuration="1m28.176330328s" podCreationTimestamp="2026-02-06 17:07:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2026-02-06 17:09:08.107171229 +0000 UTC m=+97.195776852" watchObservedRunningTime="2026-02-06 17:09:08.176330328 +0000 UTC m=+97.264935950"
Feb 06 17:11:45 minikube kubelet[2198]: I0206 17:11:45.149959    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vvdll\" (UniqueName: \"kubernetes.io/projected/d1785e7b-05ff-43e5-9c7c-579901489a3d-kube-api-access-vvdll\") pod \"ingress-nginx-controller-85d4c799dd-w2g4m\" (UID: \"d1785e7b-05ff-43e5-9c7c-579901489a3d\") " pod="ingress-nginx/ingress-nginx-controller-85d4c799dd-w2g4m"
Feb 06 17:11:45 minikube kubelet[2198]: I0206 17:11:45.153789    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert\") pod \"ingress-nginx-controller-85d4c799dd-w2g4m\" (UID: \"d1785e7b-05ff-43e5-9c7c-579901489a3d\") " pod="ingress-nginx/ingress-nginx-controller-85d4c799dd-w2g4m"
Feb 06 17:11:45 minikube kubelet[2198]: I0206 17:11:45.259565    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-szj6s\" (UniqueName: \"kubernetes.io/projected/85f973e5-98f8-4e73-8a54-0b43cc2aba63-kube-api-access-szj6s\") pod \"ingress-nginx-admission-patch-wwtsc\" (UID: \"85f973e5-98f8-4e73-8a54-0b43cc2aba63\") " pod="ingress-nginx/ingress-nginx-admission-patch-wwtsc"
Feb 06 17:11:45 minikube kubelet[2198]: E0206 17:11:45.260027    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:11:45 minikube kubelet[2198]: E0206 17:11:45.262987    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:11:45.762973194 +0000 UTC m=+254.851578807 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:11:45 minikube kubelet[2198]: I0206 17:11:45.372110    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rcphb\" (UniqueName: \"kubernetes.io/projected/267e8c5f-7c2a-4a4c-9cd9-e83773a20a68-kube-api-access-rcphb\") pod \"ingress-nginx-admission-create-4ztft\" (UID: \"267e8c5f-7c2a-4a4c-9cd9-e83773a20a68\") " pod="ingress-nginx/ingress-nginx-admission-create-4ztft"
Feb 06 17:11:45 minikube kubelet[2198]: E0206 17:11:45.806189    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:11:45 minikube kubelet[2198]: E0206 17:11:45.820748    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:11:46.820731842 +0000 UTC m=+255.909337460 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:11:46 minikube kubelet[2198]: E0206 17:11:46.887886    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:11:46 minikube kubelet[2198]: E0206 17:11:46.887932    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:11:48.887923425 +0000 UTC m=+257.976529036 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:11:48 minikube kubelet[2198]: E0206 17:11:48.906203    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:11:48 minikube kubelet[2198]: E0206 17:11:48.906611    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:11:52.906592161 +0000 UTC m=+261.995197781 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:11:49 minikube kubelet[2198]: I0206 17:11:49.399375    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3cca0aa42fe10ea604c5865487c2a1b81d9d73a2a941c4b5e0a816793eec4a9c"
Feb 06 17:11:49 minikube kubelet[2198]: I0206 17:11:49.788204    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="91773a9c8f9800a8596264a847a8e388d72380ecfdf471688866170b79174a46"
Feb 06 17:11:52 minikube kubelet[2198]: E0206 17:11:52.992039    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:11:52 minikube kubelet[2198]: E0206 17:11:52.992210    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:12:00.992198386 +0000 UTC m=+270.080804011 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:12:01 minikube kubelet[2198]: E0206 17:12:01.087724    2198 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 06 17:12:01 minikube kubelet[2198]: E0206 17:12:01.095573    2198 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert podName:d1785e7b-05ff-43e5-9c7c-579901489a3d nodeName:}" failed. No retries permitted until 2026-02-06 17:12:17.095555748 +0000 UTC m=+286.184161366 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/d1785e7b-05ff-43e5-9c7c-579901489a3d-webhook-cert") pod "ingress-nginx-controller-85d4c799dd-w2g4m" (UID: "d1785e7b-05ff-43e5-9c7c-579901489a3d") : secret "ingress-nginx-admission" not found
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.370577    2198 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rcphb\" (UniqueName: \"kubernetes.io/projected/267e8c5f-7c2a-4a4c-9cd9-e83773a20a68-kube-api-access-rcphb\") pod \"267e8c5f-7c2a-4a4c-9cd9-e83773a20a68\" (UID: \"267e8c5f-7c2a-4a4c-9cd9-e83773a20a68\") "
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.373492    2198 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/267e8c5f-7c2a-4a4c-9cd9-e83773a20a68-kube-api-access-rcphb" (OuterVolumeSpecName: "kube-api-access-rcphb") pod "267e8c5f-7c2a-4a4c-9cd9-e83773a20a68" (UID: "267e8c5f-7c2a-4a4c-9cd9-e83773a20a68"). InnerVolumeSpecName "kube-api-access-rcphb". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.471167    2198 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-szj6s\" (UniqueName: \"kubernetes.io/projected/85f973e5-98f8-4e73-8a54-0b43cc2aba63-kube-api-access-szj6s\") pod \"85f973e5-98f8-4e73-8a54-0b43cc2aba63\" (UID: \"85f973e5-98f8-4e73-8a54-0b43cc2aba63\") "
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.471280    2198 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-rcphb\" (UniqueName: \"kubernetes.io/projected/267e8c5f-7c2a-4a4c-9cd9-e83773a20a68-kube-api-access-rcphb\") on node \"minikube\" DevicePath \"\""
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.473926    2198 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/85f973e5-98f8-4e73-8a54-0b43cc2aba63-kube-api-access-szj6s" (OuterVolumeSpecName: "kube-api-access-szj6s") pod "85f973e5-98f8-4e73-8a54-0b43cc2aba63" (UID: "85f973e5-98f8-4e73-8a54-0b43cc2aba63"). InnerVolumeSpecName "kube-api-access-szj6s". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.573513    2198 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-szj6s\" (UniqueName: \"kubernetes.io/projected/85f973e5-98f8-4e73-8a54-0b43cc2aba63-kube-api-access-szj6s\") on node \"minikube\" DevicePath \"\""
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.774460    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3cca0aa42fe10ea604c5865487c2a1b81d9d73a2a941c4b5e0a816793eec4a9c"
Feb 06 17:12:15 minikube kubelet[2198]: I0206 17:12:15.804339    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="91773a9c8f9800a8596264a847a8e388d72380ecfdf471688866170b79174a46"
Feb 06 17:13:44 minikube kubelet[2198]: I0206 17:13:44.755748    2198 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-85d4c799dd-w2g4m" podStartSLOduration=46.883300602 podStartE2EDuration="2m0.755736814s" podCreationTimestamp="2026-02-06 17:11:44 +0000 UTC" firstStartedPulling="2026-02-06 17:12:18.566896548 +0000 UTC m=+287.655502159" lastFinishedPulling="2026-02-06 17:13:32.439332758 +0000 UTC m=+361.527938371" observedRunningTime="2026-02-06 17:13:33.471430694 +0000 UTC m=+362.560036339" watchObservedRunningTime="2026-02-06 17:13:44.755736814 +0000 UTC m=+373.844342433"
Feb 06 17:14:13 minikube kubelet[2198]: I0206 17:14:13.226914    2198 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rbzbt\" (UniqueName: \"kubernetes.io/projected/322646eb-fac6-45a2-853c-4c283f5987e3-kube-api-access-rbzbt\") pod \"registry-proxy-8t8s4\" (UID: \"322646eb-fac6-45a2-853c-4c283f5987e3\") " pod="kube-system/registry-proxy-8t8s4"
Feb 06 17:14:24 minikube kubelet[2198]: I0206 17:14:24.978703    2198 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="54e7d8058ccfd1b1b0bf51a44005e6f203fe021e7dc43fbfb33401ccb81477ad"

