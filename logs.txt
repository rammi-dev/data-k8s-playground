
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMMAND   ‚îÇ                                                                                                                 ARGS                                                                                                                  ‚îÇ PROFILE  ‚îÇ  USER   ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ completion ‚îÇ bash                                                                                                                                                                                                                                  ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:46 UTC ‚îÇ 06 Feb 26 13:46 UTC ‚îÇ
‚îÇ completion ‚îÇ bash                                                                                                                                                                                                                                  ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:50 UTC ‚îÇ 06 Feb 26 13:50 UTC ‚îÇ
‚îÇ start      ‚îÇ --driver=docker --nodes=3 --cpus=5 --memory=12970 --disk-size=40g --kubernetes-version=v1.34.0 --extra-config=kubelet.housekeeping-interval=10s --extra-config=kubelet.max-pods=50 --extra-config=kubelet.serialize-image-pulls=false ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:50 UTC ‚îÇ 06 Feb 26 13:55 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable ingress                                                                                                                                                                                                                        ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:56 UTC ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable ingress-dns                                                                                                                                                                                                                    ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable dashboard                                                                                                                                                                                                                      ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable metrics-server                                                                                                                                                                                                                 ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable default-storageclass                                                                                                                                                                                                           ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable storage-provisioner                                                                                                                                                                                                            ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:57 UTC ‚îÇ 06 Feb 26 13:58 UTC ‚îÇ
‚îÇ addons     ‚îÇ enable csi-hostpath-driver                                                                                                                                                                                                            ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 13:58 UTC ‚îÇ                     ‚îÇ
‚îÇ completion ‚îÇ bash                                                                                                                                                                                                                                  ‚îÇ minikube ‚îÇ vagrant ‚îÇ v1.38.0 ‚îÇ 06 Feb 26 14:01 UTC ‚îÇ 06 Feb 26 14:01 UTC ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2026/02/06 13:50:58
Running on machine: data-playground
Binary: Built with gc go1.25.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0206 13:50:58.486535    2080 out.go:360] Setting OutFile to fd 1 ...
I0206 13:50:58.486640    2080 out.go:413] isatty.IsTerminal(1) = true
I0206 13:50:58.486642    2080 out.go:374] Setting ErrFile to fd 2...
I0206 13:50:58.486644    2080 out.go:413] isatty.IsTerminal(2) = true
I0206 13:50:58.486743    2080 root.go:338] Updating PATH: /home/vagrant/.minikube/bin
W0206 13:50:58.486818    2080 root.go:314] Error reading config file at /home/vagrant/.minikube/config/config.json: open /home/vagrant/.minikube/config/config.json: no such file or directory
I0206 13:50:58.486934    2080 out.go:368] Setting JSON to false
I0206 13:50:58.487726    2080 start.go:134] hostinfo: {"hostname":"data-playground","uptime":110,"bootTime":1770385748,"procs":254,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-134-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"192972b9-1c7f-4e25-afd8-23acdf6b03e9"}
I0206 13:50:58.487761    2080 start.go:144] virtualization: vbox guest
I0206 13:50:58.489006    2080 out.go:179] üòÑ  minikube v1.38.0 on Ubuntu 22.04 (vbox/amd64)
I0206 13:50:58.490314    2080 out.go:179]     ‚ñ™ MINIKUBE_K8S_VERSION=v1.34.0
I0206 13:50:58.490562    2080 notify.go:220] Checking for updates...
W0206 13:50:58.491196    2080 preload.go:371] Failed to list preload files: open /home/vagrant/.minikube/cache/preloaded-tarball: no such file or directory
I0206 13:50:58.492426    2080 out.go:179]     ‚ñ™ MINIKUBE_CPUS=5
I0206 13:50:58.494752    2080 out.go:179]     ‚ñ™ MINIKUBE_NODES=3
I0206 13:50:58.495627    2080 out.go:179]     ‚ñ™ MINIKUBE_MEMORY=12970
I0206 13:50:58.496322    2080 out.go:179]     ‚ñ™ MINIKUBE_DRIVER=docker
I0206 13:50:58.497021    2080 out.go:179]     ‚ñ™ MINIKUBE_DISK_SIZE=40g
I0206 13:50:58.497926    2080 out.go:179]     ‚ñ™ MINIKUBE_EXTRA_CONFIG=kubelet.housekeeping-interval=10s kubelet.max-pods=50 kubelet.serialize-image-pulls=false
I0206 13:50:58.498947    2080 driver.go:422] Setting default libvirt URI to qemu:///system
I0206 13:50:58.652826    2080 docker.go:125] docker version: linux-29.2.1:Docker Engine - Community
I0206 13:50:58.652916    2080 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0206 13:50:58.990594    2080 info.go:266] docker info: {ID:4e026bac-3bb6-43df-9b72-b685e8b47580 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:48 SystemTime:2026-02-06 13:50:58.978943027 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-134-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:42054021120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:data-playground Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
I0206 13:50:58.990652    2080 docker.go:320] overlay module found
I0206 13:50:58.991534    2080 out.go:179] ‚ú®  Using the docker driver based on user configuration
I0206 13:50:58.992306    2080 start.go:310] selected driver: docker
I0206 13:50:58.992310    2080 start.go:932] validating driver "docker" against <nil>
I0206 13:50:58.992317    2080 start.go:943] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0206 13:50:58.992885    2080 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0206 13:50:59.056935    2080 info.go:266] docker info: {ID:4e026bac-3bb6-43df-9b72-b685e8b47580 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:24 OomKillDisable:false NGoroutines:48 SystemTime:2026-02-06 13:50:59.047256722 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.15.0-134-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:18 MemTotal:42054021120 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:data-playground Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
W0206 13:50:59.057526    2080 out.go:285] ‚ùó  Starting v1.39.0, minikube will default to "containerd" container runtime. See #21973 for more info.
I0206 13:50:59.057727    2080 start_flags.go:332] no existing cluster config was found, will generate one from the flags 
I0206 13:50:59.058251    2080 start_flags.go:1000] Wait components to verify : map[apiserver:true system_pods:true]
I0206 13:50:59.058977    2080 out.go:179] üìå  Using Docker driver with root privileges
I0206 13:50:59.059711    2080 cni.go:83] Creating CNI manager for ""
I0206 13:50:59.059739    2080 cni.go:135] multinode detected (0 nodes found), recommending kindnet
I0206 13:50:59.059742    2080 start_flags.go:341] Found "CNI" CNI - setting NetworkPlugin=cni
I0206 13:50:59.059784    2080 start.go:357] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 13:50:59.061146    2080 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0206 13:50:59.061872    2080 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 13:50:59.062559    2080 out.go:179] üöú  Pulling base image v0.0.49 ...
I0206 13:50:59.063465    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:50:59.063574    2080 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 13:50:59.081939    2080 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 13:50:59.082329    2080 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 13:50:59.082387    2080 image.go:151] Writing gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 13:50:59.186076    2080 preload.go:147] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0206 13:50:59.186087    2080 cache.go:66] Caching tarball of preloaded images
I0206 13:50:59.186252    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:50:59.187943    2080 out.go:179] üíæ  Downloading Kubernetes v1.34.0 preload ...
I0206 13:50:59.189242    2080 preload.go:268] Downloading preload from https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0206 13:50:59.189246    2080 preload.go:335] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 from gcs api...
I0206 13:50:59.362662    2080 preload.go:312] Got checksum from GCS API "994a4de1464928e89c992dfd0a962e35"
I0206 13:50:59.362772    2080 download.go:113] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I0206 13:51:21.742129    2080 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 13:51:21.742157    2080 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 13:51:28.685804    2080 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 13:51:28.686267    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:51:28.687430    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/config.json: {Name:mkbf9f977730e32076054a9dc7ff2814bc45f0c2 Timeout:1m0s Delay:500ms}
I0206 13:51:48.430377    2080 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 13:51:48.430421    2080 cache.go:244] Successfully downloaded all kic artifacts
I0206 13:51:48.430449    2080 start.go:359] acquireMachinesLock for minikube: {Name:mk50794f3b668552bcb175548a808224fc99ceb9 Timeout:10m0s Delay:500ms}
I0206 13:51:48.430498    2080 start.go:363] duration metric: took 41.211¬µs to acquireMachinesLock for "minikube"
I0206 13:51:48.430702    2080 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0206 13:51:48.430817    2080 start.go:124] createHost starting for "" (driver="docker")
I0206 13:51:48.432138    2080 out.go:252] üî•  Creating docker container (CPUs=5, Memory=12970MB) ...
I0206 13:51:48.432878    2080 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 13:51:48.432892    2080 client.go:173] LocalClient.Create starting
I0206 13:51:48.433130    2080 main.go:144] libmachine: Creating CA: /home/vagrant/.minikube/certs/ca.pem
I0206 13:51:48.449779    2080 main.go:144] libmachine: Creating client certificate: /home/vagrant/.minikube/certs/cert.pem
I0206 13:51:48.483102    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0206 13:51:48.504159    2080 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0206 13:51:48.504191    2080 network_create.go:285] running [docker network inspect minikube] to gather additional debugging logs...
I0206 13:51:48.504200    2080 cli_runner.go:164] Run: docker network inspect minikube
W0206 13:51:48.526676    2080 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0206 13:51:48.526687    2080 network_create.go:288] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0206 13:51:48.526717    2080 network_create.go:290] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0206 13:51:48.526886    2080 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:51:48.558255    2080 network.go:205] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0000c8390}
I0206 13:51:48.558275    2080 network_create.go:125] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0206 13:51:48.558301    2080 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0206 13:51:48.626554    2080 network_create.go:109] docker network minikube 192.168.49.0/24 created
I0206 13:51:48.626568    2080 kic.go:120] calculated static IP "192.168.49.2" for the "minikube" container
I0206 13:51:48.626604    2080 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 13:51:48.654278    2080 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0206 13:51:48.674923    2080 oci.go:102] Successfully created a docker volume minikube
I0206 13:51:48.674958    2080 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 13:51:51.161180    2080 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (2.486190897s)
I0206 13:51:51.161195    2080 oci.go:106] Successfully prepared a docker volume minikube
I0206 13:51:51.161431    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:51:51.161439    2080 kic.go:193] Starting extracting preloaded images to volume ...
I0206 13:51:51.161475    2080 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 13:51:55.520886    2080 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (4.359387092s)
I0206 13:51:55.520907    2080 kic.go:202] duration metric: took 4.359466125s to extract preloaded images to volume ...
W0206 13:51:55.520958    2080 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 13:51:55.520974    2080 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 13:51:55.520995    2080 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 13:51:55.619834    2080 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=12970mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 13:51:56.145740    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0206 13:51:56.218808    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:51:56.286511    2080 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0206 13:51:56.382065    2080 oci.go:143] the created container "minikube" has a running status.
I0206 13:51:56.382081    2080 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube/id_rsa...
I0206 13:51:56.399422    2080 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 13:51:56.437406    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:51:56.541243    2080 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 13:51:56.541250    2080 kic_runner.go:113] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 13:51:56.913432    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:51:57.474986    2080 machine.go:96] provisionDockerMachine start ...
I0206 13:51:57.475065    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:57.777124    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:57.777265    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:57.777270    2080 main.go:144] libmachine: About to run SSH command:
hostname
I0206 13:51:58.191131    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0206 13:51:58.191144    2080 ubuntu.go:182] provisioning hostname "minikube"
I0206 13:51:58.191180    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:58.237899    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:58.238023    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:58.238027    2080 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0206 13:51:58.390902    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0206 13:51:58.390941    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:58.423942    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:58.424059    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:58.424066    2080 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 13:51:58.617314    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 13:51:58.617333    2080 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 13:51:58.617347    2080 ubuntu.go:190] setting up certificates
I0206 13:51:58.617351    2080 provision.go:83] configureAuth start
I0206 13:51:58.617381    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 13:51:58.637386    2080 provision.go:142] copyHostCerts
I0206 13:51:58.637419    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 13:51:58.637478    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 13:51:58.637500    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 13:51:58.637521    2080 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0206 13:51:58.751448    2080 provision.go:176] copyRemoteCerts
I0206 13:51:58.751758    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 13:51:58.751808    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:58.775014    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:51:58.890440    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 13:51:58.908319    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1184 bytes)
I0206 13:51:58.956227    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0206 13:51:59.009630    2080 provision.go:86] duration metric: took 392.271264ms to configureAuth
I0206 13:51:59.009642    2080 ubuntu.go:206] setting minikube options for container-runtime
I0206 13:51:59.009744    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:51:59.009770    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:59.033241    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:59.033350    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:59.033355    2080 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 13:51:59.171557    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 13:51:59.171566    2080 ubuntu.go:71] root file system type: overlay
I0206 13:51:59.171620    2080 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 13:51:59.171653    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:59.190691    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:59.190807    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:59.190840    2080 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 13:51:59.339126    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 13:51:59.339167    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:51:59.361342    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:51:59.361455    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0206 13:51:59.361462    2080 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 13:52:00.853809    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 13:51:59.335852715 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 13:52:00.853825    2080 machine.go:99] duration metric: took 3.37882998s to provisionDockerMachine
I0206 13:52:00.853831    2080 client.go:176] duration metric: took 12.420936619s to LocalClient.Create
I0206 13:52:00.853845    2080 start.go:166] duration metric: took 12.420967666s to libmachine.API.Create "minikube"
I0206 13:52:00.853849    2080 start.go:292] postStartSetup for "minikube" (driver="docker")
I0206 13:52:00.853855    2080 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 13:52:00.853886    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 13:52:00.853911    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:00.873370    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:00.981674    2080 ssh_runner.go:194] Run: cat /etc/os-release
I0206 13:52:00.985759    2080 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 13:52:00.985769    2080 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 13:52:00.985776    2080 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 13:52:00.986110    2080 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 13:52:00.986391    2080 start.go:295] duration metric: took 132.537123ms for postStartSetup
I0206 13:52:00.986581    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 13:52:01.006769    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:52:01.006978    2080 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 13:52:01.006996    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:01.026934    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:01.134013    2080 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 13:52:01.141537    2080 start.go:127] duration metric: took 12.710710512s to createHost
I0206 13:52:01.141548    2080 start.go:82] releasing machines lock for "minikube", held for 12.711044036s
I0206 13:52:01.141588    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0206 13:52:01.163584    2080 ssh_runner.go:194] Run: cat /version.json
I0206 13:52:01.163623    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:01.163685    2080 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 13:52:01.163728    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:01.266269    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:01.267375    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:01.481276    2080 ssh_runner.go:194] Run: systemctl --version
I0206 13:52:01.487817    2080 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0206 13:52:01.495313    2080 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 13:52:01.495342    2080 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 13:52:01.519706    2080 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 13:52:01.519717    2080 start.go:497] detecting cgroup driver to use...
I0206 13:52:01.519736    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:52:01.519853    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:52:01.535870    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 13:52:01.545275    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 13:52:01.554975    2080 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 13:52:01.555002    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 13:52:01.564218    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:52:01.574980    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 13:52:01.584602    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:52:01.594830    2080 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 13:52:01.604270    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 13:52:01.613527    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 13:52:01.623157    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 13:52:01.633242    2080 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 13:52:01.640953    2080 crio.go:165] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 1
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0206 13:52:01.640973    2080 ssh_runner.go:194] Run: sudo modprobe br_netfilter
I0206 13:52:01.657647    2080 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 13:52:01.667350    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:01.734148    2080 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 13:52:01.819497    2080 start.go:497] detecting cgroup driver to use...
I0206 13:52:01.819523    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:52:01.819547    2080 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 13:52:01.831044    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:52:01.841603    2080 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 13:52:01.877993    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:52:01.893030    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 13:52:01.903990    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:52:01.917527    2080 ssh_runner.go:194] Run: which cri-dockerd
I0206 13:52:01.922100    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 13:52:01.929463    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 13:52:01.940997    2080 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 13:52:02.008441    2080 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 13:52:02.074549    2080 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 13:52:02.074625    2080 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 13:52:02.087939    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 13:52:02.097931    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:02.173145    2080 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 13:52:03.502766    2080 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.329603087s)
I0206 13:52:03.502801    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 13:52:03.512882    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 13:52:03.523633    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:52:03.534240    2080 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 13:52:03.614122    2080 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 13:52:03.684801    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:03.780721    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 13:52:03.798541    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 13:52:03.813586    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:03.886948    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 13:52:03.971581    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:52:03.981843    2080 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 13:52:03.981886    2080 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 13:52:03.985568    2080 start.go:575] Will wait 60s for crictl version
I0206 13:52:03.985598    2080 ssh_runner.go:194] Run: which crictl
I0206 13:52:03.989891    2080 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 13:52:04.023401    2080 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 13:52:04.023435    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:52:04.053588    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:52:04.084364    2080 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 13:52:04.084552    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:52:04.103108    2080 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 13:52:04.107760    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:52:04.119168    2080 out.go:179]     ‚ñ™ kubelet.housekeeping-interval=10s
I0206 13:52:04.120015    2080 out.go:179]     ‚ñ™ kubelet.max-pods=50
I0206 13:52:04.121443    2080 out.go:179]     ‚ñ™ kubelet.serialize-image-pulls=false
I0206 13:52:04.122299    2080 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} ...
I0206 13:52:04.122371    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:52:04.122398    2080 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0206 13:52:04.158641    2080 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0206 13:52:04.158650    2080 docker.go:623] Images already preloaded, skipping extraction
I0206 13:52:04.158694    2080 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0206 13:52:04.180278    2080 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0206 13:52:04.180287    2080 cache_images.go:85] Images are preloaded, skipping loading
I0206 13:52:04.180293    2080 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I0206 13:52:04.180350    2080 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.2 --serialize-image-pulls=false

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 13:52:04.180387    2080 ssh_runner.go:194] Run: docker info --format {{.CgroupDriver}}
I0206 13:52:04.245870    2080 cni.go:83] Creating CNI manager for ""
I0206 13:52:04.245879    2080 cni.go:135] multinode detected (1 nodes found), recommending kindnet
I0206 13:52:04.246174    2080 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0206 13:52:04.246239    2080 kubeadm.go:196] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0206 13:52:04.246308    2080 kubeadm.go:202] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0206 13:52:04.246347    2080 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 13:52:04.255060    2080 binaries.go:50] Found k8s binaries, skipping transfer
I0206 13:52:04.255091    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0206 13:52:04.263032    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (379 bytes)
I0206 13:52:04.275227    2080 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 13:52:04.289181    2080 ssh_runner.go:361] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0206 13:52:04.300923    2080 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 13:52:04.304996    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:52:04.314813    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:04.385379    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:52:04.397189    2080 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.2
I0206 13:52:04.397196    2080 certs.go:193] generating shared ca certs ...
I0206 13:52:04.397204    2080 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 13:52:04.397260    2080 certs.go:239] generating "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 13:52:04.406136    2080 crypto.go:158] Writing cert to /home/vagrant/.minikube/ca.crt ...
I0206 13:52:04.406145    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/ca.crt: {Name:mk54fb921fea0dc6fe3b2ce2b1dc3fad05642eb4 Timeout:1m0s Delay:500ms}
I0206 13:52:04.406212    2080 crypto.go:166] Writing key to /home/vagrant/.minikube/ca.key ...
I0206 13:52:04.406215    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/ca.key: {Name:mk94dad2f30a78979a99347718993843a6edb56e Timeout:1m0s Delay:500ms}
I0206 13:52:04.406239    2080 certs.go:239] generating "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 13:52:04.442305    2080 crypto.go:158] Writing cert to /home/vagrant/.minikube/proxy-client-ca.crt ...
I0206 13:52:04.442315    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/proxy-client-ca.crt: {Name:mkef714d0b413909475656b5e603dc70aa3ec74c Timeout:1m0s Delay:500ms}
I0206 13:52:04.442391    2080 crypto.go:166] Writing key to /home/vagrant/.minikube/proxy-client-ca.key ...
I0206 13:52:04.442393    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/proxy-client-ca.key: {Name:mk3c4fb3ff9963315144e7d56d94d061f5ff514b Timeout:1m0s Delay:500ms}
I0206 13:52:04.442420    2080 certs.go:255] generating profile certs ...
I0206 13:52:04.442457    2080 certs.go:362] generating signed profile cert for "minikube-user": /home/vagrant/.minikube/profiles/minikube/client.key
I0206 13:52:04.442463    2080 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/client.crt with IP's: []
I0206 13:52:04.458649    2080 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/client.crt ...
I0206 13:52:04.458658    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/client.crt: {Name:mkf790f0c34d5ddb75771609e12fde6153db43d9 Timeout:1m0s Delay:500ms}
I0206 13:52:04.458730    2080 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/client.key ...
I0206 13:52:04.458733    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/client.key: {Name:mk0f0945b480ffce7f464c4edb019923256f817f Timeout:1m0s Delay:500ms}
I0206 13:52:04.458758    2080 certs.go:362] generating signed profile cert for "minikube": /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0206 13:52:04.458765    2080 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0206 13:52:04.508434    2080 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0206 13:52:04.508445    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk6f33b21b8d250aaf51d0fcac514b1fe047019b Timeout:1m0s Delay:500ms}
I0206 13:52:04.508523    2080 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0206 13:52:04.508525    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk5f5ba42dbe6233edabcc1790c1948752e4571e Timeout:1m0s Delay:500ms}
I0206 13:52:04.508551    2080 certs.go:380] copying /home/vagrant/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/vagrant/.minikube/profiles/minikube/apiserver.crt
I0206 13:52:04.508589    2080 certs.go:384] copying /home/vagrant/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/vagrant/.minikube/profiles/minikube/apiserver.key
I0206 13:52:04.508607    2080 certs.go:362] generating signed profile cert for "aggregator": /home/vagrant/.minikube/profiles/minikube/proxy-client.key
I0206 13:52:04.508614    2080 crypto.go:70] Generating cert /home/vagrant/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0206 13:52:04.584370    2080 crypto.go:158] Writing cert to /home/vagrant/.minikube/profiles/minikube/proxy-client.crt ...
I0206 13:52:04.584381    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/proxy-client.crt: {Name:mk775a814f966808cd93d4d7921f7632223b601d Timeout:1m0s Delay:500ms}
I0206 13:52:04.584678    2080 crypto.go:166] Writing key to /home/vagrant/.minikube/profiles/minikube/proxy-client.key ...
I0206 13:52:04.584683    2080 lock.go:60] WriteFile acquiring /home/vagrant/.minikube/profiles/minikube/proxy-client.key: {Name:mk966fe2af01fcb352fc6da315d62a37c8f905e4 Timeout:1m0s Delay:500ms}
I0206 13:52:04.584764    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 13:52:04.584781    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 13:52:04.584793    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 13:52:04.584804    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 13:52:04.585315    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 13:52:04.602461    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 13:52:04.619680    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 13:52:04.638463    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 13:52:04.655829    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0206 13:52:04.675115    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0206 13:52:04.693599    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0206 13:52:04.709968    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0206 13:52:04.727478    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 13:52:04.744040    2080 ssh_runner.go:361] scp memory --> /var/lib/minikube/kubeconfig (722 bytes)
I0206 13:52:04.757417    2080 ssh_runner.go:194] Run: openssl version
I0206 13:52:04.762660    2080 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 13:52:04.770309    2080 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 13:52:04.777580    2080 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 13:52:04.781867    2080 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 13:52:04.781888    2080 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 13:52:04.805557    2080 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 13:52:04.814554    2080 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 13:52:04.822704    2080 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 13:52:04.827330    2080 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 13:52:04.827352    2080 kubeadm.go:400] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 13:52:04.827399    2080 ssh_runner.go:194] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0206 13:52:04.846388    2080 ssh_runner.go:194] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0206 13:52:04.854194    2080 ssh_runner.go:194] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0206 13:52:04.861629    2080 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0206 13:52:04.861651    2080 ssh_runner.go:194] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0206 13:52:04.869284    2080 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0206 13:52:04.869289    2080 kubeadm.go:157] found existing configuration files:

I0206 13:52:04.869307    2080 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0206 13:52:04.876536    2080 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0206 13:52:04.876836    2080 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/admin.conf
I0206 13:52:04.883658    2080 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0206 13:52:04.891109    2080 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0206 13:52:04.891129    2080 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0206 13:52:04.898837    2080 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0206 13:52:04.905906    2080 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0206 13:52:04.905923    2080 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0206 13:52:04.913274    2080 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0206 13:52:04.920550    2080 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0206 13:52:04.920566    2080 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0206 13:52:04.927779    2080 ssh_runner.go:285] Start: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0206 13:52:04.966024    2080 kubeadm.go:318] [init] Using Kubernetes version: v1.34.0
I0206 13:52:04.966058    2080 kubeadm.go:318] [preflight] Running pre-flight checks
I0206 13:52:04.986707    2080 kubeadm.go:318] [preflight] The system verification failed. Printing the output from the verification:
I0206 13:52:04.986836    2080 kubeadm.go:318] [0;37mKERNEL_VERSION[0m: [0;32m5.15.0-134-generic[0m
I0206 13:52:04.986857    2080 kubeadm.go:318] [0;37mOS[0m: [0;32mLinux[0m
I0206 13:52:04.986880    2080 kubeadm.go:318] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I0206 13:52:04.986914    2080 kubeadm.go:318] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I0206 13:52:04.986941    2080 kubeadm.go:318] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I0206 13:52:04.986997    2080 kubeadm.go:318] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I0206 13:52:04.987021    2080 kubeadm.go:318] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I0206 13:52:04.987048    2080 kubeadm.go:318] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I0206 13:52:04.987072    2080 kubeadm.go:318] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I0206 13:52:04.987093    2080 kubeadm.go:318] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I0206 13:52:05.052427    2080 kubeadm.go:318] [preflight] Pulling images required for setting up a Kubernetes cluster
I0206 13:52:05.052488    2080 kubeadm.go:318] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0206 13:52:05.052875    2080 kubeadm.go:318] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0206 13:52:05.066753    2080 kubeadm.go:318] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0206 13:52:05.068240    2080 out.go:252]     ‚ñ™ Generating certificates and keys ...
I0206 13:52:05.068370    2080 kubeadm.go:318] [certs] Using existing ca certificate authority
I0206 13:52:05.068404    2080 kubeadm.go:318] [certs] Using existing apiserver certificate and key on disk
I0206 13:52:05.317953    2080 kubeadm.go:318] [certs] Generating "apiserver-kubelet-client" certificate and key
I0206 13:52:05.387846    2080 kubeadm.go:318] [certs] Generating "front-proxy-ca" certificate and key
I0206 13:52:05.645466    2080 kubeadm.go:318] [certs] Generating "front-proxy-client" certificate and key
I0206 13:52:05.726661    2080 kubeadm.go:318] [certs] Generating "etcd/ca" certificate and key
I0206 13:52:05.964968    2080 kubeadm.go:318] [certs] Generating "etcd/server" certificate and key
I0206 13:52:05.965031    2080 kubeadm.go:318] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0206 13:52:06.032059    2080 kubeadm.go:318] [certs] Generating "etcd/peer" certificate and key
I0206 13:52:06.032145    2080 kubeadm.go:318] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0206 13:52:06.170552    2080 kubeadm.go:318] [certs] Generating "etcd/healthcheck-client" certificate and key
I0206 13:52:06.200229    2080 kubeadm.go:318] [certs] Generating "apiserver-etcd-client" certificate and key
I0206 13:52:06.289248    2080 kubeadm.go:318] [certs] Generating "sa" key and public key
I0206 13:52:06.289281    2080 kubeadm.go:318] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0206 13:52:06.443721    2080 kubeadm.go:318] [kubeconfig] Writing "admin.conf" kubeconfig file
I0206 13:52:06.582549    2080 kubeadm.go:318] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0206 13:52:06.696884    2080 kubeadm.go:318] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0206 13:52:06.763539    2080 kubeadm.go:318] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0206 13:52:07.057505    2080 kubeadm.go:318] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0206 13:52:07.057869    2080 kubeadm.go:318] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0206 13:52:07.060870    2080 kubeadm.go:318] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0206 13:52:07.063507    2080 out.go:252]     ‚ñ™ Booting up control plane ...
I0206 13:52:07.063634    2080 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0206 13:52:07.063699    2080 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0206 13:52:07.063739    2080 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0206 13:52:07.074096    2080 kubeadm.go:318] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0206 13:52:07.074144    2080 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0206 13:52:07.079473    2080 kubeadm.go:318] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0206 13:52:07.079519    2080 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0206 13:52:07.079539    2080 kubeadm.go:318] [kubelet-start] Starting the kubelet
I0206 13:52:07.162696    2080 kubeadm.go:318] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0206 13:52:07.162756    2080 kubeadm.go:318] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0206 13:52:07.678530    2080 kubeadm.go:318] [kubelet-check] The kubelet is healthy after 504.690961ms
I0206 13:52:07.678582    2080 kubeadm.go:318] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0206 13:52:07.678622    2080 kubeadm.go:318] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0206 13:52:07.678671    2080 kubeadm.go:318] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0206 13:52:07.678717    2080 kubeadm.go:318] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0206 13:52:14.416393    2080 kubeadm.go:318] [control-plane-check] kube-controller-manager is healthy after 6.730799124s
I0206 13:52:15.865846    2080 kubeadm.go:318] [control-plane-check] kube-scheduler is healthy after 8.193828861s
I0206 13:52:17.680165    2080 kubeadm.go:318] [control-plane-check] kube-apiserver is healthy after 10.002471899s
I0206 13:52:17.692429    2080 kubeadm.go:318] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0206 13:52:17.700419    2080 kubeadm.go:318] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0206 13:52:17.750518    2080 kubeadm.go:318] [upload-certs] Skipping phase. Please see --upload-certs
I0206 13:52:17.750621    2080 kubeadm.go:318] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0206 13:52:17.816024    2080 kubeadm.go:318] [bootstrap-token] Using token: 52x6v7.dvycmx96q7zvs1w9
I0206 13:52:17.869001    2080 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I0206 13:52:17.878735    2080 kubeadm.go:318] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0206 13:52:17.958831    2080 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0206 13:52:17.967736    2080 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0206 13:52:17.990882    2080 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0206 13:52:18.017774    2080 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0206 13:52:18.031581    2080 kubeadm.go:318] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0206 13:52:18.146288    2080 kubeadm.go:318] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0206 13:52:18.844518    2080 kubeadm.go:318] [addons] Applied essential addon: CoreDNS
I0206 13:52:19.207665    2080 kubeadm.go:318] [addons] Applied essential addon: kube-proxy
I0206 13:52:19.207674    2080 kubeadm.go:318] 
I0206 13:52:19.207705    2080 kubeadm.go:318] Your Kubernetes control-plane has initialized successfully!
I0206 13:52:19.207706    2080 kubeadm.go:318] 
I0206 13:52:19.207745    2080 kubeadm.go:318] To start using your cluster, you need to run the following as a regular user:
I0206 13:52:19.207747    2080 kubeadm.go:318] 
I0206 13:52:19.207762    2080 kubeadm.go:318]   mkdir -p $HOME/.kube
I0206 13:52:19.207792    2080 kubeadm.go:318]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0206 13:52:19.207817    2080 kubeadm.go:318]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0206 13:52:19.207819    2080 kubeadm.go:318] 
I0206 13:52:19.207846    2080 kubeadm.go:318] Alternatively, if you are the root user, you can run:
I0206 13:52:19.207847    2080 kubeadm.go:318] 
I0206 13:52:19.207870    2080 kubeadm.go:318]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0206 13:52:19.207872    2080 kubeadm.go:318] 
I0206 13:52:19.207922    2080 kubeadm.go:318] You should now deploy a pod network to the cluster.
I0206 13:52:19.207964    2080 kubeadm.go:318] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0206 13:52:19.207999    2080 kubeadm.go:318]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0206 13:52:19.208000    2080 kubeadm.go:318] 
I0206 13:52:19.208043    2080 kubeadm.go:318] You can now join any number of control-plane nodes by copying certificate authorities
I0206 13:52:19.208081    2080 kubeadm.go:318] and service account keys on each node and then running the following as root:
I0206 13:52:19.208083    2080 kubeadm.go:318] 
I0206 13:52:19.208125    2080 kubeadm.go:318]   kubeadm join control-plane.minikube.internal:8443 --token 52x6v7.dvycmx96q7zvs1w9 \
I0206 13:52:19.208177    2080 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 \
I0206 13:52:19.208187    2080 kubeadm.go:318] 	--control-plane 
I0206 13:52:19.208189    2080 kubeadm.go:318] 
I0206 13:52:19.208231    2080 kubeadm.go:318] Then you can join any number of worker nodes by running the following on each as root:
I0206 13:52:19.208233    2080 kubeadm.go:318] 
I0206 13:52:19.208274    2080 kubeadm.go:318] kubeadm join control-plane.minikube.internal:8443 --token 52x6v7.dvycmx96q7zvs1w9 \
I0206 13:52:19.208325    2080 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 
I0206 13:52:19.248884    2080 kubeadm.go:318] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0206 13:52:19.248998    2080 kubeadm.go:318] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/5.15.0-134-generic\n", err: exit status 1
I0206 13:52:19.249052    2080 kubeadm.go:318] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0206 13:52:19.256344    2080 cni.go:83] Creating CNI manager for ""
I0206 13:52:19.256351    2080 cni.go:135] multinode detected (1 nodes found), recommending kindnet
I0206 13:52:19.295816    2080 out.go:179] üîó  Configuring CNI (Container Networking Interface) ...
I0206 13:52:19.335412    2080 ssh_runner.go:194] Run: stat /opt/cni/bin/portmap
I0206 13:52:19.432707    2080 cni.go:181] applying CNI manifest using /var/lib/minikube/binaries/v1.34.0/kubectl ...
I0206 13:52:19.432715    2080 ssh_runner.go:361] scp memory --> /var/tmp/minikube/cni.yaml (2620 bytes)
I0206 13:52:19.591493    2080 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0206 13:52:21.218876    2080 ssh_runner.go:234] Completed: sudo /var/lib/minikube/binaries/v1.34.0/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (1.627363523s)
I0206 13:52:21.218902    2080 ssh_runner.go:194] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0206 13:52:21.218965    2080 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0206 13:52:21.219260    2080 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_06T13_52_21_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0206 13:52:21.229940    2080 ops.go:34] apiserver oom_adj: -16
I0206 13:52:21.749263    2080 kubeadm.go:1113] duration metric: took 530.324439ms to wait for elevateKubeSystemPrivileges
I0206 13:52:21.782644    2080 kubeadm.go:402] duration metric: took 16.955288337s to StartCluster
I0206 13:52:21.782664    2080 settings.go:141] acquiring lock: {Name:mkb2c3059065ecb0ccdda4c7ff3af85f8f0082c2 Timeout:1m0s Delay:500ms}
I0206 13:52:21.782726    2080 settings.go:149] Updating kubeconfig:  /home/vagrant/.kube/config
I0206 13:52:21.789147    2080 lock.go:60] WriteFile acquiring /home/vagrant/.kube/config: {Name:mk584b224ce915a9a9ad34e6e788268489afc021 Timeout:1m0s Delay:500ms}
I0206 13:52:21.792328    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0206 13:52:21.795141    2080 start.go:237] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0206 13:52:21.797821    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:52:21.797845    2080 addons.go:528] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0206 13:52:21.797876    2080 addons.go:71] Setting storage-provisioner=true in profile "minikube"
I0206 13:52:21.797884    2080 addons.go:240] Setting addon storage-provisioner=true in "minikube"
I0206 13:52:21.797896    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:52:21.798084    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:52:21.800351    2080 addons.go:71] Setting default-storageclass=true in profile "minikube"
I0206 13:52:21.800362    2080 addons_storage_classes.go:34] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0206 13:52:21.800509    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:52:21.831325    2080 out.go:179] üîé  Verifying Kubernetes components...
I0206 13:52:21.863765    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:52:22.385094    2080 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0206 13:52:22.414377    2080 addons.go:437] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0206 13:52:22.414386    2080 ssh_runner.go:361] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0206 13:52:22.414422    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:22.811143    2080 addons.go:240] Setting addon default-storageclass=true in "minikube"
I0206 13:52:22.811167    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:52:22.811386    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:52:22.891959    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:23.327569    2080 ssh_runner.go:234] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.535222867s)
I0206 13:52:23.328332    2080 addons.go:437] installing /etc/kubernetes/addons/storageclass.yaml
I0206 13:52:23.328340    2080 ssh_runner.go:361] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0206 13:52:23.328372    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:52:23.344870    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0206 13:52:23.793364    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:52:23.811591    2080 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0206 13:52:23.919899    2080 ssh_runner.go:234] Completed: sudo systemctl daemon-reload: (2.056118129s)
I0206 13:52:23.919929    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:52:24.920052    2080 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0206 13:52:25.862824    2080 ssh_runner.go:234] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (2.517934648s)
I0206 13:52:25.862841    2080 start.go:989] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0206 13:52:26.411655    2080 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0206 13:52:27.218922    2080 ssh_runner.go:234] Completed: sudo systemctl start kubelet: (3.298980799s)
I0206 13:52:27.219259    2080 api_server.go:51] waiting for apiserver process to appear ...
I0206 13:52:27.219280    2080 ssh_runner.go:194] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0206 13:52:27.222077    2080 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.410471019s)
I0206 13:52:27.224267    2080 ssh_runner.go:234] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.304201255s)
I0206 13:52:27.317220    2080 api_server.go:71] duration metric: took 5.522050493s to wait for apiserver process to appear ...
I0206 13:52:27.317231    2080 api_server.go:87] waiting for apiserver healthz status ...
I0206 13:52:27.317241    2080 api_server.go:298] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0206 13:52:27.343974    2080 api_server.go:324] https://192.168.49.2:8443/healthz returned 200:
ok
I0206 13:52:27.346435    2080 out.go:179] üåü  Enabled addons: storage-provisioner, default-storageclass
I0206 13:52:27.346769    2080 api_server.go:140] control plane version: v1.34.0
I0206 13:52:27.347429    2080 api_server.go:130] duration metric: took 30.191612ms to wait for apiserver health ...
I0206 13:52:27.347436    2080 system_pods.go:42] waiting for kube-system pods to appear ...
I0206 13:52:27.358311    2080 addons.go:531] duration metric: took 5.560462299s for enable addons: enabled=[storage-provisioner default-storageclass]
I0206 13:52:27.374050    2080 system_pods.go:58] 8 kube-system pods found
I0206 13:52:27.374063    2080 system_pods.go:60] "coredns-66bc5c9577-t6rgl" [e58ee960-450f-4562-a69c-30bcbcdab036] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0206 13:52:27.374066    2080 system_pods.go:60] "etcd-minikube" [36ec2fd5-f480-4e28-b878-23df8ad31204] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0206 13:52:27.374070    2080 system_pods.go:60] "kindnet-mxnc7" [41ae4e59-5070-4ca3-8d67-22e9ee9dfaab] Pending / Ready:ContainersNotReady (containers with unready status: [kindnet-cni]) / ContainersReady:ContainersNotReady (containers with unready status: [kindnet-cni])
I0206 13:52:27.374072    2080 system_pods.go:60] "kube-apiserver-minikube" [8a6dd669-9eda-4f62-b130-ae3ec9fbfc95] Running
I0206 13:52:27.374075    2080 system_pods.go:60] "kube-controller-manager-minikube" [a74fb102-457e-44a2-888d-6414b46c684b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0206 13:52:27.374077    2080 system_pods.go:60] "kube-proxy-rgwg2" [4cecd8fc-849a-40f0-bd62-b58423631251] Pending / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0206 13:52:27.374078    2080 system_pods.go:60] "kube-scheduler-minikube" [257d97e9-61ba-448b-b9c2-43844ff342c9] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0206 13:52:27.374080    2080 system_pods.go:60] "storage-provisioner" [60c06097-227c-4eeb-9247-43519730d296] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0206 13:52:27.374085    2080 system_pods.go:73] duration metric: took 26.645526ms to wait for pod list to return data ...
I0206 13:52:27.374091    2080 kubeadm.go:586] duration metric: took 5.578927013s to wait for: map[apiserver:true system_pods:true]
I0206 13:52:27.374171    2080 node_conditions.go:101] verifying NodePressure condition ...
I0206 13:52:27.377924    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:52:27.377933    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:52:27.377942    2080 node_conditions.go:104] duration metric: took 3.768426ms to run NodePressure ...
I0206 13:52:27.377949    2080 start.go:243] waiting for startup goroutines ...
I0206 13:52:27.377952    2080 start.go:248] waiting for cluster config update ...
I0206 13:52:27.377958    2080 start.go:257] writing updated cluster config ...
I0206 13:52:27.379020    2080 out.go:203] 
I0206 13:52:27.394923    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:52:27.394980    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:52:27.420890    2080 out.go:179] üëç  Starting "minikube-m02" worker node in "minikube" cluster
I0206 13:52:27.450718    2080 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 13:52:27.487302    2080 out.go:179] üöú  Pulling base image v0.0.49 ...
I0206 13:52:27.518579    2080 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 13:52:27.520646    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:52:27.520655    2080 cache.go:66] Caching tarball of preloaded images
I0206 13:52:27.520715    2080 preload.go:250] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0206 13:52:27.520729    2080 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 13:52:27.520778    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:52:27.802203    2080 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 13:52:27.802294    2080 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 13:52:27.802300    2080 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0206 13:52:27.802302    2080 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0206 13:52:27.802306    2080 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 13:52:27.802309    2080 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 13:53:12.547383    2080 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 13:53:12.547431    2080 cache.go:244] Successfully downloaded all kic artifacts
I0206 13:53:12.547451    2080 start.go:359] acquireMachinesLock for minikube-m02: {Name:mk7681bfb3a3fdfa52a372a04630919dda81469f Timeout:10m0s Delay:500ms}
I0206 13:53:12.547505    2080 start.go:363] duration metric: took 47.854¬µs to acquireMachinesLock for "minikube-m02"
I0206 13:53:12.547515    2080 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name:m02 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:53:12.547548    2080 start.go:124] createHost starting for "m02" (driver="docker")
I0206 13:53:12.549466    2080 out.go:252] üî•  Creating docker container (CPUs=5, Memory=12970MB) ...
I0206 13:53:12.549580    2080 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 13:53:12.549589    2080 client.go:173] LocalClient.Create starting
I0206 13:53:12.549618    2080 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/ca.pem
I0206 13:53:12.549630    2080 main.go:144] libmachine: Decoding PEM data...
I0206 13:53:12.549644    2080 main.go:144] libmachine: Parsing certificate...
I0206 13:53:12.549673    2080 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/cert.pem
I0206 13:53:12.549679    2080 main.go:144] libmachine: Decoding PEM data...
I0206 13:53:12.549684    2080 main.go:144] libmachine: Parsing certificate...
I0206 13:53:12.549783    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:53:12.568090    2080 network_create.go:78] Found existing network {name:minikube subnet:0xc00242e660 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0206 13:53:12.568109    2080 kic.go:120] calculated static IP "192.168.49.3" for the "minikube-m02" container
I0206 13:53:12.568139    2080 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 13:53:12.694370    2080 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I0206 13:53:12.721771    2080 oci.go:102] Successfully created a docker volume minikube-m02
I0206 13:53:12.721806    2080 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 13:53:13.953826    2080 cli_runner.go:217] Completed: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (1.231995435s)
I0206 13:53:13.953842    2080 oci.go:106] Successfully prepared a docker volume minikube-m02
I0206 13:53:13.953865    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:53:13.953870    2080 kic.go:193] Starting extracting preloaded images to volume ...
I0206 13:53:13.953897    2080 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 13:53:19.186160    2080 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (5.232240892s)
I0206 13:53:19.186175    2080 kic.go:202] duration metric: took 5.232302742s to extract preloaded images to volume ...
W0206 13:53:19.186220    2080 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 13:53:19.186237    2080 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 13:53:19.186299    2080 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 13:53:19.254645    2080 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=12970mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 13:53:20.530747    2080 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=12970mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945: (1.276073721s)
I0206 13:53:20.530787    2080 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I0206 13:53:20.724062    2080 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 13:53:20.789378    2080 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I0206 13:53:21.250741    2080 oci.go:143] the created container "minikube-m02" has a running status.
I0206 13:53:21.250756    2080 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube-m02/id_rsa...
I0206 13:53:21.264331    2080 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 13:53:21.586028    2080 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 13:53:21.800609    2080 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 13:53:21.800618    2080 kic_runner.go:113] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 13:53:22.882427    2080 kic_runner.go:122] Done: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]: (1.081793055s)
I0206 13:53:22.882493    2080 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0206 13:53:23.333084    2080 machine.go:96] provisionDockerMachine start ...
I0206 13:53:23.333131    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:23.710442    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:23.710578    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:23.710583    2080 main.go:144] libmachine: About to run SSH command:
hostname
I0206 13:53:24.223812    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0206 13:53:24.223823    2080 ubuntu.go:182] provisioning hostname "minikube-m02"
I0206 13:53:24.223859    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:24.873600    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:24.873718    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:24.873722    2080 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0206 13:53:25.800368    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0206 13:53:25.800405    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:26.112738    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:26.112852    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:26.112859    2080 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 13:53:26.987468    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 13:53:26.987480    2080 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 13:53:26.987486    2080 ubuntu.go:190] setting up certificates
I0206 13:53:26.987498    2080 provision.go:83] configureAuth start
I0206 13:53:26.987553    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 13:53:27.504694    2080 provision.go:142] copyHostCerts
I0206 13:53:27.504728    2080 exec_runner.go:143] found /home/vagrant/.minikube/ca.pem, removing ...
I0206 13:53:27.504735    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/ca.pem
I0206 13:53:27.506605    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 13:53:27.506669    2080 exec_runner.go:143] found /home/vagrant/.minikube/cert.pem, removing ...
I0206 13:53:27.506672    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/cert.pem
I0206 13:53:27.506690    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 13:53:27.506710    2080 exec_runner.go:143] found /home/vagrant/.minikube/key.pem, removing ...
I0206 13:53:27.506712    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/key.pem
I0206 13:53:27.506720    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 13:53:27.506740    2080 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube-m02 san=[127.0.0.1 192.168.49.3 localhost minikube minikube-m02]
I0206 13:53:27.637146    2080 provision.go:176] copyRemoteCerts
I0206 13:53:27.637187    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 13:53:27.637209    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:27.968150    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 13:53:28.444688    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 13:53:28.703316    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0206 13:53:28.907721    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0206 13:53:29.036333    2080 provision.go:86] duration metric: took 2.048823785s to configureAuth
I0206 13:53:29.036346    2080 ubuntu.go:206] setting minikube options for container-runtime
I0206 13:53:29.039799    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:53:29.039834    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:29.475084    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:29.475206    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:29.475210    2080 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 13:53:30.302500    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 13:53:30.302508    2080 ubuntu.go:71] root file system type: overlay
I0206 13:53:30.302569    2080 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 13:53:30.302602    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:30.766440    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:30.766577    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:30.766613    2080 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 13:53:31.512754    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 13:53:31.512797    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:32.000012    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:53:32.000129    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32773 <nil> <nil>}
I0206 13:53:32.000136    2080 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 13:53:34.304850    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 13:53:31.449473205 +0000
@@ -9,23 +9,35 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+Environment=NO_PROXY=192.168.49.2
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 13:53:34.304861    2080 machine.go:99] duration metric: took 10.971768664s to provisionDockerMachine
I0206 13:53:34.304867    2080 client.go:176] duration metric: took 21.755275925s to LocalClient.Create
I0206 13:53:34.304874    2080 start.go:166] duration metric: took 21.755295038s to libmachine.API.Create "minikube"
I0206 13:53:34.304878    2080 start.go:292] postStartSetup for "minikube-m02" (driver="docker")
I0206 13:53:34.304882    2080 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 13:53:34.304914    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 13:53:34.304932    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:34.338369    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 13:53:34.435112    2080 ssh_runner.go:194] Run: cat /etc/os-release
I0206 13:53:34.439229    2080 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 13:53:34.439238    2080 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 13:53:34.439243    2080 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 13:53:34.439277    2080 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 13:53:34.439287    2080 start.go:295] duration metric: took 134.406697ms for postStartSetup
I0206 13:53:34.439485    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 13:53:34.473421    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:53:34.473598    2080 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 13:53:34.473618    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:34.496954    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 13:53:34.596305    2080 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 13:53:34.602126    2080 start.go:127] duration metric: took 22.05457163s to createHost
I0206 13:53:34.602134    2080 start.go:82] releasing machines lock for "minikube-m02", held for 22.054624814s
I0206 13:53:34.602167    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0206 13:53:34.625710    2080 out.go:179] üåê  Found network options:
I0206 13:53:34.626575    2080 out.go:179]     ‚ñ™ NO_PROXY=192.168.49.2
W0206 13:53:34.627663    2080 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 13:53:34.627705    2080 proxy.go:121] fail to check proxy env: Error ip not in block
I0206 13:53:34.627753    2080 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0206 13:53:34.627770    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:34.627788    2080 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 13:53:34.627813    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0206 13:53:34.653396    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0206 13:53:34.657461    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32773 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m02/id_rsa Username:docker}
W0206 13:53:34.754761    2080 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 13:53:34.754797    2080 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 13:53:34.850135    2080 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 13:53:34.850146    2080 start.go:497] detecting cgroup driver to use...
I0206 13:53:34.850167    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:53:34.850224    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:53:34.864083    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 13:53:34.873731    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 13:53:34.883115    2080 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 13:53:34.883138    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 13:53:34.893705    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:53:34.902831    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 13:53:34.911743    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:53:34.921625    2080 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 13:53:34.930609    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 13:53:34.939831    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 13:53:34.949646    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 13:53:34.959417    2080 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 13:53:34.967026    2080 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 13:53:34.974945    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:35.047912    2080 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 13:53:35.174212    2080 start.go:497] detecting cgroup driver to use...
I0206 13:53:35.174239    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:53:35.174265    2080 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 13:53:35.271608    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:53:35.316638    2080 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 13:53:35.354256    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:53:35.365012    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 13:53:35.375431    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:53:35.391894    2080 ssh_runner.go:194] Run: which cri-dockerd
I0206 13:53:35.396661    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 13:53:35.405339    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 13:53:35.418542    2080 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 13:53:35.477137    2080 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 13:53:35.545477    2080 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 13:53:35.545497    2080 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 13:53:35.557727    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 13:53:35.569651    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:35.624630    2080 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 13:53:37.178946    2080 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.554299338s)
I0206 13:53:37.178982    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 13:53:37.188706    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 13:53:37.200082    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:53:37.211054    2080 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 13:53:37.285729    2080 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 13:53:37.362010    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:37.430966    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 13:53:37.443710    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 13:53:37.454537    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:37.522507    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 13:53:37.627237    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:53:37.645050    2080 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 13:53:37.645078    2080 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 13:53:37.649734    2080 start.go:575] Will wait 60s for crictl version
I0206 13:53:37.649760    2080 ssh_runner.go:194] Run: which crictl
I0206 13:53:37.654871    2080 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 13:53:37.693635    2080 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 13:53:37.693667    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:53:37.755443    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:53:37.834883    2080 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 13:53:37.835962    2080 out.go:179]     ‚ñ™ env NO_PROXY=192.168.49.2
I0206 13:53:37.837562    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:53:37.863991    2080 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 13:53:37.868621    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:53:37.879063    2080 mustload.go:66] Loading cluster: minikube
I0206 13:53:37.879161    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:53:37.879261    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:53:37.900850    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:53:37.900972    2080 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.3
I0206 13:53:37.900974    2080 certs.go:193] generating shared ca certs ...
I0206 13:53:37.900980    2080 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 13:53:37.901030    2080 certs.go:234] skipping valid "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 13:53:37.901046    2080 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 13:53:37.901085    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 13:53:37.901098    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 13:53:37.901108    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 13:53:37.901145    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 13:53:37.901168    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 13:53:37.919617    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 13:53:37.935573    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 13:53:37.951983    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 13:53:37.979124    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 13:53:37.995685    2080 ssh_runner.go:194] Run: openssl version
I0206 13:53:38.004227    2080 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 13:53:38.011817    2080 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 13:53:38.019376    2080 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 13:53:38.023993    2080 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 13:53:38.024015    2080 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 13:53:38.046386    2080 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 13:53:38.053367    2080 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 13:53:38.060964    2080 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 13:53:38.073823    2080 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 13:53:38.073843    2080 kubeadm.go:934] updating node {m02 192.168.49.3 8443 v1.34.0 docker false true} ...
I0206 13:53:38.073903    2080 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m02 --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.3 --serialize-image-pulls=false

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 13:53:38.073935    2080 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 13:53:38.081662    2080 binaries.go:50] Found k8s binaries, skipping transfer
I0206 13:53:38.081681    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0206 13:53:38.089069    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (383 bytes)
I0206 13:53:38.099614    2080 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 13:53:38.111062    2080 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 13:53:38.115588    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:53:38.125026    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:38.201246    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:53:38.212475    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:53:38.212597    2080 start.go:319] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 13:53:38.212644    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0206 13:53:38.212661    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:53:38.385454    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:53:38.542932    2080 start.go:345] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:53:38.542968    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token keyt8d.q66avq3xrohkwv8u --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m02"
I0206 13:53:39.432345    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "systemctl daemon-reload && systemctl enable kubelet && systemctl start kubelet"
I0206 13:53:39.763015    2080 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m02 minikube.k8s.io/updated_at=2026_02_06T13_53_39_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0206 13:53:40.008169    2080 start.go:321] duration metric: took 1.795568565s to joinCluster
I0206 13:53:40.008201    2080 start.go:237] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:53:40.019569    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:53:40.039011    2080 out.go:179] üîé  Verifying Kubernetes components...
I0206 13:53:40.071668    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:53:40.273806    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:53:40.398722    2080 kubeadm.go:586] duration metric: took 390.503326ms to wait for: map[apiserver:true system_pods:true]
I0206 13:53:40.398733    2080 node_conditions.go:101] verifying NodePressure condition ...
I0206 13:53:40.476146    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:53:40.476155    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:53:40.476162    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:53:40.476163    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:53:40.476165    2080 node_conditions.go:104] duration metric: took 77.429493ms to run NodePressure ...
I0206 13:53:40.476172    2080 start.go:243] waiting for startup goroutines ...
I0206 13:53:40.476187    2080 start.go:257] writing updated cluster config ...
I0206 13:53:40.523378    2080 out.go:203] 
I0206 13:53:40.554476    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:53:40.554538    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:53:40.592105    2080 out.go:179] üëç  Starting "minikube-m03" worker node in "minikube" cluster
I0206 13:53:40.608441    2080 cache.go:135] Beginning downloading kic base image for docker with docker
I0206 13:53:40.624230    2080 out.go:179] üöú  Pulling base image v0.0.49 ...
I0206 13:53:40.654374    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:53:40.654384    2080 cache.go:66] Caching tarball of preloaded images
I0206 13:53:40.654444    2080 preload.go:250] Found /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0206 13:53:40.656370    2080 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local docker daemon
I0206 13:53:40.659681    2080 cache.go:69] Finished verifying existence of preloaded tar for v1.34.0 on docker
I0206 13:53:40.659751    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:53:41.452257    2080 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 to local cache
I0206 13:53:41.456764    2080 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory
I0206 13:53:41.456773    2080 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 in local cache directory, skipping pull
I0206 13:53:41.456775    2080 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 exists in cache, skipping pull
I0206 13:53:41.456781    2080 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 as a tarball
I0206 13:53:41.456784    2080 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from local cache
I0206 13:54:51.097332    2080 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 from cached tarball
I0206 13:54:51.097377    2080 cache.go:244] Successfully downloaded all kic artifacts
I0206 13:54:51.097397    2080 start.go:359] acquireMachinesLock for minikube-m03: {Name:mk288658bee5399c456495e25cfaba36322fce43 Timeout:10m0s Delay:500ms}
I0206 13:54:51.097457    2080 start.go:363] duration metric: took 52.422¬µs to acquireMachinesLock for "minikube-m03"
I0206 13:54:51.097467    2080 start.go:92] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} &{Name:m03 IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:54:51.097520    2080 start.go:124] createHost starting for "m03" (driver="docker")
I0206 13:54:51.098435    2080 out.go:252] üî•  Creating docker container (CPUs=5, Memory=12970MB) ...
I0206 13:54:51.098530    2080 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0206 13:54:51.098542    2080 client.go:173] LocalClient.Create starting
I0206 13:54:51.098570    2080 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/ca.pem
I0206 13:54:51.098583    2080 main.go:144] libmachine: Decoding PEM data...
I0206 13:54:51.098590    2080 main.go:144] libmachine: Parsing certificate...
I0206 13:54:51.098618    2080 main.go:144] libmachine: Reading certificate data from /home/vagrant/.minikube/certs/cert.pem
I0206 13:54:51.098625    2080 main.go:144] libmachine: Decoding PEM data...
I0206 13:54:51.098630    2080 main.go:144] libmachine: Parsing certificate...
I0206 13:54:51.098725    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:54:51.119791    2080 network_create.go:78] Found existing network {name:minikube subnet:0xc00211fd40 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0206 13:54:51.119808    2080 kic.go:120] calculated static IP "192.168.49.4" for the "minikube-m03" container
I0206 13:54:51.119844    2080 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0206 13:54:51.163978    2080 cli_runner.go:164] Run: docker volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I0206 13:54:51.181867    2080 oci.go:102] Successfully created a docker volume minikube-m03
I0206 13:54:51.181906    2080 cli_runner.go:164] Run: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib
I0206 13:54:52.780090    2080 cli_runner.go:217] Completed: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -d /var/lib: (1.598156583s)
I0206 13:54:52.780103    2080 oci.go:106] Successfully prepared a docker volume minikube-m03
I0206 13:54:52.780130    2080 preload.go:187] Checking if preload exists for k8s version v1.34.0 and runtime docker
I0206 13:54:52.780134    2080 kic.go:193] Starting extracting preloaded images to volume ...
I0206 13:54:52.780161    2080 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir
I0206 13:54:58.205346    2080 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/vagrant/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 -I lz4 -xf /preloaded.tar -C /extractDir: (5.424881245s)
I0206 13:54:58.205371    2080 kic.go:202] duration metric: took 5.425231127s to extract preloaded images to volume ...
W0206 13:54:58.205433    2080 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0206 13:54:58.205447    2080 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0206 13:54:58.205469    2080 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0206 13:54:58.622827    2080 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=12970mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945
I0206 13:54:59.906459    2080 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=12970mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945: (1.283533327s)
I0206 13:54:59.906497    2080 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Running}}
I0206 13:55:00.028750    2080 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 13:55:00.138589    2080 cli_runner.go:164] Run: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I0206 13:55:00.328513    2080 oci.go:143] the created container "minikube-m03" has a running status.
I0206 13:55:00.328527    2080 kic.go:224] Creating ssh key for kic: /home/vagrant/.minikube/machines/minikube-m03/id_rsa...
I0206 13:55:00.348114    2080 kic_runner.go:190] docker (temp): /home/vagrant/.minikube/machines/minikube-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0206 13:55:00.533084    2080 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 13:55:00.576443    2080 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0206 13:55:00.576450    2080 kic_runner.go:113] Args: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I0206 13:55:01.183186    2080 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0206 13:55:01.551358    2080 machine.go:96] provisionDockerMachine start ...
I0206 13:55:01.551407    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:01.631517    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:01.631715    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:01.631719    2080 main.go:144] libmachine: About to run SSH command:
hostname
I0206 13:55:01.632448    2080 main.go:144] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35704->127.0.0.1:32778: read: connection reset by peer
I0206 13:55:04.634904    2080 main.go:144] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35718->127.0.0.1:32778: read: connection reset by peer
I0206 13:55:07.970934    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0206 13:55:07.970945    2080 ubuntu.go:182] provisioning hostname "minikube-m03"
I0206 13:55:07.970979    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:08.069327    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:08.069452    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:08.069457    2080 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0206 13:55:08.227704    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0206 13:55:08.227739    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:08.250392    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:08.250509    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:08.250516    2080 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0206 13:55:08.392103    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0206 13:55:08.392127    2080 ubuntu.go:188] set auth options {CertDir:/home/vagrant/.minikube CaCertPath:/home/vagrant/.minikube/certs/ca.pem CaPrivateKeyPath:/home/vagrant/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/vagrant/.minikube/machines/server.pem ServerKeyPath:/home/vagrant/.minikube/machines/server-key.pem ClientKeyPath:/home/vagrant/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/vagrant/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/vagrant/.minikube}
I0206 13:55:08.392134    2080 ubuntu.go:190] setting up certificates
I0206 13:55:08.392138    2080 provision.go:83] configureAuth start
I0206 13:55:08.392168    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 13:55:08.415162    2080 provision.go:142] copyHostCerts
I0206 13:55:08.415195    2080 exec_runner.go:143] found /home/vagrant/.minikube/key.pem, removing ...
I0206 13:55:08.415199    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/key.pem
I0206 13:55:08.415243    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/key.pem --> /home/vagrant/.minikube/key.pem (1679 bytes)
I0206 13:55:08.415291    2080 exec_runner.go:143] found /home/vagrant/.minikube/ca.pem, removing ...
I0206 13:55:08.415293    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/ca.pem
I0206 13:55:08.415303    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/ca.pem --> /home/vagrant/.minikube/ca.pem (1082 bytes)
I0206 13:55:08.415326    2080 exec_runner.go:143] found /home/vagrant/.minikube/cert.pem, removing ...
I0206 13:55:08.415327    2080 exec_runner.go:202] rm: /home/vagrant/.minikube/cert.pem
I0206 13:55:08.415399    2080 exec_runner.go:150] cp: /home/vagrant/.minikube/certs/cert.pem --> /home/vagrant/.minikube/cert.pem (1123 bytes)
I0206 13:55:08.415430    2080 provision.go:116] generating server cert: /home/vagrant/.minikube/machines/server.pem ca-key=/home/vagrant/.minikube/certs/ca.pem private-key=/home/vagrant/.minikube/certs/ca-key.pem org=vagrant.minikube-m03 san=[127.0.0.1 192.168.49.4 localhost minikube minikube-m03]
I0206 13:55:08.455879    2080 provision.go:176] copyRemoteCerts
I0206 13:55:08.455910    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0206 13:55:08.455928    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:08.481915    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 13:55:08.586934    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0206 13:55:08.611420    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0206 13:55:08.633803    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0206 13:55:08.651232    2080 provision.go:86] duration metric: took 259.085826ms to configureAuth
I0206 13:55:08.651246    2080 ubuntu.go:206] setting minikube options for container-runtime
I0206 13:55:08.651376    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:55:08.651426    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:08.684332    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:08.684466    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:08.684470    2080 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0206 13:55:08.829828    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0206 13:55:08.829837    2080 ubuntu.go:71] root file system type: overlay
I0206 13:55:08.829889    2080 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0206 13:55:08.829918    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:08.854082    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:08.854195    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:08.854228    2080 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0206 13:55:09.022260    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0206 13:55:09.022368    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:09.178223    2080 main.go:144] libmachine: Using SSH client type: native
I0206 13:55:09.178353    2080 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906340] 0x908fe0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0206 13:55:09.178361    2080 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0206 13:55:11.259891    2080 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-01-26 19:24:35.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-06 13:55:09.014539500 +0000
@@ -9,23 +9,36 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0206 13:55:11.259902    2080 machine.go:99] duration metric: took 9.708536048s to provisionDockerMachine
I0206 13:55:11.259907    2080 client.go:176] duration metric: took 20.161363699s to LocalClient.Create
I0206 13:55:11.259916    2080 start.go:166] duration metric: took 20.161385854s to libmachine.API.Create "minikube"
I0206 13:55:11.259918    2080 start.go:292] postStartSetup for "minikube-m03" (driver="docker")
I0206 13:55:11.259923    2080 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0206 13:55:11.259952    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0206 13:55:11.260023    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:11.547736    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 13:55:11.677985    2080 ssh_runner.go:194] Run: cat /etc/os-release
I0206 13:55:11.682093    2080 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0206 13:55:11.682101    2080 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0206 13:55:11.682106    2080 filesync.go:125] Scanning /home/vagrant/.minikube/addons for local assets ...
I0206 13:55:11.682132    2080 filesync.go:125] Scanning /home/vagrant/.minikube/files for local assets ...
I0206 13:55:11.682139    2080 start.go:295] duration metric: took 422.218871ms for postStartSetup
I0206 13:55:11.682318    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 13:55:11.710337    2080 profile.go:143] Saving config to /home/vagrant/.minikube/profiles/minikube/config.json ...
I0206 13:55:11.710567    2080 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0206 13:55:11.710587    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:11.735903    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 13:55:11.847850    2080 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0206 13:55:11.854802    2080 start.go:127] duration metric: took 20.757273637s to createHost
I0206 13:55:11.854810    2080 start.go:82] releasing machines lock for "minikube-m03", held for 20.757350748s
I0206 13:55:11.854850    2080 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0206 13:55:11.874876    2080 out.go:179] üåê  Found network options:
I0206 13:55:11.875701    2080 out.go:179]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
W0206 13:55:11.876412    2080 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 13:55:11.876419    2080 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 13:55:11.876449    2080 proxy.go:121] fail to check proxy env: Error ip not in block
W0206 13:55:11.876451    2080 proxy.go:121] fail to check proxy env: Error ip not in block
I0206 13:55:11.876493    2080 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0206 13:55:11.876509    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:11.876665    2080 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0206 13:55:11.876689    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0206 13:55:12.044612    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0206 13:55:12.048356    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/vagrant/.minikube/machines/minikube-m03/id_rsa Username:docker}
W0206 13:55:12.217898    2080 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0206 13:55:12.217931    2080 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0206 13:55:12.242361    2080 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0206 13:55:12.242372    2080 start.go:497] detecting cgroup driver to use...
I0206 13:55:12.242392    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:55:12.242449    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:55:12.265796    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0206 13:55:12.274856    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0206 13:55:12.284147    2080 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0206 13:55:12.284170    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0206 13:55:12.293343    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:55:12.302921    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0206 13:55:12.312648    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0206 13:55:12.321732    2080 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0206 13:55:12.330235    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0206 13:55:12.339382    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0206 13:55:12.349089    2080 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0206 13:55:12.358658    2080 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0206 13:55:12.365740    2080 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0206 13:55:12.373278    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:12.438913    2080 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0206 13:55:12.565196    2080 start.go:497] detecting cgroup driver to use...
I0206 13:55:12.565221    2080 detect.go:178] detected "systemd" cgroup driver on host os
I0206 13:55:12.565247    2080 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0206 13:55:12.679397    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:55:12.710439    2080 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0206 13:55:12.737310    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0206 13:55:12.752691    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0206 13:55:12.790689    2080 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0206 13:55:12.806517    2080 ssh_runner.go:194] Run: which cri-dockerd
I0206 13:55:12.810572    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0206 13:55:12.825235    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0206 13:55:12.837239    2080 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0206 13:55:12.916402    2080 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0206 13:55:12.994135    2080 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0206 13:55:12.994155    2080 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0206 13:55:13.010055    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0206 13:55:13.019571    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:13.111355    2080 ssh_runner.go:194] Run: sudo systemctl restart docker
I0206 13:55:14.815384    2080 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.704013119s)
I0206 13:55:14.815420    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0206 13:55:14.825799    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0206 13:55:14.839150    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:55:14.851990    2080 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0206 13:55:14.923641    2080 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0206 13:55:15.031082    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:15.120380    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0206 13:55:15.131712    2080 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0206 13:55:15.142347    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:15.298161    2080 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0206 13:55:15.379782    2080 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0206 13:55:15.391048    2080 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0206 13:55:15.391078    2080 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0206 13:55:15.396768    2080 start.go:575] Will wait 60s for crictl version
I0206 13:55:15.396794    2080 ssh_runner.go:194] Run: which crictl
I0206 13:55:15.403040    2080 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0206 13:55:15.453762    2080 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.0
RuntimeApiVersion:  v1
I0206 13:55:15.453798    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:55:15.531867    2080 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0206 13:55:15.562139    2080 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 29.2.0 ...
I0206 13:55:15.562925    2080 out.go:179]     ‚ñ™ env NO_PROXY=192.168.49.2
I0206 13:55:15.563685    2080 out.go:179]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
I0206 13:55:15.564588    2080 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0206 13:55:15.593702    2080 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0206 13:55:15.598219    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:55:15.608872    2080 mustload.go:66] Loading cluster: minikube
I0206 13:55:15.609037    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:55:15.609142    2080 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0206 13:55:15.626293    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:55:15.626410    2080 certs.go:67] Setting up /home/vagrant/.minikube/profiles/minikube for IP: 192.168.49.4
I0206 13:55:15.626413    2080 certs.go:193] generating shared ca certs ...
I0206 13:55:15.626421    2080 certs.go:225] acquiring lock for ca certs: {Name:mk99734a69f246b009342ee30e5dd25cb3da1093 Timeout:1m0s Delay:500ms}
I0206 13:55:15.626468    2080 certs.go:234] skipping valid "minikubeCA" ca cert: /home/vagrant/.minikube/ca.key
I0206 13:55:15.626485    2080 certs.go:234] skipping valid "proxyClientCA" ca cert: /home/vagrant/.minikube/proxy-client-ca.key
I0206 13:55:15.626545    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca-key.pem (1675 bytes)
I0206 13:55:15.626558    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/ca.pem (1082 bytes)
I0206 13:55:15.626569    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/cert.pem (1123 bytes)
I0206 13:55:15.626579    2080 certs.go:482] found cert: /home/vagrant/.minikube/certs/key.pem (1679 bytes)
I0206 13:55:15.626600    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0206 13:55:15.643190    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0206 13:55:15.659068    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0206 13:55:15.675552    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0206 13:55:15.703756    2080 ssh_runner.go:361] scp /home/vagrant/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0206 13:55:15.743276    2080 ssh_runner.go:194] Run: openssl version
I0206 13:55:15.748892    2080 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0206 13:55:15.756139    2080 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0206 13:55:15.765973    2080 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0206 13:55:15.770160    2080 certs.go:526] hashing: -rw-r--r-- 1 root root 1111 Feb  6 13:52 /usr/share/ca-certificates/minikubeCA.pem
I0206 13:55:15.770189    2080 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0206 13:55:15.790910    2080 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0206 13:55:15.798497    2080 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0206 13:55:15.806225    2080 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0206 13:55:15.810982    2080 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0206 13:55:15.810999    2080 kubeadm.go:934] updating node {m03 192.168.49.4 8443 v1.34.0 docker false true} ...
I0206 13:55:15.811043    2080 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m03 --housekeeping-interval=10s --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=50 --node-ip=192.168.49.4 --serialize-image-pulls=false

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0206 13:55:15.811076    2080 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I0206 13:55:15.818550    2080 binaries.go:50] Found k8s binaries, skipping transfer
I0206 13:55:15.818572    2080 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0206 13:55:15.826013    2080 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (383 bytes)
I0206 13:55:15.837204    2080 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0206 13:55:15.849465    2080 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0206 13:55:15.853359    2080 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0206 13:55:15.864146    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:15.943945    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:55:15.956002    2080 host.go:67] Checking if "minikube" exists ...
I0206 13:55:15.956122    2080 start.go:319] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.49@sha256:e6daddbb1dc09ccd195c5605f65e2d38406c36ef36c5a492ffe805d9d36f4945 Memory:12970 CPUs:5 DiskSize:40960 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:10s} {Component:kubelet Key:max-pods Value:50} {Component:kubelet Key:serialize-image-pulls Value:false}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0206 13:55:15.956184    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm token create --print-join-command --ttl=0"
I0206 13:55:15.956201    2080 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0206 13:55:16.072186    2080 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/vagrant/.minikube/machines/minikube/id_rsa Username:docker}
I0206 13:55:16.691279    2080 start.go:345] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:55:16.691317    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token i8lmke.upho5mw99zhou60w --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m03"
I0206 13:55:17.951636    2080 ssh_runner.go:234] Completed: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm join control-plane.minikube.internal:8443 --token i8lmke.upho5mw99zhou60w --discovery-token-ca-cert-hash sha256:baaea5ed0d9aeeadf9030610cd7456ab8ac50e73d5dfee085c7fc9ce421d6d64 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m03": (1.260305885s)
I0206 13:55:17.951668    2080 ssh_runner.go:194] Run: sudo /bin/bash -c "systemctl daemon-reload && systemctl enable kubelet && systemctl start kubelet"
I0206 13:55:18.428835    2080 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m03 minikube.k8s.io/updated_at=2026_02_06T13_55_18_0700 minikube.k8s.io/version=v1.38.0 minikube.k8s.io/commit=de81223c61ab1bd97dcfcfa6d9d5c59e5da4a0cf minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0206 13:55:18.699658    2080 start.go:321] duration metric: took 2.743532471s to joinCluster
I0206 13:55:18.699691    2080 start.go:237] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:false Worker:true}
I0206 13:55:18.716727    2080 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I0206 13:55:18.746853    2080 out.go:179] üîé  Verifying Kubernetes components...
I0206 13:55:18.791410    2080 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0206 13:55:19.043006    2080 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0206 13:55:19.193421    2080 kubeadm.go:586] duration metric: took 493.712378ms to wait for: map[apiserver:true system_pods:true]
I0206 13:55:19.193432    2080 node_conditions.go:101] verifying NodePressure condition ...
I0206 13:55:19.288095    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:55:19.288104    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:55:19.288110    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:55:19.288112    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:55:19.288113    2080 node_conditions.go:121] node storage ephemeral capacity is 40581564Ki
I0206 13:55:19.288114    2080 node_conditions.go:122] node cpu capacity is 18
I0206 13:55:19.288116    2080 node_conditions.go:104] duration metric: took 94.68188ms to run NodePressure ...
I0206 13:55:19.288123    2080 start.go:243] waiting for startup goroutines ...
I0206 13:55:19.288134    2080 start.go:257] writing updated cluster config ...
I0206 13:55:19.290988    2080 ssh_runner.go:194] Run: rm -f paused
I0206 13:55:20.698739    2080 start.go:629] kubectl: 1.28.15, cluster: 1.34.0 (minor skew: 6)
I0206 13:55:20.740246    2080 out.go:203] 
W0206 13:55:20.771011    2080 out.go:285] ‚ùó  /usr/bin/kubectl is version 1.28.15, which may have incompatibilities with Kubernetes 1.34.0.
I0206 13:55:20.810786    2080 out.go:179]     ‚ñ™ Want kubectl v1.34.0? Try 'minikube kubectl -- get pods -A'
I0206 13:55:20.863265    2080 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


